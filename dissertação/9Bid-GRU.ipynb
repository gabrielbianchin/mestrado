{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classesQ8 = base.iloc[:1400000, 20:28].values\n",
    "classesQ8 = np.reshape(classesQ8, (2000, 700, 8))\n",
    "print(classesQ8.shape)\n",
    "\n",
    "classesQ3 = base.iloc[:1400000, 28:31].values\n",
    "classesQ3 = np.reshape(classesQ3, (2000, 700, 3))\n",
    "print(classesQ3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNGRU, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede(saida):\n",
    "    model = Sequential()\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(saida, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test, saida):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], saida))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], saida))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "    \n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 17:31:55.370047  4388 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0823 17:31:55.374036  4388 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0823 17:31:55.375055  4388 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0823 17:31:55.376002  4388 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    2    4 ... 1996 1997 1999] TEST: [   1    3    6    8    9   10   11   12   21   22   23   26   29   30\n",
      "   39   42   44   47   57   59   61   62   66   70   74   76   86   87\n",
      "   96   97  104  105  106  112  116  121  123  124  138  145  155  165\n",
      "  175  179  180  182  194  200  204  205  206  208  214  221  223  228\n",
      "  229  233  235  237  246  249  251  255  260  261  269  272  282  287\n",
      "  288  295  299  302  304  305  315  325  337  339  347  348  350  357\n",
      "  369  379  384  392  403  407  408  412  415  419  420  421  427  432\n",
      "  436  443  446  465  468  475  494  496  497  502  507  513  515  520\n",
      "  524  527  528  529  534  542  546  549  554  562  567  579  582  585\n",
      "  594  598  600  603  606  607  617  618  621  624  629  641  643  650\n",
      "  651  657  662  666  673  674  677  684  690  691  698  702  704  710\n",
      "  714  717  721  725  727  730  737  738  750  753  762  763  776  784\n",
      "  785  788  791  795  796  809  810  812  820  828  829  840  841  849\n",
      "  861  862  865  870  872  880  881  887  896  898  901  903  905  906\n",
      "  912  918  924  937  939  940  942  953  963  965  973  975  993  995\n",
      "  997 1003 1004 1008 1010 1012 1024 1025 1031 1032 1033 1035 1044 1049\n",
      " 1050 1059 1063 1067 1075 1080 1081 1084 1090 1092 1093 1097 1101 1106\n",
      " 1111 1116 1118 1124 1132 1139 1141 1142 1145 1147 1153 1155 1156 1158\n",
      " 1166 1167 1173 1179 1188 1189 1202 1206 1207 1213 1237 1238 1245 1247\n",
      " 1279 1280 1284 1290 1296 1302 1306 1314 1317 1322 1326 1332 1347 1349\n",
      " 1354 1365 1367 1371 1376 1402 1406 1414 1415 1418 1422 1423 1424 1432\n",
      " 1433 1441 1442 1445 1447 1453 1454 1457 1458 1465 1469 1472 1478 1482\n",
      " 1484 1487 1489 1493 1494 1496 1497 1498 1499 1501 1512 1513 1515 1516\n",
      " 1517 1530 1541 1543 1556 1563 1567 1573 1588 1610 1620 1628 1632 1636\n",
      " 1639 1648 1657 1658 1659 1660 1667 1672 1679 1684 1688 1691 1699 1701\n",
      " 1702 1708 1714 1718 1730 1733 1737 1738 1748 1752 1762 1763 1766 1768\n",
      " 1771 1776 1777 1778 1780 1788 1793 1801 1816 1818 1829 1832 1838 1845\n",
      " 1846 1857 1860 1884 1887 1900 1902 1904 1908 1910 1927 1930 1931 1938\n",
      " 1939 1945 1953 1969 1982 1984 1988 1998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 17:31:57.565779  4388 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.5907 - acc: 0.1559\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5306 - acc: 0.1709\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5240 - acc: 0.1740\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5161 - acc: 0.1781\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5135 - acc: 0.1787\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5092 - acc: 0.1808\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5050 - acc: 0.1828\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5022 - acc: 0.1837\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4967 - acc: 0.1862\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4933 - acc: 0.1875\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4926 - acc: 0.1878\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4858 - acc: 0.1908\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4839 - acc: 0.1916\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4796 - acc: 0.1933\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4772 - acc: 0.1944\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4746 - acc: 0.1958\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4672 - acc: 0.1988\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4660 - acc: 0.1986\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4550 - acc: 0.2030\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4514 - acc: 0.2046\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4437 - acc: 0.2076\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4446 - acc: 0.2065\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4359 - acc: 0.2100\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4293 - acc: 0.2123\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4277 - acc: 0.2128\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4218 - acc: 0.2148\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4183 - acc: 0.2159\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4140 - acc: 0.2174\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4101 - acc: 0.2188\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4049 - acc: 0.2201\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4004 - acc: 0.2217\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3981 - acc: 0.2225\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3952 - acc: 0.2236\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3896 - acc: 0.2257\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3887 - acc: 0.2257\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3857 - acc: 0.2268\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3801 - acc: 0.2285\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3765 - acc: 0.2299\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3739 - acc: 0.2306\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3709 - acc: 0.2316\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3684 - acc: 0.2327\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3647 - acc: 0.2333\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3613 - acc: 0.2348\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3583 - acc: 0.2359\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3565 - acc: 0.2361\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3555 - acc: 0.2364\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3511 - acc: 0.2379\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3477 - acc: 0.2390\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3454 - acc: 0.2396\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3427 - acc: 0.2407\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3426 - acc: 0.2404\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3386 - acc: 0.2421\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3366 - acc: 0.2426\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3348 - acc: 0.2434\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3345 - acc: 0.2433\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3303 - acc: 0.2446\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3288 - acc: 0.2452\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3269 - acc: 0.2456\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3269 - acc: 0.2458\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3248 - acc: 0.2464\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3218 - acc: 0.2473\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3202 - acc: 0.2478\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3180 - acc: 0.2487\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3157 - acc: 0.2494\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3142 - acc: 0.2500\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3145 - acc: 0.2501\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3119 - acc: 0.2508\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3105 - acc: 0.2511\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3110 - acc: 0.2511\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3076 - acc: 0.2524\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3055 - acc: 0.2530\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3043 - acc: 0.2534\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3042 - acc: 0.2534\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3028 - acc: 0.2537\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3003 - acc: 0.2546\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2996 - acc: 0.2551\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2995 - acc: 0.2549\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2970 - acc: 0.2553\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2959 - acc: 0.2561\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2946 - acc: 0.2564\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2931 - acc: 0.2570\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2924 - acc: 0.2569\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2923 - acc: 0.2573\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2898 - acc: 0.2579\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2894 - acc: 0.2584\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2882 - acc: 0.2585\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2856 - acc: 0.2595\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2843 - acc: 0.2597\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2849 - acc: 0.2599\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2844 - acc: 0.2601\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2823 - acc: 0.2606\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2814 - acc: 0.2609\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2802 - acc: 0.2616\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2807 - acc: 0.2612\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2787 - acc: 0.2619\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2779 - acc: 0.2621\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2755 - acc: 0.2630\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2764 - acc: 0.2627\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2751 - acc: 0.2630\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2739 - acc: 0.2636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.03      1301\n",
      "           1       0.66      0.67      0.67     21855\n",
      "           2       0.35      0.25      0.30      4129\n",
      "           3       0.76      0.81      0.78     35259\n",
      "           4       0.40      0.23      0.30       705\n",
      "           5       0.44      0.59      0.51     21022\n",
      "           6       0.37      0.12      0.18      9143\n",
      "           7       0.42      0.41      0.41     11876\n",
      "\n",
      "    accuracy                           0.59    105290\n",
      "   macro avg       0.49      0.39      0.40    105290\n",
      "weighted avg       0.58      0.59      0.58    105290\n",
      "\n",
      "Acurácia\n",
      "0.386483134939126\n",
      "Precisao\n",
      "0.5824345296635496\n",
      "Recall\n",
      "0.5945103998480388\n",
      "F1\n",
      "0.5770038771794059\n",
      "[[   19   255    34   150     5   670    37   131]\n",
      " [    2 14668   242  1944    17  3894   196   892]\n",
      " [    1   427  1048   856     1  1044    98   654]\n",
      " [    2  1544   487 28422   154  2906   203  1541]\n",
      " [    0    71    17   360   165    64     4    24]\n",
      " [    6  3046   477  2417    30 12382   754  1910]\n",
      " [    4  1140   238  1148    15  4139  1074  1385]\n",
      " [    4   995   430  2263    24  2840   502  4818]]\n",
      "TRAIN: [   1    2    3 ... 1997 1998 1999] TEST: [   0   14   16   17   18   24   32   34   46   52   63   80   90   93\n",
      "  101  109  114  117  118  126  129  131  146  153  156  161  162  164\n",
      "  169  174  181  185  186  187  198  210  215  226  230  231  232  238\n",
      "  242  243  252  254  257  265  268  274  277  280  291  294  309  334\n",
      "  341  343  345  346  352  353  356  358  360  362  366  372  376  382\n",
      "  391  395  399  402  409  428  434  435  444  459  461  469  471  472\n",
      "  483  485  486  488  491  498  504  511  525  526  530  531  537  547\n",
      "  556  558  573  581  591  593  597  613  614  622  626  627  638  639\n",
      "  640  642  647  648  654  655  656  658  659  663  664  669  672  675\n",
      "  678  689  693  695  700  707  708  716  718  726  733  736  739  746\n",
      "  747  749  757  760  761  765  769  783  789  801  813  821  826  832\n",
      "  833  834  838  839  846  850  851  854  858  864  875  890  891  892\n",
      "  893  895  900  902  908  910  922  930  938  944  947  948  950  954\n",
      "  957  959  966  974  980  984  986  991  996  998 1001 1011 1015 1019\n",
      " 1020 1027 1028 1037 1038 1045 1048 1052 1054 1056 1058 1069 1070 1078\n",
      " 1082 1088 1091 1094 1098 1103 1105 1107 1114 1120 1123 1125 1128 1133\n",
      " 1134 1138 1140 1143 1146 1151 1163 1170 1171 1180 1183 1185 1187 1201\n",
      " 1208 1210 1214 1216 1218 1225 1229 1235 1257 1261 1268 1272 1273 1275\n",
      " 1286 1287 1305 1318 1319 1328 1329 1331 1339 1340 1342 1350 1363 1370\n",
      " 1380 1382 1385 1391 1398 1413 1419 1420 1425 1431 1437 1440 1456 1461\n",
      " 1464 1470 1479 1485 1495 1506 1510 1514 1525 1527 1538 1539 1544 1553\n",
      " 1554 1557 1558 1559 1564 1565 1569 1571 1577 1582 1584 1586 1591 1592\n",
      " 1593 1608 1612 1615 1619 1621 1624 1630 1635 1638 1640 1644 1647 1650\n",
      " 1653 1656 1671 1673 1674 1675 1682 1687 1693 1697 1700 1703 1705 1713\n",
      " 1720 1724 1728 1732 1734 1741 1744 1746 1754 1755 1757 1764 1765 1779\n",
      " 1781 1783 1784 1790 1792 1796 1797 1799 1802 1805 1807 1810 1812 1814\n",
      " 1817 1836 1840 1843 1849 1851 1852 1853 1856 1858 1863 1874 1875 1876\n",
      " 1879 1881 1882 1886 1892 1901 1909 1914 1921 1924 1937 1944 1950 1966\n",
      " 1967 1978 1979 1985 1986 1993 1995 1996]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5846 - acc: 0.1613\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5336 - acc: 0.1714\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5236 - acc: 0.1765\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5203 - acc: 0.1780\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5168 - acc: 0.1799\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5108 - acc: 0.1823\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5080 - acc: 0.1836\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5051 - acc: 0.1847\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5016 - acc: 0.1862\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4948 - acc: 0.1892\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4941 - acc: 0.1894\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4887 - acc: 0.1918\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4856 - acc: 0.1933\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4839 - acc: 0.1939\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4786 - acc: 0.1957\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4783 - acc: 0.1963\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4740 - acc: 0.1979\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4708 - acc: 0.1994\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4685 - acc: 0.2002\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4561 - acc: 0.2049\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4522 - acc: 0.2065\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4487 - acc: 0.2075\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4439 - acc: 0.2094\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4393 - acc: 0.2110\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4353 - acc: 0.2120\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4309 - acc: 0.2138\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4264 - acc: 0.2155\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4220 - acc: 0.2171\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4181 - acc: 0.2186\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4165 - acc: 0.2191\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4109 - acc: 0.2206\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4077 - acc: 0.2219\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4040 - acc: 0.2229\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4013 - acc: 0.2239\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3956 - acc: 0.2258\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3924 - acc: 0.2267\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3908 - acc: 0.2271\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3850 - acc: 0.2292\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3821 - acc: 0.2301\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3825 - acc: 0.2298\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3767 - acc: 0.2318\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3735 - acc: 0.2325\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3695 - acc: 0.2340\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3670 - acc: 0.2348\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3633 - acc: 0.2359\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3634 - acc: 0.2360\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3595 - acc: 0.2372\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3578 - acc: 0.2375\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3559 - acc: 0.2384\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3534 - acc: 0.2391\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3513 - acc: 0.2401\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3486 - acc: 0.2406\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3458 - acc: 0.2414\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3428 - acc: 0.2426\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3409 - acc: 0.2432\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3387 - acc: 0.2437\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3389 - acc: 0.2438\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3372 - acc: 0.2440\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3344 - acc: 0.2452\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3321 - acc: 0.2460\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3308 - acc: 0.2463\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3286 - acc: 0.2471\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3275 - acc: 0.2477\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3269 - acc: 0.2475\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3241 - acc: 0.2488\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3210 - acc: 0.2496\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3206 - acc: 0.2498\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3194 - acc: 0.2501\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3185 - acc: 0.2505\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3162 - acc: 0.2511\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3141 - acc: 0.2516\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3131 - acc: 0.2521\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3121 - acc: 0.2525\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3117 - acc: 0.2527\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3107 - acc: 0.2531\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3082 - acc: 0.2539\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3069 - acc: 0.2542\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3051 - acc: 0.2549\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3040 - acc: 0.2551\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3024 - acc: 0.2557\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3012 - acc: 0.2563\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3007 - acc: 0.2566\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3005 - acc: 0.2565\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2979 - acc: 0.2574\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2973 - acc: 0.2576\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2963 - acc: 0.2578\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2945 - acc: 0.2584\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2938 - acc: 0.2588\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2932 - acc: 0.2590\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2918 - acc: 0.2593\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2906 - acc: 0.2596\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2897 - acc: 0.2603\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2883 - acc: 0.2605\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2876 - acc: 0.2607\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2876 - acc: 0.2610\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2885 - acc: 0.2606\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2850 - acc: 0.2618\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2852 - acc: 0.2616\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2842 - acc: 0.2622\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2826 - acc: 0.2626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.00      0.01      1202\n",
      "           1       0.64      0.72      0.68     21589\n",
      "           2       0.41      0.32      0.36      4073\n",
      "           3       0.76      0.82      0.79     34221\n",
      "           4       0.50      0.30      0.38       739\n",
      "           5       0.47      0.56      0.51     20565\n",
      "           6       0.36      0.13      0.19      9065\n",
      "           7       0.44      0.40      0.42     11722\n",
      "\n",
      "    accuracy                           0.61    103176\n",
      "   macro avg       0.55      0.41      0.42    103176\n",
      "weighted avg       0.59      0.61      0.59    103176\n",
      "\n",
      "Acurácia\n",
      "0.4071403454509964\n",
      "Precisao\n",
      "0.5914653445587409\n",
      "Recall\n",
      "0.6064297898736141\n",
      "F1\n",
      "0.5872567670675553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    4   275    28   155     1   577    57   105]\n",
      " [    0 15626   165  1678    17  3165   247   691]\n",
      " [    0   387  1319   835     8   867   101   556]\n",
      " [    0  1929   474 28044   126  2172   238  1238]\n",
      " [    0    54    14   344   221    56     4    46]\n",
      " [    1  3668   458  2250    18 11518   873  1779]\n",
      " [    0  1415   272  1144    17  3639  1183  1395]\n",
      " [    0  1238   497  2329    30  2430   544  4654]]\n",
      "TRAIN: [   0    1    3 ... 1996 1998 1999] TEST: [   2    5    7   13   37   38   49   50   51   64   72   73   79   81\n",
      "   88   89  100  108  110  111  120  127  130  139  142  144  151  154\n",
      "  158  159  166  173  177  199  203  209  213  217  218  222  224  225\n",
      "  240  250  263  267  275  278  279  283  285  289  292  293  297  298\n",
      "  300  308  311  312  314  316  317  318  321  329  331  335  338  344\n",
      "  354  355  361  367  370  371  377  378  380  387  389  396  400  404\n",
      "  414  417  422  423  430  437  441  453  454  456  458  463  476  482\n",
      "  492  493  499  501  508  523  543  548  551  555  560  572  575  578\n",
      "  580  588  589  590  596  599  601  602  605  609  611  616  628  630\n",
      "  631  633  636  644  660  668  680  683  696  701  703  724  728  732\n",
      "  734  743  751  755  766  773  778  786  793  794  800  805  811  817\n",
      "  827  831  835  860  873  878  884  888  899  907  913  916  917  919\n",
      "  921  923  925  928  932  935  941  945  949  960  961  962  967  970\n",
      "  982  985  988  992  999 1007 1014 1017 1023 1026 1029 1041 1042 1055\n",
      " 1060 1066 1076 1077 1079 1085 1086 1099 1102 1104 1109 1110 1119 1121\n",
      " 1126 1129 1130 1135 1137 1152 1157 1164 1172 1176 1182 1186 1190 1191\n",
      " 1193 1197 1198 1200 1204 1211 1215 1217 1219 1220 1221 1223 1226 1227\n",
      " 1228 1231 1240 1243 1244 1250 1252 1255 1262 1263 1266 1267 1271 1276\n",
      " 1278 1285 1294 1299 1310 1313 1316 1320 1321 1324 1325 1330 1334 1336\n",
      " 1345 1346 1353 1356 1357 1358 1373 1374 1386 1387 1388 1399 1403 1404\n",
      " 1412 1427 1428 1430 1443 1444 1450 1451 1460 1462 1473 1474 1476 1491\n",
      " 1492 1500 1502 1504 1507 1508 1509 1520 1521 1523 1524 1531 1534 1537\n",
      " 1545 1548 1550 1560 1562 1568 1570 1572 1576 1578 1579 1580 1585 1587\n",
      " 1589 1590 1595 1597 1598 1604 1609 1614 1622 1625 1627 1633 1634 1643\n",
      " 1645 1654 1655 1661 1662 1663 1664 1665 1668 1681 1685 1686 1689 1694\n",
      " 1695 1710 1711 1715 1726 1729 1740 1742 1745 1747 1750 1756 1758 1759\n",
      " 1769 1782 1785 1794 1804 1806 1808 1813 1819 1820 1824 1828 1834 1848\n",
      " 1854 1859 1864 1865 1866 1889 1890 1891 1893 1894 1896 1905 1907 1917\n",
      " 1935 1941 1954 1958 1972 1975 1992 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5895 - acc: 0.1526\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5326 - acc: 0.1685\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5186 - acc: 0.1753\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5178 - acc: 0.1757\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5107 - acc: 0.1789\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5062 - acc: 0.1810\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5031 - acc: 0.1826\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5024 - acc: 0.1827\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4943 - acc: 0.1860\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4919 - acc: 0.1873\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4909 - acc: 0.1875\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4834 - acc: 0.1907\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4811 - acc: 0.1917\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4771 - acc: 0.1937\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4783 - acc: 0.1930\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4719 - acc: 0.1953\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4680 - acc: 0.1971\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4633 - acc: 0.1992\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4589 - acc: 0.2007\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4558 - acc: 0.2015\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4431 - acc: 0.2065\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4390 - acc: 0.2080\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4348 - acc: 0.2094\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4293 - acc: 0.2115\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4260 - acc: 0.2126\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4208 - acc: 0.2142\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4166 - acc: 0.2157\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4132 - acc: 0.2166\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4060 - acc: 0.2193\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4016 - acc: 0.2205\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4026 - acc: 0.2204\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3946 - acc: 0.2227\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3908 - acc: 0.2242\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3856 - acc: 0.2258\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3826 - acc: 0.2267\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3791 - acc: 0.2281\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3754 - acc: 0.2291\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3729 - acc: 0.2300\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3696 - acc: 0.2312\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3658 - acc: 0.2323\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3634 - acc: 0.2329\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3604 - acc: 0.2340\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3576 - acc: 0.2351\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3580 - acc: 0.2344\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3520 - acc: 0.2368\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3505 - acc: 0.2369\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3466 - acc: 0.2382\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3440 - acc: 0.2390\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3417 - acc: 0.2401\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3395 - acc: 0.2410\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3369 - acc: 0.2415\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3368 - acc: 0.2415\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3327 - acc: 0.2430\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3316 - acc: 0.2431\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3289 - acc: 0.2442\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3273 - acc: 0.2444\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3258 - acc: 0.2449\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3226 - acc: 0.2462\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3205 - acc: 0.2467\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3199 - acc: 0.2470\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3179 - acc: 0.2478\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3172 - acc: 0.2480\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3140 - acc: 0.2487\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3136 - acc: 0.2491\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3113 - acc: 0.2499\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3089 - acc: 0.2507\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3091 - acc: 0.2509\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3062 - acc: 0.2516\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3058 - acc: 0.2519\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3035 - acc: 0.2524\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3035 - acc: 0.2523\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3015 - acc: 0.2529\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3003 - acc: 0.2536\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2997 - acc: 0.2539\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2968 - acc: 0.2549\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2971 - acc: 0.2550\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2952 - acc: 0.2555\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2948 - acc: 0.2556\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2921 - acc: 0.2565\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2908 - acc: 0.2568\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2900 - acc: 0.2572\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2880 - acc: 0.2579\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2882 - acc: 0.2574\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2859 - acc: 0.2584\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2859 - acc: 0.2584\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2839 - acc: 0.2590\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2827 - acc: 0.2596\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2834 - acc: 0.2595\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2822 - acc: 0.2597\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2814 - acc: 0.2598\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2821 - acc: 0.2597\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2805 - acc: 0.2604\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2772 - acc: 0.2613\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2772 - acc: 0.2615\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2753 - acc: 0.2623\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2741 - acc: 0.2623\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2745 - acc: 0.2623\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2735 - acc: 0.2626\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2726 - acc: 0.2630\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2711 - acc: 0.2637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.01      0.03      1284\n",
      "           1       0.61      0.67      0.64     22011\n",
      "           2       0.30      0.26      0.28      3985\n",
      "           3       0.72      0.81      0.76     35735\n",
      "           4       0.39      0.20      0.26       691\n",
      "           5       0.47      0.50      0.48     21422\n",
      "           6       0.34      0.14      0.20      9478\n",
      "           7       0.40      0.40      0.40     12032\n",
      "\n",
      "    accuracy                           0.58    106638\n",
      "   macro avg       0.47      0.37      0.38    106638\n",
      "weighted avg       0.56      0.58      0.56    106638\n",
      "\n",
      "Acurácia\n",
      "0.3744424552606603\n",
      "Precisao\n",
      "0.5582114835532852\n",
      "Recall\n",
      "0.5778990603724751\n",
      "F1\n",
      "0.5598158626146293\n",
      "[[   18   306    39   185     2   515    83   136]\n",
      " [    0 14807   254  2644    12  3147   300   847]\n",
      " [    0   443  1053   994     9   731   108   647]\n",
      " [    0  2140   645 28805   122  2066   300  1657]\n",
      " [    0    65    14   371   138    54     7    42]\n",
      " [   11  3794   595  3036    34 10656  1128  2168]\n",
      " [    3  1394   339  1430    19  3336  1353  1604]\n",
      " [    0  1161   585  2677    21  2121   671  4796]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1998] TEST: [   4   19   20   28   35   40   41   43   45   53   54   58   60   67\n",
      "   69   75   77   85   91   92   94  102  103  107  113  115  119  125\n",
      "  134  137  140  141  147  148  149  160  170  171  178  183  191  192\n",
      "  193  197  201  202  207  211  234  239  244  247  248  256  258  262\n",
      "  264  266  270  273  281  286  303  310  313  320  326  330  332  333\n",
      "  336  340  342  351  368  375  381  383  386  390  393  397  398  406\n",
      "  410  413  416  418  424  425  438  445  447  449  451  455  457  462\n",
      "  464  466  473  474  487  490  495  512  517  532  533  535  536  538\n",
      "  540  545  552  553  557  563  566  574  576  577  583  586  587  592\n",
      "  612  619  634  635  637  646  649  652  661  670  671  681  682  685\n",
      "  687  692  699  705  706  709  722  740  744  745  754  756  767  770\n",
      "  774  775  777  779  780  781  782  797  798  806  808  814  815  816\n",
      "  823  824  825  845  847  848  853  869  874  876  882  885  889  897\n",
      "  909  914  915  927  929  931  934  936  943  946  952  955  958  968\n",
      "  969  971  972  976  978  979  981  987  994 1002 1005 1022 1030 1046\n",
      " 1051 1057 1062 1068 1071 1072 1073 1074 1089 1096 1100 1108 1113 1115\n",
      " 1122 1127 1136 1144 1149 1159 1160 1165 1174 1178 1184 1192 1194 1195\n",
      " 1196 1205 1209 1224 1234 1236 1239 1241 1242 1248 1249 1256 1258 1264\n",
      " 1270 1274 1277 1281 1282 1288 1291 1298 1307 1309 1311 1312 1323 1327\n",
      " 1337 1343 1348 1359 1361 1362 1364 1369 1378 1379 1381 1383 1393 1394\n",
      " 1396 1400 1407 1408 1409 1410 1411 1426 1429 1434 1435 1436 1446 1448\n",
      " 1455 1463 1466 1467 1468 1471 1477 1490 1503 1505 1519 1526 1529 1532\n",
      " 1533 1535 1542 1546 1549 1551 1602 1606 1607 1617 1618 1623 1637 1646\n",
      " 1649 1652 1670 1677 1680 1683 1690 1696 1698 1706 1707 1709 1716 1717\n",
      " 1719 1722 1723 1725 1727 1735 1743 1749 1760 1761 1772 1775 1786 1787\n",
      " 1789 1791 1795 1798 1811 1822 1825 1831 1833 1837 1855 1861 1867 1868\n",
      " 1869 1870 1871 1872 1880 1885 1895 1897 1903 1911 1915 1916 1918 1919\n",
      " 1922 1925 1928 1933 1934 1943 1946 1949 1952 1955 1956 1961 1964 1965\n",
      " 1968 1970 1980 1981 1987 1990 1994 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5942 - acc: 0.1596\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5420 - acc: 0.1715\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5316 - acc: 0.1762\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5258 - acc: 0.1792\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5184 - acc: 0.1826\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5143 - acc: 0.1852\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5101 - acc: 0.1861\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5054 - acc: 0.1886\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5007 - acc: 0.1906\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4991 - acc: 0.1910\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4969 - acc: 0.1923\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4931 - acc: 0.1939\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4901 - acc: 0.1952\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4874 - acc: 0.1959\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4832 - acc: 0.1975\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4805 - acc: 0.1990\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4738 - acc: 0.2016\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4665 - acc: 0.2045\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4580 - acc: 0.2072\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4516 - acc: 0.2095\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4488 - acc: 0.2106\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4425 - acc: 0.2131\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4393 - acc: 0.2143\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4352 - acc: 0.2159\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4320 - acc: 0.2165\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4268 - acc: 0.2184\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4203 - acc: 0.2209\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4163 - acc: 0.2222\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4141 - acc: 0.2227\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4097 - acc: 0.2240\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4048 - acc: 0.2257\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4026 - acc: 0.2265\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4000 - acc: 0.2273\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3962 - acc: 0.2289\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3896 - acc: 0.2304\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3867 - acc: 0.2319\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3838 - acc: 0.2324\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3826 - acc: 0.2331\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3779 - acc: 0.2345\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3752 - acc: 0.2355\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3719 - acc: 0.2364\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3677 - acc: 0.2378\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3654 - acc: 0.2388\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3634 - acc: 0.2391\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3611 - acc: 0.2399\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3568 - acc: 0.2412\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3547 - acc: 0.2421\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3533 - acc: 0.2424\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3507 - acc: 0.2433\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3474 - acc: 0.2443\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3457 - acc: 0.2450\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3443 - acc: 0.2453\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3397 - acc: 0.2472\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3398 - acc: 0.2468\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3384 - acc: 0.2471\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3358 - acc: 0.2481\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3330 - acc: 0.2493\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3321 - acc: 0.2495\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3313 - acc: 0.2496\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3273 - acc: 0.2507\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3256 - acc: 0.2517\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3241 - acc: 0.2518\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3226 - acc: 0.2527\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3210 - acc: 0.2527\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3208 - acc: 0.2530\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3191 - acc: 0.2537\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3170 - acc: 0.2543\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3141 - acc: 0.2551\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3139 - acc: 0.2551\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3134 - acc: 0.2558\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3106 - acc: 0.2567\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3097 - acc: 0.2568\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3088 - acc: 0.2571\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3067 - acc: 0.2577\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3064 - acc: 0.2575\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3032 - acc: 0.2587\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3023 - acc: 0.2595\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3008 - acc: 0.2596\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2999 - acc: 0.2601\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2979 - acc: 0.2606\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2982 - acc: 0.2605\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2970 - acc: 0.2611\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2950 - acc: 0.2616\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2935 - acc: 0.2623\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2943 - acc: 0.2619\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2927 - acc: 0.2623\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2913 - acc: 0.2631\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2912 - acc: 0.2631\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2906 - acc: 0.2630\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2896 - acc: 0.2634\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2877 - acc: 0.2640\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2863 - acc: 0.2644\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2857 - acc: 0.2647\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2842 - acc: 0.2654\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2831 - acc: 0.2658\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2833 - acc: 0.2655\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2818 - acc: 0.2662\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2808 - acc: 0.2663\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2793 - acc: 0.2672\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2783 - acc: 0.2675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.02      0.04      1237\n",
      "           1       0.66      0.70      0.68     20919\n",
      "           2       0.36      0.28      0.31      3831\n",
      "           3       0.73      0.84      0.78     32209\n",
      "           4       0.38      0.30      0.34       650\n",
      "           5       0.50      0.55      0.52     20441\n",
      "           6       0.41      0.14      0.21      8687\n",
      "           7       0.44      0.42      0.43     11241\n",
      "\n",
      "    accuracy                           0.61     99215\n",
      "   macro avg       0.50      0.41      0.41     99215\n",
      "weighted avg       0.59      0.61      0.59     99215\n",
      "\n",
      "Acurácia\n",
      "0.40714019244811434\n",
      "Precisao\n",
      "0.5875654315195264\n",
      "Recall\n",
      "0.6072166507080583\n",
      "F1\n",
      "0.5869353591890785\n",
      "[[   29   237    30   175     7   596    54   109]\n",
      " [    3 14607   162  2030    28  3022   224   843]\n",
      " [    1   370  1062   992    21   723    84   578]\n",
      " [    0  1602   408 27149   158  1575   145  1172]\n",
      " [    0    45     6   317   196    47     5    34]\n",
      " [   12  3161   494  2818    55 11221   825  1855]\n",
      " [    4  1087   279  1291    14  3310  1252  1450]\n",
      " [    3   955   546  2428    41  2049   490  4729]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [  15   25   27   31   33   36   48   55   56   65   68   71   78   82\n",
      "   83   84   95   98   99  122  128  132  133  135  136  143  150  152\n",
      "  157  163  167  168  172  176  184  188  189  190  195  196  212  216\n",
      "  219  220  227  236  241  245  253  259  271  276  284  290  296  301\n",
      "  306  307  319  322  323  324  327  328  349  359  363  364  365  373\n",
      "  374  385  388  394  401  405  411  426  429  431  433  439  440  442\n",
      "  448  450  452  460  467  470  477  478  479  480  481  484  489  500\n",
      "  503  505  506  509  510  514  516  518  519  521  522  539  541  544\n",
      "  550  559  561  564  565  568  569  570  571  584  595  604  608  610\n",
      "  615  620  623  625  632  645  653  665  667  676  679  686  688  694\n",
      "  697  711  712  713  715  719  720  723  729  731  735  741  742  748\n",
      "  752  758  759  764  768  771  772  787  790  792  799  802  803  804\n",
      "  807  818  819  822  830  836  837  842  843  844  852  855  856  857\n",
      "  859  863  866  867  868  871  877  879  883  886  894  904  911  920\n",
      "  926  933  951  956  964  977  983  989  990 1000 1006 1009 1013 1016\n",
      " 1018 1021 1034 1036 1039 1040 1043 1047 1053 1061 1064 1065 1083 1087\n",
      " 1095 1112 1117 1131 1148 1150 1154 1161 1162 1168 1169 1175 1177 1181\n",
      " 1199 1203 1212 1222 1230 1232 1233 1246 1251 1253 1254 1259 1260 1265\n",
      " 1269 1283 1289 1292 1293 1295 1297 1300 1301 1303 1304 1308 1315 1333\n",
      " 1335 1338 1341 1344 1351 1352 1355 1360 1366 1368 1372 1375 1377 1384\n",
      " 1389 1390 1392 1395 1397 1401 1405 1416 1417 1421 1438 1439 1449 1452\n",
      " 1459 1475 1480 1481 1483 1486 1488 1511 1518 1522 1528 1536 1540 1547\n",
      " 1552 1555 1561 1566 1574 1575 1581 1583 1594 1596 1599 1600 1601 1603\n",
      " 1605 1611 1613 1616 1626 1629 1631 1641 1642 1651 1666 1669 1676 1678\n",
      " 1692 1704 1712 1721 1731 1736 1739 1751 1753 1767 1770 1773 1774 1800\n",
      " 1803 1809 1815 1821 1823 1826 1827 1830 1835 1839 1841 1842 1844 1847\n",
      " 1850 1862 1873 1877 1878 1883 1888 1898 1899 1906 1912 1913 1920 1923\n",
      " 1926 1929 1932 1936 1940 1942 1947 1948 1951 1957 1959 1960 1962 1963\n",
      " 1971 1973 1974 1976 1977 1983 1989 1991]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5872 - acc: 0.1617\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5373 - acc: 0.1720\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5311 - acc: 0.1747\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5195 - acc: 0.1797\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5153 - acc: 0.1818\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5125 - acc: 0.1834\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5063 - acc: 0.1858\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5028 - acc: 0.1874\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5003 - acc: 0.1885\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4957 - acc: 0.1906\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4939 - acc: 0.1914\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4885 - acc: 0.1935\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4859 - acc: 0.1945\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4840 - acc: 0.1953\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4783 - acc: 0.1978\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4739 - acc: 0.1999\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4737 - acc: 0.1996\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4712 - acc: 0.2008\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4597 - acc: 0.2053\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4522 - acc: 0.2080\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4468 - acc: 0.2095\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4425 - acc: 0.2117\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4385 - acc: 0.2131\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4327 - acc: 0.2149\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4284 - acc: 0.2164\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4249 - acc: 0.2172\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4224 - acc: 0.2181\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4164 - acc: 0.2204\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4127 - acc: 0.2215\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4090 - acc: 0.2228\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4041 - acc: 0.2246\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4004 - acc: 0.2258\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3974 - acc: 0.2268\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3948 - acc: 0.2276\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3915 - acc: 0.2284\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3871 - acc: 0.2302\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3850 - acc: 0.2306\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3816 - acc: 0.2316\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3780 - acc: 0.2329\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3752 - acc: 0.2338\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3724 - acc: 0.2345\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3683 - acc: 0.2359\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3649 - acc: 0.2371\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3625 - acc: 0.2376\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3621 - acc: 0.2380\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3578 - acc: 0.2391\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3546 - acc: 0.2402\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3546 - acc: 0.2401\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3507 - acc: 0.2413\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3486 - acc: 0.2421\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3462 - acc: 0.2428\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3436 - acc: 0.2437\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3401 - acc: 0.2451\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3402 - acc: 0.2449\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3385 - acc: 0.2456\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3349 - acc: 0.2467\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3346 - acc: 0.2468\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3319 - acc: 0.2476\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3315 - acc: 0.2479\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3280 - acc: 0.2489\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3262 - acc: 0.2495\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3248 - acc: 0.2499\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3225 - acc: 0.2506\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3214 - acc: 0.2510\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3212 - acc: 0.2512\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3184 - acc: 0.2522\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3186 - acc: 0.2519\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3150 - acc: 0.2531\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3130 - acc: 0.2539\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3136 - acc: 0.2536\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3116 - acc: 0.2543\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3098 - acc: 0.2551\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3092 - acc: 0.2548\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3076 - acc: 0.2555\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3066 - acc: 0.2558\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3049 - acc: 0.2565\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3025 - acc: 0.2573\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3017 - acc: 0.2573\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3013 - acc: 0.2576\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3007 - acc: 0.2578\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2990 - acc: 0.2583\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2967 - acc: 0.2591\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2961 - acc: 0.2595\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2961 - acc: 0.2595\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2945 - acc: 0.2598\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2933 - acc: 0.2604\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2915 - acc: 0.2610\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2917 - acc: 0.2609\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2911 - acc: 0.2610\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2887 - acc: 0.2618\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2877 - acc: 0.2621\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2866 - acc: 0.2624\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2865 - acc: 0.2627\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2846 - acc: 0.2634\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2830 - acc: 0.2637\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2833 - acc: 0.2638\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2822 - acc: 0.2642\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2812 - acc: 0.2643\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2817 - acc: 0.2644\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2811 - acc: 0.2646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.02      0.03      1240\n",
      "           1       0.66      0.70      0.68     22558\n",
      "           2       0.40      0.28      0.33      3853\n",
      "           3       0.75      0.82      0.78     31719\n",
      "           4       0.52      0.27      0.36       732\n",
      "           5       0.47      0.56      0.51     20635\n",
      "           6       0.36      0.14      0.20      9215\n",
      "           7       0.42      0.42      0.42     11537\n",
      "\n",
      "    accuracy                           0.60    101489\n",
      "   macro avg       0.54      0.40      0.41    101489\n",
      "weighted avg       0.58      0.60      0.58    101489\n",
      "\n",
      "Acurácia\n",
      "0.40117253551572896\n",
      "Precisao\n",
      "0.5848117563077982\n",
      "Recall\n",
      "0.5988826375272197\n",
      "F1\n",
      "0.5807838826863404\n",
      "[[   20   272    31   152     1   587    51   126]\n",
      " [    1 15870   204  1811    20  3460   261   931]\n",
      " [    0   400  1089   857     2   796    87   622]\n",
      " [    0  1631   385 25936   120  2047   230  1370]\n",
      " [    0    64     3   321   200    77    14    53]\n",
      " [    4  3479   387  2250    17 11570   949  1979]\n",
      " [    0  1296   209  1084    10  3742  1245  1629]\n",
      " [    2  1055   420  2143    18  2452   597  4850]]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_36 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_37 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_38 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_39 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_40 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_41 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_42 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_43 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_44 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 1,524,408\n",
      "Trainable params: 1,524,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Acurácias total\n",
      "[0.386483134939126, 0.4071403454509964, 0.3744424552606603, 0.40714019244811434, 0.40117253551572896]\n",
      "0.3952757327229252\n",
      "Precision total\n",
      "[0.5824345296635496, 0.5914653445587409, 0.5582114835532852, 0.5875654315195264, 0.5848117563077982]\n",
      "0.5808977091205801\n",
      "Recalls total\n",
      "[0.5945103998480388, 0.6064297898736141, 0.5778990603724751, 0.6072166507080583, 0.5988826375272197]\n",
      "0.5969877076658812\n",
      "F1 total\n",
      "[0.5770038771794059, 0.5872567670675553, 0.5598158626146293, 0.5869353591890785, 0.5807838826863404]\n",
      "0.5783591497474019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [   6   13   26   32   43   44   52   53   60   65   67   77   81   86\n",
      "   91   92   94  102  105  110  111  118  120  122  125  126  143  149\n",
      "  160  171  173  177  178  195  196  197  204  205  206  216  219  229\n",
      "  230  238  241  246  261  262  264  266  279  288  291  295  304  306\n",
      "  320  329  349  353  354  355  366  372  373  375  383  385  391  400\n",
      "  413  416  417  419  421  424  425  436  441  445  446  447  458  459\n",
      "  464  474  476  478  483  486  492  493  499  500  503  505  510  512\n",
      "  515  516  520  528  535  540  545  551  552  554  558  560  564  567\n",
      "  573  582  584  585  591  593  595  599  602  606  610  618  629  634\n",
      "  640  648  651  654  655  656  660  662  671  672  676  681  687  693\n",
      "  696  699  702  703  710  712  715  717  721  729  733  738  742  751\n",
      "  755  764  785  790  791  792  804  814  816  819  823  827  842  846\n",
      "  847  849  853  859  860  872  873  877  881  894  895  897  901  902\n",
      "  921  927  932  933  936  943  949  950  961  969  970  971  974  981\n",
      "  990  992 1002 1013 1016 1029 1033 1036 1037 1053 1054 1063 1066 1071\n",
      " 1079 1082 1086 1093 1095 1100 1101 1121 1126 1133 1147 1152 1153 1154\n",
      " 1162 1165 1167 1170 1172 1173 1177 1188 1194 1195 1209 1211 1216 1221\n",
      " 1228 1234 1247 1251 1259 1260 1267 1273 1283 1284 1290 1297 1302 1304\n",
      " 1309 1318 1319 1324 1330 1334 1343 1345 1349 1350 1356 1359 1365 1369\n",
      " 1370 1372 1374 1384 1385 1390 1400 1420 1429 1437 1465 1470 1475 1478\n",
      " 1488 1489 1491 1493 1496 1499 1501 1505 1508 1515 1516 1525 1526 1528\n",
      " 1530 1535 1542 1546 1550 1552 1556 1565 1566 1572 1576 1580 1591 1597\n",
      " 1600 1606 1619 1621 1622 1630 1631 1632 1638 1649 1653 1654 1665 1670\n",
      " 1680 1684 1693 1698 1705 1707 1709 1710 1716 1717 1718 1719 1744 1755\n",
      " 1758 1763 1769 1771 1773 1775 1776 1778 1780 1782 1785 1787 1789 1794\n",
      " 1798 1803 1807 1819 1823 1830 1833 1837 1840 1841 1847 1850 1851 1858\n",
      " 1861 1867 1876 1881 1885 1887 1898 1903 1904 1910 1914 1916 1920 1922\n",
      " 1926 1928 1929 1930 1932 1939 1942 1949 1954 1957 1958 1967 1970 1973\n",
      " 1981 1985 1990 1991 1993 1994 1995 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3445 - acc: 0.7740\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3188 - acc: 0.8549\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3104 - acc: 0.8573\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3071 - acc: 0.8602\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3063 - acc: 0.8611\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3040 - acc: 0.8627\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3019 - acc: 0.8633\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2988 - acc: 0.8652\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2962 - acc: 0.8665\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2951 - acc: 0.8671\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2933 - acc: 0.8682\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2906 - acc: 0.8695\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2879 - acc: 0.8708\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2859 - acc: 0.8721\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2858 - acc: 0.8720\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2831 - acc: 0.8733\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2805 - acc: 0.8747\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2768 - acc: 0.8763\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2755 - acc: 0.8775\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2708 - acc: 0.8798\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2686 - acc: 0.8818\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2634 - acc: 0.8840\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2576 - acc: 0.8871\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2526 - acc: 0.8896\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2479 - acc: 0.8919\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2450 - acc: 0.8939\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2424 - acc: 0.8946\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2391 - acc: 0.8964\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2389 - acc: 0.8962\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2327 - acc: 0.8996\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2299 - acc: 0.9004\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2246 - acc: 0.9031\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2222 - acc: 0.9040\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2195 - acc: 0.9053\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2203 - acc: 0.9050\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2187 - acc: 0.9062\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2109 - acc: 0.9096\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2072 - acc: 0.9108\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2058 - acc: 0.9114\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2034 - acc: 0.9131\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2017 - acc: 0.9136\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1997 - acc: 0.9146\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1986 - acc: 0.9153\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1945 - acc: 0.9168\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1923 - acc: 0.9177\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1898 - acc: 0.9190\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1881 - acc: 0.9194\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1861 - acc: 0.9205\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1845 - acc: 0.9213\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1826 - acc: 0.9217\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1816 - acc: 0.9225\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1801 - acc: 0.9233\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1766 - acc: 0.9250\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1753 - acc: 0.9255\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1753 - acc: 0.9256\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1722 - acc: 0.9266\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1710 - acc: 0.9273\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1700 - acc: 0.9276\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1673 - acc: 0.9287\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1692 - acc: 0.9281\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1673 - acc: 0.9289\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1653 - acc: 0.9298\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1646 - acc: 0.9299\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1622 - acc: 0.9313\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1611 - acc: 0.9318\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1597 - acc: 0.9327\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1577 - acc: 0.9335\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1560 - acc: 0.9341\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1552 - acc: 0.9344\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1546 - acc: 0.9347\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1537 - acc: 0.9351\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1532 - acc: 0.9355\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1508 - acc: 0.9363\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1496 - acc: 0.9368\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1486 - acc: 0.9374\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1484 - acc: 0.9373\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1460 - acc: 0.9389\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1475 - acc: 0.9378\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1452 - acc: 0.9389\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1443 - acc: 0.9395\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1452 - acc: 0.9388\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1435 - acc: 0.9396\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1426 - acc: 0.9398\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1413 - acc: 0.9405\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1397 - acc: 0.9411\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1391 - acc: 0.9414\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1385 - acc: 0.9418\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1375 - acc: 0.9421\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1387 - acc: 0.9416\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1369 - acc: 0.9426\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1364 - acc: 0.9425\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1345 - acc: 0.9435\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1338 - acc: 0.9438\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1339 - acc: 0.9436\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1341 - acc: 0.9437\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1338 - acc: 0.9436\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1322 - acc: 0.9447\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1312 - acc: 0.9449\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1322 - acc: 0.9444\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1295 - acc: 0.9457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73     42025\n",
      "           1       0.71      0.63      0.67     22598\n",
      "           2       0.78      0.78      0.78     38185\n",
      "\n",
      "    accuracy                           0.73    102808\n",
      "   macro avg       0.73      0.72      0.72    102808\n",
      "weighted avg       0.73      0.73      0.73    102808\n",
      "\n",
      "Acurácia\n",
      "0.7198650072411583\n",
      "Precisao\n",
      "0.734712374262338\n",
      "Recall\n",
      "0.7344759162711073\n",
      "F1\n",
      "0.7338409386097668\n",
      "[[31530  4330  6165]\n",
      " [ 6320 14259  2019]\n",
      " [ 6860  1604 29721]]\n",
      "TRAIN: [   1    3    4 ... 1997 1998 1999] TEST: [   0    2    5   16   23   28   41   48   55   59   64   70   80   85\n",
      "   90  100  104  108  109  115  119  137  139  146  154  162  169  181\n",
      "  182  184  185  189  194  200  203  208  218  221  225  228  232  236\n",
      "  239  252  253  269  270  280  298  301  303  308  323  326  327  330\n",
      "  337  339  344  356  357  362  364  367  368  382  390  392  396  397\n",
      "  399  406  409  410  412  431  434  435  437  439  442  444  451  457\n",
      "  461  465  472  473  477  514  518  522  524  526  527  531  538  541\n",
      "  550  555  562  565  566  577  578  581  590  605  607  608  617  619\n",
      "  621  627  628  630  643  653  659  667  668  670  686  690  697  700\n",
      "  704  713  719  732  734  739  746  753  762  763  770  771  772  774\n",
      "  776  777  782  784  786  787  789  793  794  796  797  801  805  806\n",
      "  810  813  817  830  833  835  837  854  864  866  867  869  871  883\n",
      "  886  887  893  906  910  915  923  928  930  938  947  948  956  963\n",
      "  972  973  977  985  986  987 1007 1020 1023 1032 1038 1040 1047 1050\n",
      " 1060 1062 1070 1083 1084 1085 1097 1099 1105 1110 1114 1117 1123 1124\n",
      " 1127 1130 1136 1141 1144 1145 1146 1149 1150 1163 1169 1178 1181 1182\n",
      " 1183 1187 1189 1191 1201 1205 1214 1215 1218 1222 1227 1229 1233 1236\n",
      " 1238 1241 1242 1243 1248 1257 1258 1261 1264 1266 1272 1275 1281 1288\n",
      " 1298 1299 1303 1306 1308 1310 1312 1315 1320 1321 1326 1327 1328 1336\n",
      " 1341 1344 1351 1352 1353 1357 1363 1371 1373 1375 1376 1378 1380 1386\n",
      " 1393 1395 1397 1401 1413 1416 1421 1426 1436 1438 1442 1449 1450 1451\n",
      " 1459 1460 1462 1463 1464 1472 1477 1480 1502 1510 1514 1518 1519 1531\n",
      " 1533 1544 1554 1557 1558 1564 1567 1568 1577 1579 1583 1586 1598 1610\n",
      " 1611 1613 1620 1645 1647 1652 1656 1657 1659 1661 1664 1672 1673 1674\n",
      " 1676 1678 1690 1692 1694 1702 1712 1713 1714 1715 1726 1727 1733 1737\n",
      " 1742 1747 1754 1757 1762 1765 1766 1788 1790 1793 1797 1805 1809 1814\n",
      " 1815 1824 1827 1829 1838 1846 1848 1883 1892 1893 1894 1897 1901 1905\n",
      " 1907 1915 1917 1921 1931 1933 1936 1946 1948 1950 1959 1961 1962 1965\n",
      " 1969 1975 1976 1978 1984 1987 1988 1996]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3397 - acc: 0.6765\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3151 - acc: 0.8558\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3088 - acc: 0.8599\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3042 - acc: 0.8625\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3034 - acc: 0.8628\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3007 - acc: 0.8642\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2987 - acc: 0.8655\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2966 - acc: 0.8662\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2952 - acc: 0.8665\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2928 - acc: 0.8680\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2913 - acc: 0.8696\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2896 - acc: 0.8699\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2882 - acc: 0.8705\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2871 - acc: 0.8713\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2871 - acc: 0.8719\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2842 - acc: 0.8729\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2814 - acc: 0.8750\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2792 - acc: 0.8757\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2777 - acc: 0.8760\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2746 - acc: 0.8787\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2726 - acc: 0.8798\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2724 - acc: 0.8805\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2668 - acc: 0.8832\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2638 - acc: 0.8844\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2604 - acc: 0.8865\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2544 - acc: 0.8893\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2489 - acc: 0.8922\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2468 - acc: 0.8932\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2424 - acc: 0.8949\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2398 - acc: 0.8964\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2361 - acc: 0.8983\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2317 - acc: 0.9002\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2276 - acc: 0.9020\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2266 - acc: 0.9023\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2218 - acc: 0.9043\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2204 - acc: 0.9056\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2169 - acc: 0.9072\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2131 - acc: 0.9087\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2117 - acc: 0.9091\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2078 - acc: 0.9112\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2052 - acc: 0.9122\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2040 - acc: 0.9123\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2002 - acc: 0.9143\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1990 - acc: 0.9153\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1978 - acc: 0.9154\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1925 - acc: 0.9181\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1921 - acc: 0.9184\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1895 - acc: 0.9192\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1869 - acc: 0.9204\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1857 - acc: 0.9210\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1849 - acc: 0.9213\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1827 - acc: 0.9224\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1811 - acc: 0.9229\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1792 - acc: 0.9238\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1770 - acc: 0.9247\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1761 - acc: 0.9252\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1736 - acc: 0.9263\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1714 - acc: 0.9276\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1716 - acc: 0.9271\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1694 - acc: 0.9284\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1676 - acc: 0.9291\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1654 - acc: 0.9301\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1658 - acc: 0.9297\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1641 - acc: 0.9306\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1620 - acc: 0.9314\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1603 - acc: 0.9322\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1592 - acc: 0.9327\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1585 - acc: 0.9335\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1574 - acc: 0.9338\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1566 - acc: 0.9339\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1550 - acc: 0.9346\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1545 - acc: 0.9348\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1541 - acc: 0.9350\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1526 - acc: 0.9356\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1512 - acc: 0.9363\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1498 - acc: 0.9371\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1491 - acc: 0.9372\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1473 - acc: 0.9378\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1464 - acc: 0.9383\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1461 - acc: 0.9385\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1452 - acc: 0.9391\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1442 - acc: 0.9395\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1432 - acc: 0.9399\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1431 - acc: 0.9398\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1426 - acc: 0.9401\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1412 - acc: 0.9410\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1399 - acc: 0.9414\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1397 - acc: 0.9415\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1389 - acc: 0.9421\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1391 - acc: 0.9417\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1374 - acc: 0.9425\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1364 - acc: 0.9430\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1360 - acc: 0.9432\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1345 - acc: 0.9437\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1344 - acc: 0.9438\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1332 - acc: 0.9443\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1329 - acc: 0.9446\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1328 - acc: 0.9443\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1328 - acc: 0.9445\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1320 - acc: 0.9447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73     43282\n",
      "           1       0.71      0.62      0.66     23516\n",
      "           2       0.78      0.77      0.78     39265\n",
      "\n",
      "    accuracy                           0.73    106063\n",
      "   macro avg       0.73      0.72      0.72    106063\n",
      "weighted avg       0.73      0.73      0.73    106063\n",
      "\n",
      "Acurácia\n",
      "0.7164353331461868\n",
      "Precisao\n",
      "0.7328208249363877\n",
      "Recall\n",
      "0.732300613786146\n",
      "F1\n",
      "0.7314319727401591\n",
      "[[32711  4315  6256]\n",
      " [ 6614 14571  2331]\n",
      " [ 7303  1574 30388]]\n",
      "TRAIN: [   0    2    3 ... 1996 1998 1999] TEST: [   1   11   22   36   37   40   46   51   57   58   63   66   68   73\n",
      "   74   75   78   82   83   84   87   88   97   98  106  113  121  124\n",
      "  132  135  138  142  144  145  159  161  165  167  170  172  179  188\n",
      "  190  191  192  193  212  214  217  224  227  231  243  244  245  250\n",
      "  256  267  272  277  282  290  294  307  310  312  324  325  331  336\n",
      "  340  345  346  351  358  359  361  363  365  369  370  371  379  394\n",
      "  395  404  408  414  415  423  443  454  455  463  469  471  475  479\n",
      "  480  485  487  494  498  501  502  504  508  525  529  532  533  536\n",
      "  537  539  547  549  561  563  568  569  588  604  611  624  641  649\n",
      "  652  657  665  666  669  673  674  677  678  679  685  688  689  691\n",
      "  694  698  705  709  711  723  730  731  736  741  743  757  767  778\n",
      "  780  781  799  809  815  822  824  825  826  831  834  838  839  841\n",
      "  843  844  851  857  858  865  882  889  892  914  920  922  931  934\n",
      "  939  944  945  946  952  958  959  960  966  968  976  978  980  982\n",
      "  993  995 1000 1001 1009 1011 1017 1030 1039 1043 1046 1055 1057 1059\n",
      " 1067 1069 1075 1076 1077 1087 1088 1090 1091 1092 1102 1103 1113 1115\n",
      " 1119 1120 1125 1128 1129 1134 1138 1140 1143 1155 1158 1161 1164 1166\n",
      " 1168 1171 1179 1185 1197 1200 1207 1208 1212 1213 1217 1219 1220 1230\n",
      " 1232 1245 1254 1262 1269 1274 1278 1279 1289 1291 1295 1296 1300 1301\n",
      " 1311 1314 1316 1322 1329 1332 1333 1338 1366 1367 1368 1377 1388 1392\n",
      " 1394 1396 1404 1406 1409 1414 1415 1418 1422 1423 1424 1427 1435 1440\n",
      " 1441 1447 1461 1466 1469 1474 1485 1487 1495 1498 1506 1529 1534 1536\n",
      " 1538 1539 1540 1545 1547 1548 1561 1573 1590 1595 1599 1601 1604 1607\n",
      " 1608 1609 1618 1625 1626 1633 1634 1641 1655 1662 1677 1682 1685 1696\n",
      " 1699 1701 1708 1711 1722 1723 1730 1736 1740 1741 1743 1745 1751 1753\n",
      " 1756 1760 1767 1770 1781 1783 1792 1796 1800 1801 1802 1806 1808 1811\n",
      " 1813 1816 1828 1832 1843 1853 1860 1862 1873 1874 1875 1877 1879 1906\n",
      " 1911 1912 1918 1923 1924 1934 1935 1940 1941 1943 1945 1951 1952 1956\n",
      " 1964 1966 1974 1979 1982 1986 1989 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3500 - acc: 0.7428\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3185 - acc: 0.8555\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3148 - acc: 0.8578\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3102 - acc: 0.8593\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3082 - acc: 0.8609\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3063 - acc: 0.8620\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3049 - acc: 0.8627\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3039 - acc: 0.8630\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3004 - acc: 0.8646\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2991 - acc: 0.8659\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2980 - acc: 0.8663\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2947 - acc: 0.8678\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2945 - acc: 0.8683\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2923 - acc: 0.8693\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2924 - acc: 0.8694\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2853 - acc: 0.8725\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2855 - acc: 0.8727\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2816 - acc: 0.8745\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2800 - acc: 0.8757\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2776 - acc: 0.8769\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2732 - acc: 0.8791\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2694 - acc: 0.8816\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2653 - acc: 0.8837\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2602 - acc: 0.8867\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2565 - acc: 0.8886\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2517 - acc: 0.8906\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2554 - acc: 0.8887\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2443 - acc: 0.8945\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2404 - acc: 0.8962\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2385 - acc: 0.8971\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2350 - acc: 0.8990\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2317 - acc: 0.9003\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2285 - acc: 0.9018\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2260 - acc: 0.9028\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2252 - acc: 0.9035\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2210 - acc: 0.9053\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2190 - acc: 0.9063\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2159 - acc: 0.9077\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2144 - acc: 0.9085\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2101 - acc: 0.9105\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2077 - acc: 0.9111\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2062 - acc: 0.9120\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2036 - acc: 0.9133\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2021 - acc: 0.9138\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1987 - acc: 0.9155\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1966 - acc: 0.9164\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1941 - acc: 0.9174\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1917 - acc: 0.9187\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1907 - acc: 0.9191\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1889 - acc: 0.9200\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1867 - acc: 0.9208\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1853 - acc: 0.9213\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1842 - acc: 0.9218\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1824 - acc: 0.9229\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1790 - acc: 0.9243\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1779 - acc: 0.9246\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1759 - acc: 0.9257\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1747 - acc: 0.9263\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1736 - acc: 0.9266\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1712 - acc: 0.9278\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1697 - acc: 0.9284\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1684 - acc: 0.9288\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1672 - acc: 0.9295\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1656 - acc: 0.9301\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1645 - acc: 0.9306\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1639 - acc: 0.9311\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1627 - acc: 0.9313\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1607 - acc: 0.9323\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1601 - acc: 0.9328\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1584 - acc: 0.9333\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1574 - acc: 0.9338\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1562 - acc: 0.9344\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1546 - acc: 0.9352\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1540 - acc: 0.9354\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1531 - acc: 0.9357\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1518 - acc: 0.9362\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1514 - acc: 0.9365\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1509 - acc: 0.9366\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1496 - acc: 0.9370\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1487 - acc: 0.9376\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1472 - acc: 0.9383\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1460 - acc: 0.9389\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1454 - acc: 0.9391\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1456 - acc: 0.9387\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1437 - acc: 0.9397\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1437 - acc: 0.9400\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1419 - acc: 0.9406\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1418 - acc: 0.9406\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1404 - acc: 0.9409\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1403 - acc: 0.9412\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1398 - acc: 0.9415\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1390 - acc: 0.9418\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1376 - acc: 0.9422\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1368 - acc: 0.9424\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1372 - acc: 0.9425\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1373 - acc: 0.9425\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1357 - acc: 0.9432\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1347 - acc: 0.9438\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1342 - acc: 0.9439\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1334 - acc: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.71     39642\n",
      "           1       0.68      0.65      0.66     22137\n",
      "           2       0.75      0.78      0.76     34074\n",
      "\n",
      "    accuracy                           0.72     95853\n",
      "   macro avg       0.72      0.71      0.71     95853\n",
      "weighted avg       0.72      0.72      0.72     95853\n",
      "\n",
      "Acurácia\n",
      "0.7126062700021086\n",
      "Precisao\n",
      "0.7199764081480101\n",
      "Recall\n",
      "0.7208329421092715\n",
      "F1\n",
      "0.7202165760055971\n",
      "[[28266  4925  6451]\n",
      " [ 5469 14317  2351]\n",
      " [ 5774  1789 26511]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   8   17   19   21   24   25   29   30   31   34   35   38   49   54\n",
      "   62   71   72   76   79   96  103  107  112  116  117  131  134  136\n",
      "  140  141  147  150  157  166  168  174  180  183  198  201  202  209\n",
      "  215  220  226  234  240  249  251  254  255  257  258  259  263  273\n",
      "  275  278  283  286  287  289  293  299  300  305  309  311  316  319\n",
      "  321  322  332  333  338  342  348  352  360  376  377  381  386  388\n",
      "  389  393  398  402  411  418  422  426  429  449  450  453  456  462\n",
      "  466  467  468  470  484  489  490  495  497  509  511  513  519  521\n",
      "  530  542  543  544  546  548  556  557  571  574  579  580  583  596\n",
      "  597  598  601  603  612  613  615  622  623  632  633  635  636  637\n",
      "  638  642  646  647  650  658  663  664  680  682  683  684  695  701\n",
      "  707  708  714  720  725  726  727  728  735  744  747  748  749  752\n",
      "  754  758  760  766  768  769  775  788  795  802  808  818  821  829\n",
      "  836  840  861  868  874  876  878  885  890  898  899  900  904  905\n",
      "  909  911  917  924  926  929  935  937  940  942  951  954  955  964\n",
      "  965  967  979  984  988  989  991  996  999 1004 1005 1006 1008 1010\n",
      " 1012 1015 1018 1022 1025 1028 1031 1041 1042 1045 1049 1051 1056 1064\n",
      " 1068 1073 1081 1089 1104 1107 1118 1122 1139 1142 1148 1156 1159 1160\n",
      " 1174 1175 1180 1184 1192 1193 1198 1206 1210 1224 1235 1237 1239 1240\n",
      " 1244 1246 1250 1255 1256 1263 1268 1270 1285 1286 1287 1293 1305 1313\n",
      " 1317 1335 1337 1340 1347 1354 1358 1360 1379 1387 1389 1398 1419 1430\n",
      " 1431 1433 1434 1439 1445 1452 1453 1454 1456 1457 1471 1481 1482 1484\n",
      " 1492 1494 1500 1504 1513 1522 1527 1532 1537 1541 1543 1553 1555 1560\n",
      " 1562 1563 1569 1574 1578 1581 1584 1585 1593 1596 1612 1615 1623 1624\n",
      " 1627 1629 1639 1642 1658 1660 1668 1669 1671 1675 1681 1683 1688 1689\n",
      " 1695 1697 1700 1732 1734 1738 1746 1752 1759 1761 1764 1772 1774 1779\n",
      " 1784 1791 1812 1818 1821 1831 1834 1835 1836 1842 1844 1845 1852 1855\n",
      " 1857 1859 1864 1865 1866 1868 1870 1871 1880 1886 1889 1890 1900 1908\n",
      " 1909 1938 1944 1947 1953 1971 1972 1983]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3433 - acc: 0.7965\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3132 - acc: 0.8577\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3087 - acc: 0.8600\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3051 - acc: 0.8626\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3012 - acc: 0.8648\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2988 - acc: 0.8655\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2968 - acc: 0.8670\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3015 - acc: 0.8637\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2947 - acc: 0.8672\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2920 - acc: 0.8686\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2895 - acc: 0.8701\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2907 - acc: 0.8698\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2883 - acc: 0.8707\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2837 - acc: 0.8726\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2835 - acc: 0.8735\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2812 - acc: 0.8752\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2820 - acc: 0.8745\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2754 - acc: 0.8780\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2743 - acc: 0.8783\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2710 - acc: 0.8802\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2687 - acc: 0.8816\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2669 - acc: 0.8831\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2620 - acc: 0.8856\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2531 - acc: 0.8900\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2503 - acc: 0.8909\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2438 - acc: 0.8947\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2397 - acc: 0.8962\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2367 - acc: 0.8975\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2335 - acc: 0.8992\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2299 - acc: 0.9005\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2273 - acc: 0.9018\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2236 - acc: 0.9036\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2210 - acc: 0.9051\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2174 - acc: 0.9067\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2167 - acc: 0.9067\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2111 - acc: 0.9094\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2115 - acc: 0.9097\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2058 - acc: 0.9118\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2028 - acc: 0.9134\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2011 - acc: 0.9140\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1986 - acc: 0.9151\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1963 - acc: 0.9162\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1943 - acc: 0.9170\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1927 - acc: 0.9180\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1884 - acc: 0.9197\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1870 - acc: 0.9202\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1857 - acc: 0.9209\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1849 - acc: 0.9211\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1822 - acc: 0.9223\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1798 - acc: 0.9234\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1779 - acc: 0.9240\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1765 - acc: 0.9250\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1745 - acc: 0.9256\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1724 - acc: 0.9268\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1707 - acc: 0.9279\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1701 - acc: 0.9278\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1673 - acc: 0.9292\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1671 - acc: 0.9295\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1659 - acc: 0.9298\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1641 - acc: 0.9305\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1631 - acc: 0.9310\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1628 - acc: 0.9312\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1619 - acc: 0.9316\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1597 - acc: 0.9326\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1569 - acc: 0.9338\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1563 - acc: 0.9340\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1538 - acc: 0.9352\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1537 - acc: 0.9350\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1527 - acc: 0.9358\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1522 - acc: 0.9360\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1514 - acc: 0.9361\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1501 - acc: 0.9368\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1498 - acc: 0.9372\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1485 - acc: 0.9377\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1462 - acc: 0.9387\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1455 - acc: 0.9389\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1447 - acc: 0.9393\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1471 - acc: 0.9382\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1446 - acc: 0.9393\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1434 - acc: 0.9397\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1422 - acc: 0.9403\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1427 - acc: 0.9402\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1410 - acc: 0.9408\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1389 - acc: 0.9415\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1392 - acc: 0.9417\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1378 - acc: 0.9422\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1373 - acc: 0.9425\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1365 - acc: 0.9426\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1354 - acc: 0.9432\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1353 - acc: 0.9434\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1340 - acc: 0.9439\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1324 - acc: 0.9446\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1333 - acc: 0.9441\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1320 - acc: 0.9446\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1317 - acc: 0.9447\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1321 - acc: 0.9446\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1297 - acc: 0.9456\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1302 - acc: 0.9452\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1298 - acc: 0.9456\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1279 - acc: 0.9464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.71     43505\n",
      "           1       0.70      0.62      0.66     23658\n",
      "           2       0.73      0.80      0.76     37400\n",
      "\n",
      "    accuracy                           0.72    104563\n",
      "   macro avg       0.72      0.71      0.71    104563\n",
      "weighted avg       0.72      0.72      0.72    104563\n",
      "\n",
      "Acurácia\n",
      "0.7096153633035769\n",
      "Precisao\n",
      "0.7198130242361097\n",
      "Recall\n",
      "0.7207903369260638\n",
      "F1\n",
      "0.7193182051918265\n",
      "[[30750  4715  8040]\n",
      " [ 6075 14747  2836]\n",
      " [ 5795  1734 29871]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1998] TEST: [   3    4    7    9   10   12   14   15   18   20   27   33   39   42\n",
      "   45   47   50   56   61   69   89   93   95   99  101  114  123  127\n",
      "  128  129  130  133  148  151  152  153  155  156  158  163  164  175\n",
      "  176  186  187  199  207  210  211  213  222  223  233  235  237  242\n",
      "  247  248  260  265  268  271  274  276  281  284  285  292  296  297\n",
      "  302  313  314  315  317  318  328  334  335  341  343  347  350  374\n",
      "  378  380  384  387  401  403  405  407  420  427  428  430  432  433\n",
      "  438  440  448  452  460  481  482  488  491  496  506  507  517  523\n",
      "  534  553  559  570  572  575  576  586  587  589  592  594  600  609\n",
      "  614  616  620  625  626  631  639  644  645  661  675  692  706  716\n",
      "  718  722  724  737  740  745  750  756  759  761  765  773  779  783\n",
      "  798  800  803  807  811  812  820  828  832  845  848  850  852  855\n",
      "  856  862  863  870  875  879  880  884  888  891  896  903  907  908\n",
      "  912  913  916  918  919  925  941  953  957  962  975  983  994  997\n",
      "  998 1003 1014 1019 1021 1024 1026 1027 1034 1035 1044 1048 1052 1058\n",
      " 1061 1065 1072 1074 1078 1080 1094 1096 1098 1106 1108 1109 1111 1112\n",
      " 1116 1131 1132 1135 1137 1151 1157 1176 1186 1190 1196 1199 1202 1203\n",
      " 1204 1223 1225 1226 1231 1249 1252 1253 1265 1271 1276 1277 1280 1282\n",
      " 1292 1294 1307 1323 1325 1331 1339 1342 1346 1348 1355 1361 1362 1364\n",
      " 1381 1382 1383 1391 1399 1402 1403 1405 1407 1408 1410 1411 1412 1417\n",
      " 1425 1428 1432 1443 1444 1446 1448 1455 1458 1467 1468 1473 1476 1479\n",
      " 1483 1486 1490 1497 1503 1507 1509 1511 1512 1517 1520 1521 1523 1524\n",
      " 1549 1551 1559 1570 1571 1575 1582 1587 1588 1589 1592 1594 1602 1603\n",
      " 1605 1614 1616 1617 1628 1635 1636 1637 1640 1643 1644 1646 1648 1650\n",
      " 1651 1663 1666 1667 1679 1686 1687 1691 1703 1704 1706 1720 1721 1724\n",
      " 1725 1728 1729 1731 1735 1739 1748 1749 1750 1768 1777 1786 1795 1799\n",
      " 1804 1810 1817 1820 1822 1825 1826 1839 1849 1854 1856 1863 1869 1872\n",
      " 1878 1882 1884 1888 1891 1895 1896 1899 1902 1913 1919 1925 1927 1937\n",
      " 1955 1960 1963 1968 1977 1980 1992 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3433 - acc: 0.8138\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3125 - acc: 0.8587\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3061 - acc: 0.8618\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3024 - acc: 0.8635\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3026 - acc: 0.8639\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2978 - acc: 0.8659\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2975 - acc: 0.8663\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2941 - acc: 0.8672\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2943 - acc: 0.8681\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2895 - acc: 0.8695\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2890 - acc: 0.8701\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2872 - acc: 0.8714\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2854 - acc: 0.8724\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2826 - acc: 0.8746\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2792 - acc: 0.8755\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2768 - acc: 0.8774\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2752 - acc: 0.8785\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2730 - acc: 0.8799\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2696 - acc: 0.8814\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2640 - acc: 0.8847\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2574 - acc: 0.8879\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2529 - acc: 0.8900\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2536 - acc: 0.8899\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2459 - acc: 0.8935\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2433 - acc: 0.8947\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2406 - acc: 0.8960\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2381 - acc: 0.8969\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2393 - acc: 0.8964\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2331 - acc: 0.8996\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2281 - acc: 0.9018\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2265 - acc: 0.9027\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2260 - acc: 0.9028\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2195 - acc: 0.9055\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2189 - acc: 0.9058\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2144 - acc: 0.9081\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2111 - acc: 0.9095\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2098 - acc: 0.9101\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2068 - acc: 0.9116\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2045 - acc: 0.9123\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2018 - acc: 0.9137\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1988 - acc: 0.9152\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1972 - acc: 0.9159\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1966 - acc: 0.9161\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1943 - acc: 0.9174\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1906 - acc: 0.9188\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1873 - acc: 0.9202\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1857 - acc: 0.9210\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1844 - acc: 0.9218\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1818 - acc: 0.9226\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1799 - acc: 0.9238\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1792 - acc: 0.9239\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1783 - acc: 0.9243\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1765 - acc: 0.9250\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1736 - acc: 0.9265\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1725 - acc: 0.9271\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1707 - acc: 0.9277\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1696 - acc: 0.9284\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1694 - acc: 0.9287\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1668 - acc: 0.9299\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1646 - acc: 0.9305\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1635 - acc: 0.9307\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1622 - acc: 0.9315\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1622 - acc: 0.9314\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1610 - acc: 0.9320\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1619 - acc: 0.9319\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1580 - acc: 0.9334\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1569 - acc: 0.9339\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1552 - acc: 0.9345\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1535 - acc: 0.9352\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1521 - acc: 0.9360\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1510 - acc: 0.9364\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1501 - acc: 0.9371\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1493 - acc: 0.9372\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1493 - acc: 0.9370\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1475 - acc: 0.9381\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1467 - acc: 0.9383\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1466 - acc: 0.9385\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1447 - acc: 0.9394\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1443 - acc: 0.9395\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1438 - acc: 0.9397\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1428 - acc: 0.9401\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1418 - acc: 0.9407\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1413 - acc: 0.9405\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1400 - acc: 0.9413\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1392 - acc: 0.9417\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1385 - acc: 0.9417\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1375 - acc: 0.9423\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1365 - acc: 0.9429\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1356 - acc: 0.9431\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1358 - acc: 0.9433\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1352 - acc: 0.9432\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1347 - acc: 0.9434\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1340 - acc: 0.9439\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1340 - acc: 0.9436\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1350 - acc: 0.9432\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1325 - acc: 0.9444\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1310 - acc: 0.9450\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1314 - acc: 0.9446\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1298 - acc: 0.9456\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1291 - acc: 0.9461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72     43144\n",
      "           1       0.71      0.59      0.64     23287\n",
      "           2       0.75      0.78      0.77     40090\n",
      "\n",
      "    accuracy                           0.72    106521\n",
      "   macro avg       0.72      0.70      0.71    106521\n",
      "weighted avg       0.72      0.72      0.72    106521\n",
      "\n",
      "Acurácia\n",
      "0.7013348023832021\n",
      "Precisao\n",
      "0.719609526439172\n",
      "Recall\n",
      "0.7201303029449592\n",
      "F1\n",
      "0.7183889863005013\n",
      "[[31652  3919  7573]\n",
      " [ 6781 13694  2812]\n",
      " [ 6947  1780 31363]]\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_81 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_82 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_83 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_84 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_85 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_86 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_87 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_88 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_89 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 1,523,403\n",
      "Trainable params: 1,523,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Acurácias total\n",
      "[0.7198650072411583, 0.7164353331461868, 0.7126062700021086, 0.7096153633035769, 0.7013348023832021]\n",
      "0.7119713552152466\n",
      "Precision total\n",
      "[0.734712374262338, 0.7328208249363877, 0.7199764081480101, 0.7198130242361097, 0.719609526439172]\n",
      "0.7253864316044035\n",
      "Recalls total\n",
      "[0.7344759162711073, 0.732300613786146, 0.7208329421092715, 0.7207903369260638, 0.7201303029449592]\n",
      "0.7257060224075096\n",
      "F1 total\n",
      "[0.7338409386097668, 0.7314319727401591, 0.7202165760055971, 0.7193182051918265, 0.7183889863005013]\n",
      "0.7246393357695702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede(8)\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classesQ8[train_index],\n",
    "                           previsores[test_index], classesQ8[test_index], 8)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Acurácias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())\n",
    "\n",
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede(3)\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classesQ3[train_index],\n",
    "                           previsores[test_index], classesQ3[test_index], 3)\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "print('Acurácias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
