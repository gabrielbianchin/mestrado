{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 00:36:45.824917  6492 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0821 00:36:45.829909  6492 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0821 00:36:45.831899  6492 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0821 00:36:45.833894  6492 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    2    4 ... 1996 1997 1998] TEST: [   1    3    6    7   16   32   38   41   44   46   47   49   52   72\n",
      "   76   77   94  101  105  112  127  137  141  143  147  148  153  155\n",
      "  157  162  169  185  186  188  190  197  211  220  221  222  232  240\n",
      "  253  256  263  273  280  282  298  299  308  310  313  315  316  320\n",
      "  333  334  339  346  349  354  356  360  362  364  374  375  382  387\n",
      "  391  395  410  411  419  424  425  428  435  436  447  452  454  458\n",
      "  465  469  472  481  482  487  495  496  503  504  511  520  522  524\n",
      "  527  530  536  538  543  544  547  559  560  571  573  574  577  580\n",
      "  581  583  587  592  594  598  600  601  602  623  630  632  643  645\n",
      "  648  660  665  666  671  678  685  687  690  694  698  701  704  709\n",
      "  711  717  723  724  731  744  750  752  754  755  756  758  765  767\n",
      "  782  783  784  785  789  791  802  806  810  817  821  825  829  834\n",
      "  836  843  844  847  871  874  877  880  881  882  883  885  889  898\n",
      "  905  914  919  923  928  931  934  936  942  947  955  958  964  971\n",
      "  972  974  979  989  996  998 1008 1009 1010 1014 1017 1024 1029 1037\n",
      " 1039 1050 1053 1063 1065 1066 1072 1076 1077 1079 1082 1090 1098 1103\n",
      " 1106 1108 1110 1111 1118 1138 1146 1149 1155 1156 1161 1165 1170 1171\n",
      " 1172 1177 1178 1186 1187 1205 1209 1219 1220 1223 1226 1235 1236 1243\n",
      " 1245 1246 1257 1258 1266 1267 1274 1286 1293 1294 1297 1299 1306 1307\n",
      " 1311 1313 1314 1324 1326 1330 1347 1349 1368 1373 1374 1378 1384 1397\n",
      " 1398 1403 1404 1406 1408 1412 1413 1416 1418 1437 1449 1459 1469 1472\n",
      " 1477 1478 1488 1492 1495 1504 1509 1510 1521 1530 1531 1535 1538 1539\n",
      " 1560 1567 1568 1572 1574 1578 1581 1584 1585 1590 1596 1612 1614 1615\n",
      " 1618 1619 1622 1625 1626 1630 1643 1648 1654 1656 1669 1674 1685 1689\n",
      " 1690 1694 1696 1697 1706 1710 1715 1716 1720 1721 1731 1733 1737 1753\n",
      " 1762 1763 1766 1771 1775 1779 1790 1798 1803 1810 1811 1812 1819 1823\n",
      " 1829 1834 1838 1840 1843 1844 1849 1856 1863 1864 1867 1870 1871 1876\n",
      " 1877 1879 1881 1885 1887 1896 1898 1901 1903 1919 1925 1929 1931 1937\n",
      " 1938 1944 1955 1957 1971 1972 1982 1999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 00:36:49.519373  6492 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 37s 23ms/sample - loss: 0.3703 - acc: 0.6866\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3424 - acc: 0.8323\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3292 - acc: 0.8398\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3093 - acc: 0.8565\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3062 - acc: 0.8579\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2979 - acc: 0.8617\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2946 - acc: 0.8652\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2937 - acc: 0.8658\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2907 - acc: 0.8682\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2901 - acc: 0.8683\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2842 - acc: 0.8717\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2839 - acc: 0.8717\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2829 - acc: 0.8719\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2804 - acc: 0.8734\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2805 - acc: 0.8736\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2772 - acc: 0.8753\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2719 - acc: 0.8778\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2709 - acc: 0.8784\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2657 - acc: 0.8809\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2633 - acc: 0.8833\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2626 - acc: 0.8835\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2597 - acc: 0.8851\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2548 - acc: 0.8883\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2507 - acc: 0.8899\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2494 - acc: 0.8910\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2415 - acc: 0.8940\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2387 - acc: 0.8962\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2373 - acc: 0.8966\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2321 - acc: 0.8992\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2264 - acc: 0.9020\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2227 - acc: 0.9036\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2211 - acc: 0.9044\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2142 - acc: 0.9072\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2128 - acc: 0.9080\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2076 - acc: 0.9102\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2036 - acc: 0.9115\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1975 - acc: 0.9137\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1950 - acc: 0.9154\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1913 - acc: 0.9173\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1906 - acc: 0.9175\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1857 - acc: 0.9202\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1829 - acc: 0.9212\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1788 - acc: 0.9229\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1767 - acc: 0.9236\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1737 - acc: 0.9250\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1709 - acc: 0.9262\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1698 - acc: 0.9275\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1672 - acc: 0.9271\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1628 - acc: 0.9303\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1592 - acc: 0.9315\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1581 - acc: 0.9318\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1564 - acc: 0.9319\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1522 - acc: 0.9347\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1498 - acc: 0.9350\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1488 - acc: 0.9356\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1474 - acc: 0.9362\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1454 - acc: 0.9372\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1443 - acc: 0.9373\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1433 - acc: 0.9378\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1417 - acc: 0.9394\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1382 - acc: 0.9403\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1363 - acc: 0.9411\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1352 - acc: 0.9413\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1339 - acc: 0.9417\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1322 - acc: 0.9418\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1306 - acc: 0.9418\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1292 - acc: 0.9441\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1279 - acc: 0.9444\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1258 - acc: 0.9442\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1251 - acc: 0.9448\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1248 - acc: 0.9454\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1217 - acc: 0.9478\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1206 - acc: 0.9478\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1196 - acc: 0.9419\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1186 - acc: 0.9461\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1174 - acc: 0.9455\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1163 - acc: 0.9478\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1161 - acc: 0.9476\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1163 - acc: 0.9478\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1167 - acc: 0.9493\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1136 - acc: 0.9482\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1122 - acc: 0.9506\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1107 - acc: 0.9510\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1092 - acc: 0.9521\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1095 - acc: 0.9513\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1127 - acc: 0.9512\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1131 - acc: 0.9503\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1086 - acc: 0.9518\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1058 - acc: 0.9534\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1045 - acc: 0.9525\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1038 - acc: 0.9524\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1032 - acc: 0.9528\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1020 - acc: 0.9517\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1024 - acc: 0.9451\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1012 - acc: 0.9549\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1000 - acc: 0.9392\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.0983 - acc: 0.9129\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.0973 - acc: 0.9236\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.0990 - acc: 0.9420\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1015 - acc: 0.9499\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70     45057\n",
      "           1       0.64      0.61      0.62     23923\n",
      "           2       0.72      0.72      0.72     40298\n",
      "\n",
      "    accuracy                           0.69    109278\n",
      "   macro avg       0.68      0.68      0.68    109278\n",
      "weighted avg       0.69      0.69      0.69    109278\n",
      "\n",
      "Acur√°cia\n",
      "0.6791665480447882\n",
      "Precisao\n",
      "0.690388887845675\n",
      "Recall\n",
      "0.6909075934771867\n",
      "F1\n",
      "0.6905117179155797\n",
      "[[31792  5170  8095]\n",
      " [ 6303 14557  3063]\n",
      " [ 8139  3007 29152]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [   8   10   11   15   18   19   20   21   23   26   30   31   34   45\n",
      "   56   62   69   82   91   93   95  103  110  113  121  129  131  132\n",
      "  134  145  158  159  163  167  174  177  180  183  184  191  205  207\n",
      "  223  227  228  238  244  248  250  254  264  266  268  271  274  281\n",
      "  284  285  292  294  297  303  304  306  309  322  323  325  332  340\n",
      "  350  351  359  368  372  376  378  379  380  394  396  398  401  402\n",
      "  404  405  407  417  422  432  433  445  448  449  451  460  468  477\n",
      "  480  483  486  494  498  505  506  513  515  516  517  521  540  542\n",
      "  550  551  553  555  562  568  578  590  591  599  603  605  612  624\n",
      "  629  638  640  641  642  646  663  664  668  670  679  680  710  719\n",
      "  725  727  729  739  740  751  761  762  763  778  795  799  801  805\n",
      "  807  811  812  818  820  824  833  842  853  860  861  863  869  875\n",
      "  878  884  888  892  899  901  903  906  911  917  929  937  948  952\n",
      "  957  962  965  970  975  983  986  993 1000 1003 1012 1019 1026 1034\n",
      " 1042 1043 1047 1049 1052 1055 1057 1062 1069 1074 1078 1089 1091 1092\n",
      " 1101 1113 1124 1128 1129 1131 1137 1144 1148 1151 1153 1159 1162 1168\n",
      " 1173 1175 1188 1189 1193 1207 1211 1213 1214 1216 1221 1232 1241 1242\n",
      " 1248 1255 1261 1264 1270 1272 1276 1283 1285 1289 1291 1296 1300 1303\n",
      " 1304 1305 1308 1310 1315 1317 1320 1323 1337 1340 1342 1344 1348 1350\n",
      " 1351 1353 1359 1369 1376 1385 1386 1387 1394 1400 1402 1405 1411 1417\n",
      " 1419 1432 1440 1442 1447 1451 1455 1460 1464 1470 1471 1473 1475 1476\n",
      " 1479 1490 1494 1498 1500 1505 1511 1517 1526 1528 1532 1534 1545 1546\n",
      " 1547 1551 1557 1563 1566 1575 1576 1577 1580 1588 1589 1592 1593 1598\n",
      " 1599 1608 1616 1617 1629 1634 1636 1640 1642 1645 1661 1664 1667 1672\n",
      " 1677 1679 1682 1698 1701 1702 1704 1718 1726 1727 1728 1732 1739 1741\n",
      " 1742 1747 1751 1756 1760 1761 1769 1776 1780 1785 1794 1796 1802 1807\n",
      " 1817 1821 1826 1835 1836 1837 1842 1848 1851 1852 1854 1861 1869 1873\n",
      " 1882 1888 1890 1895 1900 1905 1907 1913 1916 1923 1924 1946 1958 1964\n",
      " 1967 1973 1975 1977 1981 1988 1992 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.3777 - acc: 0.7744\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3521 - acc: 0.8291\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3376 - acc: 0.8373\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3183 - acc: 0.8500\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3079 - acc: 0.8586\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3032 - acc: 0.8615\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3020 - acc: 0.8624\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2983 - acc: 0.8636\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2939 - acc: 0.8663\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2910 - acc: 0.8668\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2915 - acc: 0.8675\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2880 - acc: 0.8693\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2845 - acc: 0.8708\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2841 - acc: 0.8712\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2816 - acc: 0.8724\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2789 - acc: 0.8736\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2770 - acc: 0.8750\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2782 - acc: 0.8743\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2733 - acc: 0.8774\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2682 - acc: 0.8804\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2695 - acc: 0.8792\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2635 - acc: 0.8828\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2600 - acc: 0.8845\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2545 - acc: 0.8873\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2497 - acc: 0.8910\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2478 - acc: 0.8916\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2457 - acc: 0.8927\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2434 - acc: 0.8943\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2352 - acc: 0.8981\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2310 - acc: 0.8999\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2264 - acc: 0.9025\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2240 - acc: 0.9036\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2188 - acc: 0.9060\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2146 - acc: 0.9080\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2101 - acc: 0.9096\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2086 - acc: 0.9109\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2019 - acc: 0.9137\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2003 - acc: 0.9143\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1966 - acc: 0.9162\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1935 - acc: 0.9177\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1886 - acc: 0.9197\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1862 - acc: 0.9206\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1836 - acc: 0.9215\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1803 - acc: 0.9232\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1775 - acc: 0.9240\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1729 - acc: 0.9259\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1708 - acc: 0.9273\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1690 - acc: 0.9280\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1666 - acc: 0.9291\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1643 - acc: 0.9299\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1621 - acc: 0.9307\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1584 - acc: 0.9325\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1576 - acc: 0.9331\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1566 - acc: 0.9334\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1536 - acc: 0.9348\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1516 - acc: 0.9353\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1503 - acc: 0.9361\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1482 - acc: 0.9368\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1458 - acc: 0.9377\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1434 - acc: 0.9388\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1414 - acc: 0.9401\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1401 - acc: 0.9404\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1396 - acc: 0.9408\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1388 - acc: 0.9413\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1360 - acc: 0.9424\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1338 - acc: 0.9436\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1315 - acc: 0.9442\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1299 - acc: 0.9451\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1291 - acc: 0.9454\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1286 - acc: 0.9459\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1280 - acc: 0.9462\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1259 - acc: 0.9470\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1251 - acc: 0.9476\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1225 - acc: 0.9487\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1230 - acc: 0.9481\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1220 - acc: 0.9485\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1213 - acc: 0.9489\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1193 - acc: 0.9498\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1180 - acc: 0.9503\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1169 - acc: 0.9508\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1157 - acc: 0.9513\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1157 - acc: 0.9513\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1132 - acc: 0.9523\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1134 - acc: 0.9521\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1116 - acc: 0.9529\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1109 - acc: 0.9535\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1103 - acc: 0.9533\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1095 - acc: 0.9542\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1084 - acc: 0.9546\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1102 - acc: 0.9537\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1090 - acc: 0.9541\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1071 - acc: 0.9552\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1068 - acc: 0.9552\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1046 - acc: 0.9563\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1041 - acc: 0.9564\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1025 - acc: 0.9570\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1028 - acc: 0.9569\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1016 - acc: 0.9573\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1019 - acc: 0.9572\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1025 - acc: 0.9569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70     41326\n",
      "           1       0.68      0.60      0.63     22631\n",
      "           2       0.73      0.74      0.73     39100\n",
      "\n",
      "    accuracy                           0.70    103057\n",
      "   macro avg       0.70      0.69      0.69    103057\n",
      "weighted avg       0.70      0.70      0.70    103057\n",
      "\n",
      "Acur√°cia\n",
      "0.6855690963543123\n",
      "Precisao\n",
      "0.7003783490860442\n",
      "Recall\n",
      "0.7007384263077714\n",
      "F1\n",
      "0.6997370685662236\n",
      "[[29833  4039  7454]\n",
      " [ 5885 13478  3268]\n",
      " [ 7878  2317 28905]]\n",
      "TRAIN: [   1    2    3 ... 1997 1998 1999] TEST: [   0   13   22   29   50   53   54   55   57   59   61   68   70   73\n",
      "   78   79   81   84   96  100  104  108  109  115  116  122  124  135\n",
      "  139  144  146  150  154  156  160  165  166  168  172  173  175  176\n",
      "  179  187  193  194  195  196  198  203  212  215  216  225  230  234\n",
      "  236  245  246  257  267  276  287  289  290  296  300  312  314  318\n",
      "  335  338  352  361  367  373  406  408  409  413  415  418  429  438\n",
      "  440  441  446  461  463  464  470  471  479  485  489  490  493  497\n",
      "  500  501  510  529  532  539  541  546  548  549  552  557  563  570\n",
      "  572  575  582  584  588  589  593  597  606  613  615  617  622  628\n",
      "  631  634  636  639  647  652  655  657  661  669  672  681  682  683\n",
      "  686  691  693  699  703  716  722  726  728  735  737  738  741  742\n",
      "  743  749  753  766  768  770  771  775  777  787  788  797  803  804\n",
      "  808  809  813  814  819  826  827  837  840  846  848  851  854  857\n",
      "  858  859  864  865  873  886  890  895  897  900  902  907  913  921\n",
      "  922  924  927  935  940  944  945  969  980  981  982  990 1002 1004\n",
      " 1011 1018 1021 1022 1031 1032 1036 1038 1040 1051 1056 1058 1064 1068\n",
      " 1070 1073 1075 1080 1085 1086 1087 1094 1097 1099 1100 1102 1117 1121\n",
      " 1123 1127 1140 1167 1181 1191 1197 1200 1203 1206 1215 1217 1222 1228\n",
      " 1229 1230 1233 1234 1250 1252 1253 1265 1273 1278 1280 1282 1288 1298\n",
      " 1301 1327 1331 1332 1341 1343 1346 1354 1355 1357 1358 1362 1366 1367\n",
      " 1372 1375 1377 1379 1381 1388 1390 1392 1393 1395 1425 1426 1427 1433\n",
      " 1443 1444 1450 1454 1458 1461 1462 1467 1481 1486 1489 1496 1497 1501\n",
      " 1502 1518 1537 1541 1544 1548 1555 1556 1559 1570 1571 1591 1597 1603\n",
      " 1611 1613 1621 1623 1624 1627 1631 1638 1647 1651 1657 1658 1659 1663\n",
      " 1665 1668 1675 1683 1686 1691 1699 1705 1708 1712 1717 1719 1734 1744\n",
      " 1745 1746 1748 1764 1767 1773 1781 1791 1793 1808 1809 1814 1815 1818\n",
      " 1820 1825 1847 1850 1853 1858 1860 1866 1868 1872 1874 1875 1889 1897\n",
      " 1899 1908 1914 1918 1922 1935 1940 1947 1950 1952 1956 1960 1965 1970\n",
      " 1974 1978 1979 1980 1987 1991 1993 1996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.3799 - acc: 0.7491\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3514 - acc: 0.8302\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3350 - acc: 0.8414\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3184 - acc: 0.8536\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3097 - acc: 0.8593\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3047 - acc: 0.8617\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3015 - acc: 0.8629\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2988 - acc: 0.8648\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2969 - acc: 0.8659\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2930 - acc: 0.8671\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2915 - acc: 0.8686\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2909 - acc: 0.8688\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2872 - acc: 0.8706\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2843 - acc: 0.8717\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2816 - acc: 0.8728\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2802 - acc: 0.8743\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2773 - acc: 0.8756\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2753 - acc: 0.8771\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2724 - acc: 0.8788\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2718 - acc: 0.8786\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2659 - acc: 0.8819\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2608 - acc: 0.8840\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2589 - acc: 0.8844\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2530 - acc: 0.8884\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2536 - acc: 0.8878\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2496 - acc: 0.8893\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2430 - acc: 0.8929\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2382 - acc: 0.8949\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2326 - acc: 0.8974\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2272 - acc: 0.8988\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2239 - acc: 0.9010\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2239 - acc: 0.9010\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2151 - acc: 0.9043\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2151 - acc: 0.9064\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2107 - acc: 0.9059\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2059 - acc: 0.9093\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2020 - acc: 0.9115\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2005 - acc: 0.9113\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1949 - acc: 0.9140\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1918 - acc: 0.9156\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1870 - acc: 0.9166\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1833 - acc: 0.9195\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1815 - acc: 0.9194\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1770 - acc: 0.9214\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1763 - acc: 0.9233\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1725 - acc: 0.9229\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1706 - acc: 0.9245\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1680 - acc: 0.9260\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1656 - acc: 0.9268\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1635 - acc: 0.9269\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1600 - acc: 0.9270\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1573 - acc: 0.9308\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1560 - acc: 0.9325\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1537 - acc: 0.9322\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1556 - acc: 0.9302\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1573 - acc: 0.9278\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1505 - acc: 0.9328\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1457 - acc: 0.9327\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1437 - acc: 0.9349\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1416 - acc: 0.9363\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1400 - acc: 0.9353\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1385 - acc: 0.9357\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1357 - acc: 0.9384\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1353 - acc: 0.9386\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1384 - acc: 0.9346\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1352 - acc: 0.9341\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1330 - acc: 0.9358\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1308 - acc: 0.9388\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1293 - acc: 0.9383\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1280 - acc: 0.9393\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1257 - acc: 0.8800\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1259 - acc: 0.8676\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1255 - acc: 0.8910\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1221 - acc: 0.8776\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1207 - acc: 0.8571\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1202 - acc: 0.8850\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1188 - acc: 0.9030\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1178 - acc: 0.8823\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1170 - acc: 0.8207\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1162 - acc: 0.8705\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1140 - acc: 0.8347\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1145 - acc: 0.8736\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1141 - acc: 0.8986\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1124 - acc: 0.9062\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1113 - acc: 0.9033\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1111 - acc: 0.9069\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1132 - acc: 0.8979\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1108 - acc: 0.8194\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1093 - acc: 0.9370\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1084 - acc: 0.9366\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1075 - acc: 0.9235\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1069 - acc: 0.9269\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1070 - acc: 0.9379\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1060 - acc: 0.9234\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1034 - acc: 0.8940\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1020 - acc: 0.9187\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1027 - acc: 0.9284\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1017 - acc: 0.9276\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1017 - acc: 0.9458\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1009 - acc: 0.9462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.70     40826\n",
      "           1       0.62      0.65      0.63     22244\n",
      "           2       0.74      0.74      0.74     37114\n",
      "\n",
      "    accuracy                           0.70    100184\n",
      "   macro avg       0.69      0.69      0.69    100184\n",
      "weighted avg       0.70      0.70      0.70    100184\n",
      "\n",
      "Acur√°cia\n",
      "0.6913342147390655\n",
      "Precisao\n",
      "0.6989045385758913\n",
      "Recall\n",
      "0.6982352471452528\n",
      "F1\n",
      "0.6985180217110422\n",
      "[[28314  5666  6846]\n",
      " [ 5148 14356  2740]\n",
      " [ 6866  2966 27282]]\n",
      "TRAIN: [   0    1    3 ... 1997 1998 1999] TEST: [   2    5   14   24   27   33   40   42   51   63   64   65   66   83\n",
      "   85   86   87   89   90   97   98   99  102  106  118  119  123  125\n",
      "  130  138  142  164  170  171  178  181  192  199  202  206  209  210\n",
      "  214  217  219  226  229  231  233  235  237  239  241  249  258  259\n",
      "  269  277  286  288  291  295  301  302  305  311  321  326  327  329\n",
      "  330  331  336  341  342  344  345  347  348  353  355  357  363  371\n",
      "  383  385  386  397  412  414  416  421  423  430  431  439  443  453\n",
      "  455  459  467  474  475  476  491  492  499  509  512  514  518  519\n",
      "  525  531  535  545  554  556  561  565  566  567  579  586  604  607\n",
      "  611  614  616  618  619  620  621  626  627  633  635  644  649  651\n",
      "  654  658  662  674  675  677  688  702  705  708  713  714  715  718\n",
      "  720  730  733  736  745  746  747  759  760  764  772  773  779  781\n",
      "  792  800  816  823  832  838  841  845  849  850  856  868  872  879\n",
      "  891  896  908  909  916  918  920  926  938  939  949  953  954  956\n",
      "  959  961  967  976  984  992  994 1001 1013 1016 1020 1033 1041 1044\n",
      " 1045 1061 1071 1084 1093 1096 1105 1107 1109 1116 1122 1125 1130 1132\n",
      " 1133 1134 1136 1143 1145 1150 1152 1154 1158 1160 1163 1176 1180 1182\n",
      " 1183 1185 1194 1199 1201 1202 1210 1212 1224 1225 1227 1231 1237 1238\n",
      " 1240 1244 1247 1249 1251 1256 1260 1263 1268 1269 1275 1279 1281 1290\n",
      " 1295 1309 1318 1321 1322 1328 1339 1356 1360 1363 1370 1380 1382 1391\n",
      " 1396 1399 1409 1414 1420 1421 1423 1424 1431 1434 1435 1438 1445 1448\n",
      " 1453 1457 1465 1468 1482 1485 1487 1503 1506 1507 1512 1513 1514 1515\n",
      " 1519 1520 1522 1524 1525 1527 1529 1540 1542 1543 1550 1553 1554 1562\n",
      " 1564 1565 1569 1573 1582 1583 1586 1602 1606 1607 1609 1633 1635 1637\n",
      " 1641 1644 1652 1655 1660 1662 1671 1673 1676 1680 1681 1687 1692 1693\n",
      " 1695 1711 1722 1723 1729 1735 1738 1743 1752 1754 1755 1757 1765 1768\n",
      " 1772 1774 1784 1787 1788 1789 1800 1801 1805 1822 1827 1830 1839 1845\n",
      " 1855 1857 1859 1862 1865 1878 1886 1892 1893 1902 1912 1926 1928 1934\n",
      " 1939 1942 1945 1949 1954 1968 1976 1985]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.3762 - acc: 0.7675\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3479 - acc: 0.8335\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3342 - acc: 0.8425\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3167 - acc: 0.8541\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3085 - acc: 0.8588\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3034 - acc: 0.8619\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2990 - acc: 0.8641\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2970 - acc: 0.8648\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3013 - acc: 0.8619\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2944 - acc: 0.8662\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2893 - acc: 0.8689\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2909 - acc: 0.8681\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2880 - acc: 0.8699\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2860 - acc: 0.8712\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2840 - acc: 0.8720\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2789 - acc: 0.8748\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2782 - acc: 0.8750\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2737 - acc: 0.8776\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2708 - acc: 0.8789\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2674 - acc: 0.8805\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2680 - acc: 0.8802\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2611 - acc: 0.8846\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2578 - acc: 0.8858\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2538 - acc: 0.8878\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2520 - acc: 0.8875\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2440 - acc: 0.8924\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2400 - acc: 0.8942\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2388 - acc: 0.8952\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2342 - acc: 0.8967\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2288 - acc: 0.8997\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2255 - acc: 0.9009\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2188 - acc: 0.9053\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2185 - acc: 0.9045\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2140 - acc: 0.9079\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2102 - acc: 0.9091\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2061 - acc: 0.9115\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1981 - acc: 0.9149\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1971 - acc: 0.9151\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1939 - acc: 0.9146\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1905 - acc: 0.9178\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1876 - acc: 0.9186\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1841 - acc: 0.9203\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1797 - acc: 0.9225\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1773 - acc: 0.9235\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1755 - acc: 0.9235\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1740 - acc: 0.9252\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1706 - acc: 0.9251\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1656 - acc: 0.9271\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1653 - acc: 0.9274\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1622 - acc: 0.9298\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1610 - acc: 0.9293\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1566 - acc: 0.9312\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1543 - acc: 0.9329\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1518 - acc: 0.9332\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1503 - acc: 0.9339\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1475 - acc: 0.9352\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1457 - acc: 0.9356\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1476 - acc: 0.9353\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1441 - acc: 0.9370\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1418 - acc: 0.9381\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1379 - acc: 0.9389\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1390 - acc: 0.9385\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1372 - acc: 0.9392\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1342 - acc: 0.9413\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1337 - acc: 0.9397\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1335 - acc: 0.9409\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1310 - acc: 0.9423\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1295 - acc: 0.9421\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1267 - acc: 0.9425\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1267 - acc: 0.9422\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1259 - acc: 0.9432\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1230 - acc: 0.9439\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1245 - acc: 0.9430\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1217 - acc: 0.9440\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1204 - acc: 0.9447\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1212 - acc: 0.9444\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1187 - acc: 0.9461\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1168 - acc: 0.9468\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1164 - acc: 0.9474\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1146 - acc: 0.9475\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1131 - acc: 0.9482\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1138 - acc: 0.9489\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1135 - acc: 0.9481\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1110 - acc: 0.9492\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1093 - acc: 0.9504\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1088 - acc: 0.9501\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1080 - acc: 0.9513\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1084 - acc: 0.9514\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1071 - acc: 0.9518\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1051 - acc: 0.9529\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1045 - acc: 0.9528\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1060 - acc: 0.9518\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1033 - acc: 0.9539\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1027 - acc: 0.9540\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1046 - acc: 0.9527\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1041 - acc: 0.9529\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1019 - acc: 0.9546\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1011 - acc: 0.9547\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1007 - acc: 0.9541\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1004 - acc: 0.9537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.73      0.71     42147\n",
      "           1       0.67      0.60      0.64     23112\n",
      "           2       0.72      0.72      0.72     36649\n",
      "\n",
      "    accuracy                           0.70    101908\n",
      "   macro avg       0.69      0.68      0.69    101908\n",
      "weighted avg       0.70      0.70      0.70    101908\n",
      "\n",
      "Acur√°cia\n",
      "0.6845891333881603\n",
      "Precisao\n",
      "0.6972015120158818\n",
      "Recall\n",
      "0.6976684853004671\n",
      "F1\n",
      "0.6967988073561013\n",
      "[[30602  4304  7241]\n",
      " [ 6071 13936  3105]\n",
      " [ 7651  2438 26560]]\n",
      "TRAIN: [   0    1    2 ... 1996 1998 1999] TEST: [   4    9   12   17   25   28   35   36   37   39   43   48   58   60\n",
      "   67   71   74   75   80   88   92  107  111  114  117  120  126  128\n",
      "  133  136  140  149  151  152  161  182  189  200  201  204  208  213\n",
      "  218  224  242  243  247  251  252  255  260  261  262  265  270  272\n",
      "  275  278  279  283  293  307  317  319  324  328  337  343  358  365\n",
      "  366  369  370  377  381  384  388  389  390  392  393  399  400  403\n",
      "  420  426  427  434  437  442  444  450  456  457  462  466  473  478\n",
      "  484  488  502  507  508  523  526  528  533  534  537  558  564  569\n",
      "  576  585  595  596  608  609  610  625  637  650  653  656  659  667\n",
      "  673  676  684  689  692  695  696  697  700  706  707  712  721  732\n",
      "  734  748  757  769  774  776  780  786  790  793  794  796  798  815\n",
      "  822  828  830  831  835  839  852  855  862  866  867  870  876  887\n",
      "  893  894  904  910  912  915  925  930  932  933  941  943  946  950\n",
      "  951  960  963  966  968  973  977  978  985  987  988  991  995  997\n",
      "  999 1005 1006 1007 1015 1023 1025 1027 1028 1030 1035 1046 1048 1054\n",
      " 1059 1060 1067 1081 1083 1088 1095 1104 1112 1114 1115 1119 1120 1126\n",
      " 1135 1139 1141 1142 1147 1157 1164 1166 1169 1174 1179 1184 1190 1192\n",
      " 1195 1196 1198 1204 1208 1218 1239 1254 1259 1262 1271 1277 1284 1287\n",
      " 1292 1302 1312 1316 1319 1325 1329 1333 1334 1335 1336 1338 1345 1352\n",
      " 1361 1364 1365 1371 1383 1389 1401 1407 1410 1415 1422 1428 1429 1430\n",
      " 1436 1439 1441 1446 1452 1456 1463 1466 1474 1480 1483 1484 1491 1493\n",
      " 1499 1508 1516 1523 1533 1536 1549 1552 1558 1561 1579 1587 1594 1595\n",
      " 1600 1601 1604 1605 1610 1620 1628 1632 1639 1646 1649 1650 1653 1666\n",
      " 1670 1678 1684 1688 1700 1703 1707 1709 1713 1714 1724 1725 1730 1736\n",
      " 1740 1749 1750 1758 1759 1770 1777 1778 1782 1783 1786 1792 1795 1797\n",
      " 1799 1804 1806 1813 1816 1824 1828 1831 1832 1833 1841 1846 1880 1883\n",
      " 1884 1891 1894 1904 1906 1909 1910 1911 1915 1917 1920 1921 1927 1930\n",
      " 1932 1933 1936 1941 1943 1948 1951 1953 1959 1961 1962 1963 1966 1969\n",
      " 1983 1984 1986 1989 1990 1994 1995 1997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.3817 - acc: 0.6889\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3531 - acc: 0.8257\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.3404 - acc: 0.8368\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3192 - acc: 0.8528\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3081 - acc: 0.8593\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3074 - acc: 0.8593\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3020 - acc: 0.8581\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3005 - acc: 0.8625\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2958 - acc: 0.8655\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2927 - acc: 0.8668\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2927 - acc: 0.8670\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2893 - acc: 0.8690\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2872 - acc: 0.8700\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2857 - acc: 0.8708\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2801 - acc: 0.8738\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2796 - acc: 0.8750\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2779 - acc: 0.8754\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2738 - acc: 0.8770\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2714 - acc: 0.8784\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2699 - acc: 0.8799\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2700 - acc: 0.8805\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2649 - acc: 0.8828\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2590 - acc: 0.8852\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2556 - acc: 0.8883\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2540 - acc: 0.8878\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2508 - acc: 0.8906\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.2458 - acc: 0.8922\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2426 - acc: 0.8948\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2373 - acc: 0.8966\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2317 - acc: 0.8996\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2292 - acc: 0.9004\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2274 - acc: 0.9018\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2202 - acc: 0.9045\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2160 - acc: 0.9050\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2124 - acc: 0.9078\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2098 - acc: 0.9090\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2047 - acc: 0.9087\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1991 - acc: 0.9120\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1987 - acc: 0.9141\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1943 - acc: 0.9159\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1909 - acc: 0.9175\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1873 - acc: 0.9190\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1853 - acc: 0.9202\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1825 - acc: 0.9221\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1791 - acc: 0.9227\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1749 - acc: 0.9243\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1716 - acc: 0.9265\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.1694 - acc: 0.9273\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1689 - acc: 0.9277\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1648 - acc: 0.9295\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1611 - acc: 0.9298\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1592 - acc: 0.9320\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1568 - acc: 0.9329\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1562 - acc: 0.9327\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1529 - acc: 0.9346\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1500 - acc: 0.9359\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1523 - acc: 0.9351\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1525 - acc: 0.9354\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1465 - acc: 0.9373\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1429 - acc: 0.9384\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1411 - acc: 0.9381\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1418 - acc: 0.9378\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1396 - acc: 0.9405\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1352 - acc: 0.9422\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1341 - acc: 0.9429\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1354 - acc: 0.9427\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1344 - acc: 0.9430\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1308 - acc: 0.9447\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1293 - acc: 0.9453\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1262 - acc: 0.9467\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1256 - acc: 0.9471\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1249 - acc: 0.9475\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1249 - acc: 0.9467\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1214 - acc: 0.9487\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1206 - acc: 0.9489\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1203 - acc: 0.9482\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1212 - acc: 0.9485\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1192 - acc: 0.9497\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1192 - acc: 0.9495\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1705 - acc: 0.9296\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1738 - acc: 0.9261\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1507 - acc: 0.9363\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1301 - acc: 0.9435\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1212 - acc: 0.9471\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1168 - acc: 0.9497\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1133 - acc: 0.9513\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1116 - acc: 0.9522\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1095 - acc: 0.9529\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1090 - acc: 0.9522\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1093 - acc: 0.9518\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1073 - acc: 0.9526\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1062 - acc: 0.9530\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1056 - acc: 0.9539\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1079 - acc: 0.9534\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1041 - acc: 0.9553\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1030 - acc: 0.9552\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1017 - acc: 0.9560\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1160 - acc: 0.9504\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1196 - acc: 0.9498\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1063 - acc: 0.9551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.70     42242\n",
      "           1       0.65      0.60      0.62     23286\n",
      "           2       0.70      0.74      0.71     35853\n",
      "\n",
      "    accuracy                           0.69    101381\n",
      "   macro avg       0.68      0.68      0.68    101381\n",
      "weighted avg       0.69      0.69      0.69    101381\n",
      "\n",
      "Acur√°cia\n",
      "0.6760656411580231\n",
      "Precisao\n",
      "0.6857049950445606\n",
      "Recall\n",
      "0.6864895789151814\n",
      "F1\n",
      "0.6856866312559341\n",
      "[[29241  4951  8050]\n",
      " [ 5793 13976  3517]\n",
      " [ 6791  2682 26380]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_40 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_41 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_42 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_43 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_44 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_45 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_46 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_47 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_48 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_49 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 2,272,603\n",
      "Trainable params: 2,272,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.6791665480447882, 0.6855690963543123, 0.6913342147390655, 0.6845891333881603, 0.6760656411580231]\n",
      "0.6833449267368699\n",
      "Precision total\n",
      "[0.690388887845675, 0.7003783490860442, 0.6989045385758913, 0.6972015120158818, 0.6857049950445606]\n",
      "0.6945156565136106\n",
      "Recalls total\n",
      "[0.6909075934771867, 0.7007384263077714, 0.6982352471452528, 0.6976684853004671, 0.6864895789151814]\n",
      "0.6948078662291719\n",
      "F1 total\n",
      "[0.6905117179155797, 0.6997370685662236, 0.6985180217110422, 0.6967988073561013, 0.6856866312559341]\n",
      "0.6942504493609762\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
