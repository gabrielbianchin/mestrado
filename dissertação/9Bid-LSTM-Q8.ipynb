{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 11:46:05.495461  3000 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 11:46:05.501445  3000 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 11:46:05.502442  3000 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 11:46:05.504437  3000 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    2    3 ... 1997 1998 1999] TEST: [   1   10   20   29   32   33   37   47   51   53   57   58   69   73\n",
      "   79   80   83   84   85   86   88   92   99  103  107  109  113  114\n",
      "  115  121  122  125  127  131  149  151  154  161  166  170  174  182\n",
      "  184  185  190  202  203  209  220  230  234  243  245  246  250  252\n",
      "  259  263  279  281  293  307  311  324  330  336  338  342  344  346\n",
      "  351  357  371  380  382  387  393  401  411  412  415  420  430  434\n",
      "  436  440  442  450  456  457  459  461  475  484  488  498  499  504\n",
      "  508  509  519  523  531  536  540  542  544  556  558  559  560  563\n",
      "  567  571  582  586  592  593  596  604  608  609  619  623  624  636\n",
      "  637  641  643  645  650  657  658  668  669  677  682  685  693  696\n",
      "  710  711  717  724  725  728  739  741  750  752  754  760  769  774\n",
      "  775  778  779  785  788  789  807  808  819  820  821  822  831  832\n",
      "  833  836  838  845  847  860  872  875  878  886  891  903  910  921\n",
      "  929  931  935  939  941  946  948  954  964  968  969  973  975  977\n",
      "  979  980  983  989  993  997 1006 1007 1009 1015 1018 1021 1024 1027\n",
      " 1034 1037 1038 1039 1041 1043 1045 1046 1047 1052 1066 1074 1078 1079\n",
      " 1083 1086 1090 1093 1099 1102 1110 1113 1114 1117 1121 1122 1132 1133\n",
      " 1139 1150 1159 1165 1172 1189 1195 1204 1207 1218 1228 1235 1236 1242\n",
      " 1250 1256 1257 1260 1266 1267 1280 1281 1285 1288 1298 1309 1310 1326\n",
      " 1332 1340 1347 1348 1355 1364 1368 1375 1378 1380 1382 1385 1390 1391\n",
      " 1396 1397 1412 1413 1423 1425 1427 1441 1443 1444 1448 1449 1452 1457\n",
      " 1464 1465 1469 1475 1482 1485 1488 1494 1514 1517 1521 1523 1533 1544\n",
      " 1549 1553 1555 1560 1562 1569 1577 1592 1595 1609 1610 1611 1616 1619\n",
      " 1621 1631 1638 1644 1645 1658 1659 1663 1665 1673 1684 1686 1687 1695\n",
      " 1697 1710 1711 1719 1723 1736 1737 1741 1751 1752 1754 1756 1757 1758\n",
      " 1763 1776 1779 1783 1788 1805 1812 1813 1816 1817 1820 1822 1825 1832\n",
      " 1833 1836 1845 1865 1866 1872 1873 1874 1886 1888 1896 1901 1905 1906\n",
      " 1909 1910 1911 1912 1915 1917 1923 1924 1933 1934 1937 1939 1970 1973\n",
      " 1976 1977 1979 1980 1984 1985 1989 1990]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 11:46:07.913779  3000 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.6389 - acc: 0.1217\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6263 - acc: 0.1191\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6254 - acc: 0.1200\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6231 - acc: 0.1227\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6228 - acc: 0.1229\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6229 - acc: 0.1222\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6220 - acc: 0.1230\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6217 - acc: 0.1230\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6214 - acc: 0.1231\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6215 - acc: 0.1231\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6219 - acc: 0.1223\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6214 - acc: 0.1231\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6213 - acc: 0.1230\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6209 - acc: 0.1231\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6208 - acc: 0.1231\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6210 - acc: 0.1232\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6208 - acc: 0.1232\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6206 - acc: 0.1232\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6207 - acc: 0.1232\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6206 - acc: 0.1234\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6226 - acc: 0.1218\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6215 - acc: 0.1226\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6212 - acc: 0.1228\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6210 - acc: 0.1229\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6207 - acc: 0.1231\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6209 - acc: 0.1231\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6206 - acc: 0.1231\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6204 - acc: 0.1232\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6204 - acc: 0.1232\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6206 - acc: 0.1233\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1237\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6222 - acc: 0.1221\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6208 - acc: 0.1227\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6206 - acc: 0.1230\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6207 - acc: 0.1231\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6205 - acc: 0.1231\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6205 - acc: 0.1231\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6205 - acc: 0.1231\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6204 - acc: 0.1231\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1231\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1232\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6202 - acc: 0.1232\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6202 - acc: 0.1232\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6202 - acc: 0.1232\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1231\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6202 - acc: 0.1232\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1231\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6200 - acc: 0.1232\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1231\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6200 - acc: 0.1231\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1232\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1232\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1232\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1232\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6200 - acc: 0.1232\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6200 - acc: 0.1232\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1232\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1232\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1232\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1232\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1232\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1232\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1231\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1232\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6200 - acc: 0.1232\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1232\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1232\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1232\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1232\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6200 - acc: 0.1232\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1231\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1232\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1232\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1232\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6194 - acc: 0.1232\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6194 - acc: 0.1232\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1232\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1194\n",
      "           1       0.30      0.02      0.04     21343\n",
      "           2       0.00      0.00      0.00      4054\n",
      "           3       0.34      0.98      0.51     34816\n",
      "           4       0.00      0.00      0.00       766\n",
      "           5       0.58      0.05      0.08     20452\n",
      "           6       0.00      0.00      0.00      8868\n",
      "           7       0.00      0.00      0.00     11647\n",
      "\n",
      "    accuracy                           0.34    103140\n",
      "   macro avg       0.15      0.13      0.08    103140\n",
      "weighted avg       0.29      0.34      0.20    103140\n",
      "\n",
      "Acurácia\n",
      "0.13111130544670405\n",
      "Precisao\n",
      "0.2928417201968834\n",
      "Recall\n",
      "0.3447934845840605\n",
      "F1\n",
      "0.1964802600922913\n",
      "[[    0    27     0  1143     0    24     0     0]\n",
      " [    0   480     0 20597     0   266     0     0]\n",
      " [    0    69     0  3949     0    36     0     0]\n",
      " [    0   459     0 34152     0   205     0     0]\n",
      " [    0     1     0   765     0     0     0     0]\n",
      " [    0   345     0 19177     0   930     0     0]\n",
      " [    0   104     0  8700     0    64     0     0]\n",
      " [    0   111     0 11461     0    75     0     0]]\n",
      "TRAIN: [   0    1    2 ... 1995 1997 1998] TEST: [   7   26   38   45   46   49   55   65   66   71   82   87   91   96\n",
      "  100  104  105  108  112  116  118  130  139  145  147  148  153  165\n",
      "  178  180  188  191  208  225  229  233  235  260  266  271  276  277\n",
      "  288  296  298  300  301  302  303  310  312  316  319  323  329  334\n",
      "  343  345  359  363  366  368  374  378  384  394  405  406  407  410\n",
      "  417  419  438  441  452  453  458  460  463  468  479  480  485  486\n",
      "  495  500  501  506  511  517  520  529  532  534  537  539  543  548\n",
      "  551  557  561  565  566  568  574  577  581  583  600  626  628  630\n",
      "  631  638  639  642  652  655  660  661  662  664  665  674  683  684\n",
      "  694  698  702  707  708  719  723  726  729  734  736  737  738  748\n",
      "  751  753  756  758  762  766  768  772  780  782  783  787  790  793\n",
      "  796  797  814  815  818  837  839  840  848  854  867  874  877  879\n",
      "  890  893  899  900  902  905  909  916  919  920  923  955  959  974\n",
      "  984 1005 1010 1026 1030 1044 1051 1056 1057 1060 1065 1068 1069 1073\n",
      " 1076 1077 1080 1081 1084 1085 1087 1089 1094 1098 1105 1111 1112 1120\n",
      " 1124 1126 1135 1140 1144 1149 1152 1154 1158 1166 1170 1175 1176 1183\n",
      " 1185 1190 1191 1193 1201 1203 1208 1212 1214 1216 1217 1222 1223 1231\n",
      " 1232 1243 1248 1252 1261 1262 1268 1278 1282 1286 1289 1297 1299 1303\n",
      " 1304 1306 1311 1312 1331 1333 1339 1346 1349 1354 1357 1358 1360 1363\n",
      " 1365 1366 1370 1371 1373 1392 1399 1402 1404 1405 1406 1408 1410 1414\n",
      " 1417 1419 1420 1422 1428 1430 1432 1435 1437 1447 1450 1453 1461 1462\n",
      " 1466 1473 1474 1479 1489 1491 1492 1493 1497 1502 1503 1505 1506 1512\n",
      " 1529 1531 1537 1541 1545 1546 1551 1552 1563 1566 1570 1574 1578 1585\n",
      " 1588 1597 1603 1605 1614 1632 1637 1639 1646 1667 1672 1678 1688 1689\n",
      " 1692 1696 1701 1707 1709 1712 1715 1716 1722 1726 1729 1731 1738 1739\n",
      " 1744 1745 1748 1764 1765 1766 1769 1771 1782 1787 1794 1800 1801 1803\n",
      " 1808 1809 1827 1831 1835 1846 1849 1855 1857 1867 1871 1876 1883 1885\n",
      " 1892 1897 1899 1903 1908 1920 1929 1932 1944 1951 1956 1957 1958 1964\n",
      " 1966 1967 1969 1975 1983 1993 1996 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.6484 - acc: 0.1274\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6367 - acc: 0.1249\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6345 - acc: 0.1261\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6337 - acc: 0.1258\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6343 - acc: 0.1260\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6331 - acc: 0.1265\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6321 - acc: 0.1267\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6318 - acc: 0.1267\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6316 - acc: 0.1268\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6316 - acc: 0.1268\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6315 - acc: 0.1268\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6314 - acc: 0.1267\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6314 - acc: 0.1268\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6313 - acc: 0.1268\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6314 - acc: 0.1270\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6316 - acc: 0.1270\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6312 - acc: 0.1269\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6309 - acc: 0.1272\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6313 - acc: 0.1275\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6319 - acc: 0.1263\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6315 - acc: 0.1265\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6312 - acc: 0.1265\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6311 - acc: 0.1268\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6311 - acc: 0.1271\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6306 - acc: 0.1277\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6309 - acc: 0.1269\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6310 - acc: 0.1269\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6311 - acc: 0.1265\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6311 - acc: 0.1268\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6299 - acc: 0.1286\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6310 - acc: 0.1266\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6311 - acc: 0.1268\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6310 - acc: 0.1267\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6307 - acc: 0.1268\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6308 - acc: 0.1267\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6309 - acc: 0.1268\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6306 - acc: 0.1266\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6305 - acc: 0.1268\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6308 - acc: 0.1268\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6306 - acc: 0.1267\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6304 - acc: 0.1268\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6305 - acc: 0.1268\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6304 - acc: 0.1267\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6304 - acc: 0.1268\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6305 - acc: 0.1268\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1267\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6306 - acc: 0.1267\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6305 - acc: 0.1268\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1269\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6304 - acc: 0.1268\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6304 - acc: 0.1269\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1268\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1268\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1268\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6305 - acc: 0.1268\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6304 - acc: 0.1268\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1268\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1268\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6310 - acc: 0.1267\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6310 - acc: 0.1267\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6307 - acc: 0.1268\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6307 - acc: 0.1268\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6305 - acc: 0.1268\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1268\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6306 - acc: 0.1267\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6301 - acc: 0.1268\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1268\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6303 - acc: 0.1268\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6301 - acc: 0.1268\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6300 - acc: 0.1268\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6300 - acc: 0.1268\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6301 - acc: 0.1268\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6301 - acc: 0.1268\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6300 - acc: 0.1268\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6301 - acc: 0.1268\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6299 - acc: 0.1268\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6301 - acc: 0.1268\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6301 - acc: 0.1268\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1268\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6302 - acc: 0.1269\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6299 - acc: 0.1268\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6300 - acc: 0.1269\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6299 - acc: 0.1269\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6299 - acc: 0.1269\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6299 - acc: 0.1266\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6295 - acc: 0.1273\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6292 - acc: 0.1292\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6301 - acc: 0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1180\n",
      "           1       0.33      0.02      0.04     20351\n",
      "           2       0.00      0.00      0.00      3757\n",
      "           3       0.33      0.99      0.49     30589\n",
      "           4       0.00      0.00      0.00       605\n",
      "           5       0.59      0.05      0.09     19461\n",
      "           6       0.00      0.00      0.00      8833\n",
      "           7       0.00      0.00      0.00     10800\n",
      "\n",
      "    accuracy                           0.33     95576\n",
      "   macro avg       0.16      0.13      0.08     95576\n",
      "weighted avg       0.30      0.33      0.18     95576\n",
      "\n",
      "Acurácia\n",
      "0.13181064114993213\n",
      "Precisao\n",
      "0.29577267681108343\n",
      "Recall\n",
      "0.329748053904746\n",
      "F1\n",
      "0.18265066780756917\n",
      "[[    0    13     0  1141     0    26     0     0]\n",
      " [    0   395     0 19672     0   284     0     0]\n",
      " [    0    45     0  3672     0    40     0     0]\n",
      " [    0   278     0 30175     0   136     0     0]\n",
      " [    0     0     0   605     0     0     0     0]\n",
      " [    0   288     0 18227     0   946     0     0]\n",
      " [    0    88     0  8658     0    87     0     0]\n",
      " [    0    80     0 10645     0    75     0     0]]\n",
      "TRAIN: [   1    2    4 ... 1994 1996 1999] TEST: [   0    3    5    8   19   27   28   30   31   43   44   52   61   62\n",
      "   63   68   70   75   95   97  101  102  110  111  119  120  126  133\n",
      "  134  138  142  150  155  160  163  167  169  172  173  176  194  213\n",
      "  214  219  227  232  238  247  248  255  267  268  274  275  278  282\n",
      "  286  287  289  294  299  304  305  309  314  317  318  320  322  332\n",
      "  335  347  353  360  361  370  373  381  388  389  391  392  399  403\n",
      "  409  413  416  418  421  423  424  426  428  429  439  444  445  447\n",
      "  448  449  462  465  470  473  478  482  489  491  493  496  503  505\n",
      "  507  513  514  530  533  553  554  576  580  590  594  597  598  606\n",
      "  607  617  618  622  629  635  666  671  672  673  675  678  681  687\n",
      "  695  700  701  709  712  715  716  721  722  727  735  740  743  744\n",
      "  745  761  765  767  776  791  800  802  804  809  817  824  826  830\n",
      "  834  843  849  858  861  863  864  865  868  888  894  895  897  898\n",
      "  911  914  924  940  947  949  951  960  965  970  971  972  986  987\n",
      "  990  991  992  994  995  996  998 1000 1001 1002 1011 1013 1025 1028\n",
      " 1031 1035 1048 1055 1067 1088 1091 1095 1096 1100 1101 1104 1109 1115\n",
      " 1129 1131 1136 1142 1145 1146 1156 1160 1161 1162 1163 1168 1171 1174\n",
      " 1187 1188 1199 1215 1227 1234 1239 1240 1241 1255 1264 1270 1271 1273\n",
      " 1274 1279 1290 1291 1296 1314 1317 1318 1320 1321 1323 1324 1328 1338\n",
      " 1341 1343 1350 1353 1359 1367 1372 1377 1379 1383 1389 1403 1407 1411\n",
      " 1421 1431 1433 1438 1442 1458 1460 1470 1478 1480 1481 1484 1495 1498\n",
      " 1508 1515 1519 1530 1534 1535 1536 1538 1548 1556 1557 1558 1572 1576\n",
      " 1579 1580 1582 1589 1590 1594 1598 1599 1601 1606 1612 1618 1620 1623\n",
      " 1625 1627 1628 1635 1643 1647 1648 1650 1651 1652 1656 1666 1671 1674\n",
      " 1676 1679 1681 1682 1683 1685 1698 1699 1704 1705 1706 1708 1714 1721\n",
      " 1733 1743 1746 1749 1753 1760 1768 1773 1775 1777 1778 1781 1784 1785\n",
      " 1786 1789 1798 1802 1811 1818 1819 1826 1841 1844 1851 1856 1860 1862\n",
      " 1868 1869 1875 1881 1889 1890 1902 1907 1914 1922 1941 1945 1946 1947\n",
      " 1949 1955 1960 1965 1992 1995 1997 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.6291 - acc: 0.1176\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6158 - acc: 0.1181\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6147 - acc: 0.1194\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6131 - acc: 0.1199\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6126 - acc: 0.1203\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6113 - acc: 0.1207\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6107 - acc: 0.1208\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6107 - acc: 0.1208\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6101 - acc: 0.1209\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6107 - acc: 0.1207\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6103 - acc: 0.1209\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6103 - acc: 0.1209\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6102 - acc: 0.1210\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6100 - acc: 0.1211\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6097 - acc: 0.1211\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1214\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6102 - acc: 0.1208\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6100 - acc: 0.1210\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6103 - acc: 0.1209\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6098 - acc: 0.1210\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6097 - acc: 0.1209\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6097 - acc: 0.1210\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6095 - acc: 0.1210\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6095 - acc: 0.1210\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6096 - acc: 0.1210\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6097 - acc: 0.1210\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6095 - acc: 0.1210\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6094 - acc: 0.1210\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6095 - acc: 0.1209\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6097 - acc: 0.1210\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6095 - acc: 0.1210\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6095 - acc: 0.1210\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6096 - acc: 0.1210\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6095 - acc: 0.1210\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6093 - acc: 0.1210\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6094 - acc: 0.1210\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6094 - acc: 0.1210\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6093 - acc: 0.1210\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6096 - acc: 0.1210\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6092 - acc: 0.1211\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6093 - acc: 0.1210\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6093 - acc: 0.1210\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6091 - acc: 0.1211\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6092 - acc: 0.1210\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6093 - acc: 0.1210\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6092 - acc: 0.1210\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6092 - acc: 0.1210\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6091 - acc: 0.1210\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6091 - acc: 0.1211\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6090 - acc: 0.1211\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6092 - acc: 0.1210\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6091 - acc: 0.1210\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6090 - acc: 0.1211\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1211\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6090 - acc: 0.1210\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6088 - acc: 0.1210\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6091 - acc: 0.1210\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6088 - acc: 0.1210\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6088 - acc: 0.1210\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6090 - acc: 0.1210\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6088 - acc: 0.1210\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6088 - acc: 0.1210\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6085 - acc: 0.1211\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1211\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1210\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1210\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1211\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1211\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1211\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6085 - acc: 0.1210\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6085 - acc: 0.1210\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6088 - acc: 0.1210\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6085 - acc: 0.1210\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1210\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1210\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6085 - acc: 0.1210\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1211\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1210\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6089 - acc: 0.1210\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6087 - acc: 0.1210\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1210\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6086 - acc: 0.1210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1429\n",
      "           1       0.28      0.02      0.04     22202\n",
      "           2       0.00      0.00      0.00      4321\n",
      "           3       0.34      0.99      0.51     37225\n",
      "           4       0.00      0.00      0.00       752\n",
      "           5       0.70      0.04      0.07     22259\n",
      "           6       0.00      0.00      0.00      9693\n",
      "           7       0.00      0.00      0.00     12450\n",
      "\n",
      "    accuracy                           0.34    110331\n",
      "   macro avg       0.17      0.13      0.08    110331\n",
      "weighted avg       0.31      0.34      0.19    110331\n",
      "\n",
      "Acurácia\n",
      "0.13046327219173365\n",
      "Precisao\n",
      "0.31294135414185104\n",
      "Recall\n",
      "0.3442550144564991\n",
      "F1\n",
      "0.19310999050294714\n",
      "[[    0    24     0  1388     0    17     0     0]\n",
      " [    0   454     0 21617     0   131     0     0]\n",
      " [    0    79     0  4219     0    23     0     0]\n",
      " [    0   425     0 36691     0   109     0     0]\n",
      " [    0     0     0   752     0     0     0     0]\n",
      " [    0   391     0 21031     0   837     0     0]\n",
      " [    0   115     0  9531     0    47     0     0]\n",
      " [    0   112     0 12302     0    36     0     0]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   4    9   12   14   16   17   21   24   36   39   40   41   56   59\n",
      "   64   67   77   89   90   94  123  124  140  152  157  162  164  179\n",
      "  181  183  186  187  189  192  193  195  198  199  200  201  204  205\n",
      "  206  207  210  212  215  216  218  221  223  224  228  236  239  240\n",
      "  257  258  269  272  273  280  284  290  291  292  295  297  306  321\n",
      "  331  333  337  340  348  354  355  356  358  362  364  365  377  383\n",
      "  385  390  395  397  398  414  422  425  427  433  446  454  455  466\n",
      "  467  469  474  481  497  502  515  516  521  525  527  538  550  555\n",
      "  562  569  570  572  575  579  584  587  591  595  602  605  610  611\n",
      "  612  613  620  627  632  633  634  644  653  654  656  659  663  679\n",
      "  680  689  690  697  704  706  730  749  755  757  759  764  771  773\n",
      "  784  794  795  798  801  803  805  813  825  827  829  851  852  853\n",
      "  855  856  859  862  876  880  881  882  884  887  889  901  907  912\n",
      "  917  918  922  925  926  932  933  943  944  953  958  962  967  976\n",
      "  982  988  999 1008 1012 1016 1017 1019 1023 1029 1033 1040 1049 1050\n",
      " 1053 1054 1059 1061 1071 1072 1092 1103 1116 1119 1123 1125 1130 1134\n",
      " 1137 1141 1153 1155 1157 1164 1173 1177 1178 1179 1181 1182 1184 1194\n",
      " 1205 1206 1210 1220 1226 1229 1233 1244 1246 1247 1251 1254 1259 1265\n",
      " 1275 1276 1277 1283 1287 1292 1295 1307 1308 1313 1316 1319 1322 1329\n",
      " 1330 1334 1335 1344 1345 1356 1362 1369 1376 1384 1394 1400 1401 1409\n",
      " 1415 1416 1418 1434 1436 1439 1446 1451 1455 1468 1483 1486 1490 1499\n",
      " 1500 1507 1510 1520 1525 1526 1539 1540 1542 1543 1550 1554 1561 1564\n",
      " 1567 1568 1583 1584 1587 1591 1593 1600 1607 1608 1626 1629 1636 1642\n",
      " 1654 1657 1661 1664 1668 1675 1677 1690 1700 1703 1713 1717 1718 1724\n",
      " 1725 1735 1742 1750 1755 1759 1761 1762 1767 1770 1780 1790 1791 1793\n",
      " 1795 1796 1806 1814 1815 1821 1823 1824 1828 1830 1837 1839 1842 1843\n",
      " 1847 1848 1853 1854 1858 1859 1864 1880 1882 1887 1891 1893 1895 1898\n",
      " 1904 1913 1918 1919 1926 1927 1928 1931 1935 1938 1942 1943 1950 1953\n",
      " 1954 1959 1962 1963 1971 1981 1986 1994]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.6466 - acc: 0.1201\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6273 - acc: 0.1220\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6256 - acc: 0.1230\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6239 - acc: 0.1237\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6239 - acc: 0.1237\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6226 - acc: 0.1240\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6218 - acc: 0.1241\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6217 - acc: 0.1241\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6212 - acc: 0.1242\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6211 - acc: 0.1243\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6212 - acc: 0.1243\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6209 - acc: 0.1242\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6211 - acc: 0.1241\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6208 - acc: 0.1243\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6208 - acc: 0.1243\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6208 - acc: 0.1242\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6207 - acc: 0.1243\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6207 - acc: 0.1243\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6205 - acc: 0.1242\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6205 - acc: 0.1243\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6228 - acc: 0.1229\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6210 - acc: 0.1240\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6208 - acc: 0.1240\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6207 - acc: 0.1241\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6207 - acc: 0.1243\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6207 - acc: 0.1242\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6206 - acc: 0.1243\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6206 - acc: 0.1243\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6204 - acc: 0.1243\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1243\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6202 - acc: 0.1244\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6204 - acc: 0.1243\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1244\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1243\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1243\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1243\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1243\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1244\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1243\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6200 - acc: 0.1243\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1243\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1244\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1243\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1246\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1246\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6191 - acc: 0.1250\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1243\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6202 - acc: 0.1243\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6200 - acc: 0.1243\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1244\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6182 - acc: 0.1244\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6139 - acc: 0.1321\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6224 - acc: 0.1223\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6206 - acc: 0.1242\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6205 - acc: 0.1243\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1243\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6203 - acc: 0.1243\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1243\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6201 - acc: 0.1243\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1243\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1243\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1243\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1242\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6199 - acc: 0.1243\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6198 - acc: 0.1242\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1244\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1243\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1243\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1243\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1243\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1243\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1243\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1244\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1243\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1243\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1243\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1243\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1243\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6197 - acc: 0.1243\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1243\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1243\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6192 - acc: 0.1244\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6194 - acc: 0.1243\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6194 - acc: 0.1243\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6194 - acc: 0.1243\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1243\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1243\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6194 - acc: 0.1244\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1243\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1243\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1243\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1243\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6196 - acc: 0.1243\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1243\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6193 - acc: 0.1244\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1243\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6194 - acc: 0.1243\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6192 - acc: 0.1244\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6195 - acc: 0.1243\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6192 - acc: 0.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1287\n",
      "           1       0.31      0.02      0.03     21564\n",
      "           2       0.00      0.00      0.00      3989\n",
      "           3       0.33      0.99      0.49     33338\n",
      "           4       0.00      0.00      0.00       695\n",
      "           5       0.68      0.04      0.08     21115\n",
      "           6       0.00      0.00      0.00      9228\n",
      "           7       0.00      0.00      0.00     11622\n",
      "\n",
      "    accuracy                           0.33    102838\n",
      "   macro avg       0.16      0.13      0.08    102838\n",
      "weighted avg       0.31      0.33      0.18    102838\n",
      "\n",
      "Acurácia\n",
      "0.13124612678382808\n",
      "Precisao\n",
      "0.31103104387349284\n",
      "Recall\n",
      "0.333417608277096\n",
      "F1\n",
      "0.18329882225951952\n",
      "[[    0    14     0  1253     0    20     0     0]\n",
      " [    0   376     0 20999     0   189     0     0]\n",
      " [    0    43     0  3922     0    24     0     0]\n",
      " [    0   230     0 33030     0    78     0     0]\n",
      " [    0     4     0   690     0     1     0     0]\n",
      " [    0   365     0 19868     0   882     0     0]\n",
      " [    0    92     0  9076     0    60     0     0]\n",
      " [    0    87     0 11488     0    47     0     0]]\n",
      "TRAIN: [   0    1    3 ... 1997 1998 1999] TEST: [   2    6   11   13   15   18   22   23   25   34   35   42   48   50\n",
      "   54   60   72   74   76   78   81   93   98  106  117  128  129  132\n",
      "  135  136  137  141  143  144  146  156  158  159  168  171  175  177\n",
      "  196  197  211  217  222  226  231  237  241  242  244  249  251  253\n",
      "  254  256  261  262  264  265  270  283  285  308  313  315  325  326\n",
      "  327  328  339  341  349  350  352  367  369  372  375  376  379  386\n",
      "  396  400  402  404  408  431  432  435  437  443  451  464  471  472\n",
      "  476  477  483  487  490  492  494  510  512  518  522  524  526  528\n",
      "  535  541  545  546  547  549  552  564  573  578  585  588  589  599\n",
      "  601  603  614  615  616  621  625  640  646  647  648  649  651  667\n",
      "  670  676  686  688  691  692  699  703  705  713  714  718  720  731\n",
      "  732  733  742  746  747  763  770  777  781  786  792  799  806  810\n",
      "  811  812  816  823  828  835  841  842  844  846  850  857  866  869\n",
      "  870  871  873  883  885  892  896  904  906  908  913  915  927  928\n",
      "  930  934  936  937  938  942  945  950  952  956  957  961  963  966\n",
      "  978  981  985 1003 1004 1014 1020 1022 1032 1036 1042 1058 1062 1063\n",
      " 1064 1070 1075 1082 1097 1106 1107 1108 1118 1127 1128 1138 1143 1147\n",
      " 1148 1151 1167 1169 1180 1186 1192 1196 1197 1198 1200 1202 1209 1211\n",
      " 1213 1219 1221 1224 1225 1230 1237 1238 1245 1249 1253 1258 1263 1269\n",
      " 1272 1284 1293 1294 1300 1301 1302 1305 1315 1325 1327 1336 1337 1342\n",
      " 1351 1352 1361 1374 1381 1386 1387 1388 1393 1395 1398 1424 1426 1429\n",
      " 1440 1445 1454 1456 1459 1463 1467 1471 1472 1476 1477 1487 1496 1501\n",
      " 1504 1509 1511 1513 1516 1518 1522 1524 1527 1528 1532 1547 1559 1565\n",
      " 1571 1573 1575 1581 1586 1596 1602 1604 1613 1615 1617 1622 1624 1630\n",
      " 1633 1634 1640 1641 1649 1653 1655 1660 1662 1669 1670 1680 1691 1693\n",
      " 1694 1702 1720 1727 1728 1730 1732 1734 1740 1747 1772 1774 1792 1797\n",
      " 1799 1804 1807 1810 1829 1834 1838 1840 1850 1852 1861 1863 1870 1877\n",
      " 1878 1879 1884 1894 1900 1916 1921 1925 1930 1936 1940 1948 1952 1961\n",
      " 1968 1972 1974 1978 1982 1987 1988 1991]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6375 - acc: 0.1246\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6107 - acc: 0.1327\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5784 - acc: 0.1521\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5632 - acc: 0.1601\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5471 - acc: 0.1664\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5300 - acc: 0.1734\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5209 - acc: 0.1770\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5192 - acc: 0.1776\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5107 - acc: 0.1815\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5080 - acc: 0.1827\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5063 - acc: 0.1836\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.5007 - acc: 0.1855\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4979 - acc: 0.1870\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4947 - acc: 0.1885\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4945 - acc: 0.1882\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4938 - acc: 0.1886\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4892 - acc: 0.1902\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4850 - acc: 0.1922\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4813 - acc: 0.1937\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4783 - acc: 0.1946\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4738 - acc: 0.1970\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4697 - acc: 0.1978\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4639 - acc: 0.2001\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4577 - acc: 0.2023\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4541 - acc: 0.2037\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4518 - acc: 0.2048\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4444 - acc: 0.2070\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4372 - acc: 0.2094\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4341 - acc: 0.2106\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4282 - acc: 0.2123\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4263 - acc: 0.2130\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4197 - acc: 0.2155\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4114 - acc: 0.2185\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4035 - acc: 0.2209\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.4017 - acc: 0.2216\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3970 - acc: 0.2233\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3893 - acc: 0.2256\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3861 - acc: 0.2263\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3787 - acc: 0.2292\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3793 - acc: 0.2290\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3721 - acc: 0.2312\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3656 - acc: 0.2332\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3629 - acc: 0.2340\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3579 - acc: 0.2356\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3551 - acc: 0.2364\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3498 - acc: 0.2381\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3463 - acc: 0.2392\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3433 - acc: 0.2403\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3393 - acc: 0.2411\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3344 - acc: 0.2429\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3307 - acc: 0.2442\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3284 - acc: 0.2447\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3229 - acc: 0.2469\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3205 - acc: 0.2472\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3192 - acc: 0.2475\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3162 - acc: 0.2489\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3136 - acc: 0.2498\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3144 - acc: 0.2494\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3088 - acc: 0.2513\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3043 - acc: 0.2525\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3015 - acc: 0.2534\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2985 - acc: 0.2546\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2968 - acc: 0.2551\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2950 - acc: 0.2555\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2944 - acc: 0.2560\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2907 - acc: 0.2570\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2880 - acc: 0.2581\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2854 - acc: 0.2590\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2836 - acc: 0.2594\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2824 - acc: 0.2603\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2794 - acc: 0.2610\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2784 - acc: 0.2613\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2747 - acc: 0.2626\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2718 - acc: 0.2636\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2717 - acc: 0.2635\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2715 - acc: 0.2637\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2689 - acc: 0.2644\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2677 - acc: 0.2648\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2665 - acc: 0.2655\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2627 - acc: 0.2666\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2608 - acc: 0.2672\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2599 - acc: 0.2675\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2590 - acc: 0.2680\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2564 - acc: 0.2692\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2569 - acc: 0.2688\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2551 - acc: 0.2694\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2536 - acc: 0.2698\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2520 - acc: 0.2703\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2522 - acc: 0.2706\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2496 - acc: 0.2712\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2480 - acc: 0.2718\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2456 - acc: 0.2725\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2451 - acc: 0.2729\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2440 - acc: 0.2732\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2421 - acc: 0.2740\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2410 - acc: 0.2743\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2399 - acc: 0.2745\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2387 - acc: 0.2752\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2392 - acc: 0.2751\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2372 - acc: 0.2759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.04      0.08      1174\n",
      "           1       0.63      0.66      0.64     23472\n",
      "           2       0.29      0.27      0.28      3750\n",
      "           3       0.70      0.75      0.72     33175\n",
      "           4       0.43      0.20      0.27       699\n",
      "           5       0.42      0.53      0.47     20798\n",
      "           6       0.38      0.14      0.21      8966\n",
      "           7       0.40      0.36      0.38     11889\n",
      "\n",
      "    accuracy                           0.56    103923\n",
      "   macro avg       0.49      0.37      0.38    103923\n",
      "weighted avg       0.55      0.56      0.54    103923\n",
      "\n",
      "Acurácia\n",
      "0.36769548842860567\n",
      "Precisao\n",
      "0.5490004070647762\n",
      "Recall\n",
      "0.5572779846617207\n",
      "F1\n",
      "0.5433165403084254\n",
      "[[   48   247    33   167     1   517    40   121]\n",
      " [    0 15441   339  2799    18  3840   246   789]\n",
      " [    0   416  1019   774     4   919   112   506]\n",
      " [    1  2698   689 24744   114  3025   259  1645]\n",
      " [    0    54    16   381   137    60     4    47]\n",
      " [    9  3347   619  2925    23 10980   874  2021]\n",
      " [   10  1218   308  1195     7  3622  1273  1333]\n",
      " [    0  1173   521  2469    13  2878   563  4272]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_9 (Bidirection (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 2,032,008\n",
      "Trainable params: 2,032,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuq8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0c897863f35d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Acurácias total'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuq8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuq8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Precision total'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuq8' is not defined"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accuq8)\n",
    "a = np.array(accuq8)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "p = np.array(precisionsq8)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "r = np.array(recallsq8)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1q8)\n",
    "f = np.array(f1q8)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
