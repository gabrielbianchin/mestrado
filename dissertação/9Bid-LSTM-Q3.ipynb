{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 19:26:37.735535  7188 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 19:26:37.740522  7188 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 19:26:37.742517  7188 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 19:26:37.743515  7188 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   2    3    4 ... 1993 1998 1999] TEST: [   0    1    7    9   25   27   31   37   39   44   45   56   60   67\n",
      "   81   86   88   93  103  120  122  126  129  132  137  139  145  154\n",
      "  161  165  166  168  180  181  184  185  189  193  194  198  203  205\n",
      "  208  215  225  227  230  232  233  234  243  244  253  261  270  273\n",
      "  275  279  280  284  286  287  289  292  305  307  309  316  320  329\n",
      "  330  331  332  336  343  349  352  362  364  365  367  379  380  382\n",
      "  385  391  392  407  408  409  416  421  422  431  436  438  443  450\n",
      "  452  453  456  469  473  482  488  506  508  512  533  540  544  551\n",
      "  557  562  563  568  570  572  583  588  590  591  599  602  607  615\n",
      "  620  622  628  636  639  643  646  653  663  674  676  677  679  687\n",
      "  695  702  711  719  724  729  732  733  734  735  737  738  739  747\n",
      "  748  753  759  774  775  787  790  799  803  805  809  811  818  825\n",
      "  830  832  845  854  855  858  861  889  890  891  897  908  915  925\n",
      "  928  933  939  941  942  946  949  956  967  973  990 1001 1005 1009\n",
      " 1011 1013 1014 1016 1018 1040 1049 1055 1056 1070 1071 1073 1074 1075\n",
      " 1076 1078 1082 1091 1094 1095 1101 1103 1104 1105 1115 1117 1118 1126\n",
      " 1135 1136 1151 1152 1154 1158 1168 1188 1193 1195 1205 1206 1221 1223\n",
      " 1236 1244 1246 1248 1252 1253 1258 1259 1260 1280 1305 1307 1308 1311\n",
      " 1327 1332 1334 1335 1337 1341 1346 1347 1366 1374 1375 1380 1382 1389\n",
      " 1393 1395 1399 1401 1405 1411 1419 1420 1426 1433 1444 1450 1459 1462\n",
      " 1471 1479 1481 1482 1484 1487 1492 1494 1497 1499 1515 1516 1517 1521\n",
      " 1529 1531 1533 1538 1541 1543 1544 1546 1547 1549 1550 1551 1552 1554\n",
      " 1561 1574 1581 1584 1594 1595 1598 1599 1603 1605 1612 1617 1623 1624\n",
      " 1625 1626 1632 1634 1643 1655 1656 1659 1662 1674 1675 1676 1677 1678\n",
      " 1681 1687 1688 1689 1694 1695 1702 1705 1715 1719 1732 1736 1740 1759\n",
      " 1764 1769 1771 1773 1778 1780 1782 1797 1799 1800 1805 1808 1814 1817\n",
      " 1825 1829 1831 1836 1838 1839 1842 1846 1859 1860 1864 1886 1895 1898\n",
      " 1899 1914 1920 1922 1924 1936 1939 1940 1945 1946 1947 1956 1963 1964\n",
      " 1967 1973 1977 1986 1994 1995 1996 1997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 19:26:40.197951  7188 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.3730 - acc: 0.7417\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3469 - acc: 0.8086\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3270 - acc: 0.8459\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3101 - acc: 0.8567\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3035 - acc: 0.8605\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2989 - acc: 0.8627\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2964 - acc: 0.8644\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2944 - acc: 0.8655\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2919 - acc: 0.8668\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2918 - acc: 0.8667\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2875 - acc: 0.8684\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2895 - acc: 0.8622\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2899 - acc: 0.8687\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2842 - acc: 0.8717\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2831 - acc: 0.8714\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2818 - acc: 0.8722\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2803 - acc: 0.8731\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2752 - acc: 0.8757\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2708 - acc: 0.8779\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2678 - acc: 0.8796\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2659 - acc: 0.8803\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2636 - acc: 0.8818\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2612 - acc: 0.8830\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2845 - acc: 0.8719\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2824 - acc: 0.8729\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2712 - acc: 0.8784\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2594 - acc: 0.8841\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2542 - acc: 0.8861\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2505 - acc: 0.8880\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2482 - acc: 0.8898\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2407 - acc: 0.8935\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2375 - acc: 0.8953\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2325 - acc: 0.8986\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2324 - acc: 0.8985\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2251 - acc: 0.9018\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2219 - acc: 0.9032\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2171 - acc: 0.9060\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2138 - acc: 0.9082\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2097 - acc: 0.9098\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2074 - acc: 0.9105\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2046 - acc: 0.9115\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2011 - acc: 0.9123\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1990 - acc: 0.9127\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1954 - acc: 0.9153\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1907 - acc: 0.9160\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1878 - acc: 0.9186\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1900 - acc: 0.9174\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1836 - acc: 0.9215\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1802 - acc: 0.9228\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1790 - acc: 0.9231\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1762 - acc: 0.9242\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1730 - acc: 0.9260\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1712 - acc: 0.9265\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1696 - acc: 0.9272\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1663 - acc: 0.9289\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1638 - acc: 0.9301\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1604 - acc: 0.9311\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1583 - acc: 0.9313\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1577 - acc: 0.9308\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1554 - acc: 0.9339\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1526 - acc: 0.9348\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1511 - acc: 0.9350\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1499 - acc: 0.9357\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1491 - acc: 0.9354\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1472 - acc: 0.9367\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1447 - acc: 0.9384\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1433 - acc: 0.9391\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1408 - acc: 0.9398\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1388 - acc: 0.9410\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1375 - acc: 0.9410\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1352 - acc: 0.9419\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1352 - acc: 0.9427\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1325 - acc: 0.9437\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1307 - acc: 0.9436\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1322 - acc: 0.9431\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1295 - acc: 0.9446\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1283 - acc: 0.9454\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1264 - acc: 0.9458\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1247 - acc: 0.9465\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1232 - acc: 0.9470\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1229 - acc: 0.9465\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1229 - acc: 0.9475\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1225 - acc: 0.9475\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1210 - acc: 0.9482\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1192 - acc: 0.9488\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1183 - acc: 0.9490\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1169 - acc: 0.9507\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1163 - acc: 0.9511\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1143 - acc: 0.9516\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1151 - acc: 0.9515\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1136 - acc: 0.9522\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1124 - acc: 0.9523\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1121 - acc: 0.9522\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1094 - acc: 0.9539\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1086 - acc: 0.9542\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1095 - acc: 0.9538\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1099 - acc: 0.9537\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1063 - acc: 0.9550\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1063 - acc: 0.9549\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1049 - acc: 0.9556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71     43799\n",
      "           1       0.67      0.61      0.64     23754\n",
      "           2       0.73      0.75      0.74     38963\n",
      "\n",
      "    accuracy                           0.70    106516\n",
      "   macro avg       0.70      0.69      0.70    106516\n",
      "weighted avg       0.70      0.70      0.70    106516\n",
      "\n",
      "Acur√°cia\n",
      "0.6919696655488886\n",
      "Precisao\n",
      "0.7037897604927188\n",
      "Recall\n",
      "0.7046265349806602\n",
      "F1\n",
      "0.7038290544734827\n",
      "[[31430  4650  7719]\n",
      " [ 6166 14525  3063]\n",
      " [ 7347  2517 29099]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   8   15   16   17   18   21   23   26   36   38   40   43   46   55\n",
      "   59   68   73   76   83   90   91  100  102  116  117  118  121  123\n",
      "  125  133  134  142  148  152  153  155  157  159  174  178  179  183\n",
      "  190  201  204  206  210  214  219  223  241  247  248  250  251  255\n",
      "  259  263  271  278  291  295  297  298  302  310  324  327  333  334\n",
      "  340  353  359  366  368  375  376  377  384  386  388  390  397  399\n",
      "  402  419  428  432  440  442  445  449  454  458  463  465  472  477\n",
      "  487  492  510  511  514  517  519  527  530  541  548  564  571  574\n",
      "  581  584  587  600  604  609  614  621  625  627  632  640  644  648\n",
      "  649  658  659  664  665  667  670  671  675  689  690  691  693  700\n",
      "  705  706  722  745  749  756  757  761  766  768  776  777  782  785\n",
      "  794  796  806  807  810  816  823  824  826  828  837  838  847  848\n",
      "  856  857  864  869  872  873  874  876  877  881  883  884  894  895\n",
      "  896  901  902  905  906  907  909  911  914  916  917  920  923  935\n",
      "  940  963  970  975  980  981  982  983  992  997  998 1000 1007 1015\n",
      " 1017 1021 1037 1046 1060 1083 1088 1092 1093 1097 1107 1119 1121 1127\n",
      " 1128 1131 1132 1134 1137 1140 1141 1143 1146 1149 1156 1157 1160 1161\n",
      " 1165 1166 1175 1176 1178 1185 1186 1196 1201 1203 1218 1224 1229 1234\n",
      " 1239 1241 1247 1251 1254 1256 1257 1262 1267 1281 1284 1286 1292 1297\n",
      " 1299 1301 1304 1316 1318 1320 1323 1328 1336 1343 1348 1351 1365 1367\n",
      " 1390 1396 1397 1398 1402 1407 1412 1413 1414 1416 1418 1422 1423 1431\n",
      " 1435 1436 1438 1439 1448 1453 1454 1455 1461 1470 1483 1493 1496 1512\n",
      " 1520 1524 1530 1535 1540 1542 1553 1565 1566 1575 1577 1582 1590 1600\n",
      " 1604 1606 1611 1618 1621 1628 1635 1637 1640 1647 1651 1652 1657 1667\n",
      " 1669 1673 1691 1708 1711 1714 1717 1721 1722 1723 1729 1730 1733 1735\n",
      " 1742 1747 1748 1750 1768 1783 1789 1793 1796 1801 1806 1813 1816 1819\n",
      " 1823 1827 1828 1830 1832 1833 1845 1847 1853 1866 1867 1873 1876 1878\n",
      " 1879 1880 1882 1901 1909 1911 1913 1926 1929 1933 1941 1943 1951 1954\n",
      " 1961 1966 1968 1969 1984 1987 1989 1990]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3724 - acc: 0.7662\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3428 - acc: 0.8350\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3221 - acc: 0.8502\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3135 - acc: 0.8566\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3036 - acc: 0.8616\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3000 - acc: 0.8636\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2980 - acc: 0.8645\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2968 - acc: 0.8656\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2916 - acc: 0.8678\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2885 - acc: 0.8697\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2855 - acc: 0.8713\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2848 - acc: 0.8717\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2821 - acc: 0.8725\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2815 - acc: 0.8730\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2781 - acc: 0.8753\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2763 - acc: 0.8751\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2739 - acc: 0.8762\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2698 - acc: 0.8783\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2684 - acc: 0.8800\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2641 - acc: 0.8820\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2604 - acc: 0.8831\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2604 - acc: 0.8840\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2528 - acc: 0.8879\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2490 - acc: 0.8911\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2465 - acc: 0.8916\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2417 - acc: 0.8942\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2356 - acc: 0.8955\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2300 - acc: 0.8980\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2261 - acc: 0.8998\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2248 - acc: 0.9014\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2194 - acc: 0.9042\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2136 - acc: 0.9067\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2113 - acc: 0.9080\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2053 - acc: 0.9100\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2004 - acc: 0.9131\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1970 - acc: 0.9136\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1943 - acc: 0.9153\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1910 - acc: 0.9168\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1866 - acc: 0.9187\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1843 - acc: 0.9199\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1803 - acc: 0.9214\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1790 - acc: 0.9228\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1741 - acc: 0.9247\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1731 - acc: 0.9244\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1686 - acc: 0.9260\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1653 - acc: 0.9280\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1622 - acc: 0.9299\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1602 - acc: 0.9301\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1595 - acc: 0.9306\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1568 - acc: 0.9322\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1530 - acc: 0.9331\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1506 - acc: 0.9330\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1483 - acc: 0.9342\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1466 - acc: 0.9351\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1441 - acc: 0.9372\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1442 - acc: 0.9369\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1432 - acc: 0.9379\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1398 - acc: 0.9391\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1392 - acc: 0.9399\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1356 - acc: 0.9412\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1343 - acc: 0.9409\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1326 - acc: 0.9413\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1316 - acc: 0.9419\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1321 - acc: 0.9421\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1329 - acc: 0.9427\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1286 - acc: 0.9442\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1272 - acc: 0.9450\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1254 - acc: 0.9462\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1236 - acc: 0.9465\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1240 - acc: 0.9468\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1214 - acc: 0.9480\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1203 - acc: 0.9484\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1216 - acc: 0.9477\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1191 - acc: 0.9482\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1162 - acc: 0.9496\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1213 - acc: 0.9470\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1177 - acc: 0.9484\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1143 - acc: 0.9496\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1135 - acc: 0.9501\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1103 - acc: 0.9510\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1092 - acc: 0.9515\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1094 - acc: 0.9516\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1074 - acc: 0.9526\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1086 - acc: 0.9516\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1069 - acc: 0.9523\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1053 - acc: 0.9533\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1051 - acc: 0.9538\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1045 - acc: 0.9544\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1047 - acc: 0.9538\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1034 - acc: 0.9541\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1037 - acc: 0.9546\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1023 - acc: 0.9543\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1019 - acc: 0.9553\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1012 - acc: 0.9553\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.0995 - acc: 0.9558\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0988 - acc: 0.9567\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0979 - acc: 0.9573\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0968 - acc: 0.9578\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0978 - acc: 0.9573\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0962 - acc: 0.9585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69     43215\n",
      "           1       0.64      0.56      0.60     23488\n",
      "           2       0.68      0.73      0.70     37414\n",
      "\n",
      "    accuracy                           0.67    104117\n",
      "   macro avg       0.67      0.66      0.66    104117\n",
      "weighted avg       0.67      0.67      0.67    104117\n",
      "\n",
      "Acur√°cia\n",
      "0.6602893552023754\n",
      "Precisao\n",
      "0.6736424292345998\n",
      "Recall\n",
      "0.674692893571655\n",
      "F1\n",
      "0.673281928043795\n",
      "[[29707  4631  8877]\n",
      " [ 6286 13245  3957]\n",
      " [ 7373  2746 27295]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [   4    5   11   28   41   49   50   51   52   54   57   61   63   64\n",
      "   66   69   71   72   75   78   79   80   84   98  110  124  127  135\n",
      "  140  141  144  147  150  151  160  173  192  200  217  218  221  224\n",
      "  226  228  235  252  254  258  260  262  267  281  285  288  299  300\n",
      "  308  312  313  323  325  328  339  342  344  345  347  350  354  360\n",
      "  363  370  387  396  398  400  404  405  411  415  423  426  429  434\n",
      "  439  447  455  457  470  471  474  478  480  485  489  493  499  500\n",
      "  501  502  504  507  513  520  524  532  534  537  543  549  552  560\n",
      "  561  569  575  577  578  582  585  593  598  603  611  613  618  623\n",
      "  647  650  652  654  655  656  657  668  673  681  682  683  684  686\n",
      "  688  701  703  704  707  713  714  715  723  727  730  736  742  746\n",
      "  751  758  771  772  773  786  789  793  795  800  802  814  817  820\n",
      "  829  839  846  852  862  865  868  875  878  880  886  892  900  904\n",
      "  912  919  921  927  929  930  948  951  955  971  972  974  986  987\n",
      "  994  995 1003 1022 1026 1027 1028 1033 1036 1048 1051 1052 1059 1066\n",
      " 1067 1069 1096 1102 1112 1114 1120 1124 1142 1147 1150 1163 1171 1173\n",
      " 1174 1182 1187 1197 1202 1204 1209 1214 1216 1217 1222 1230 1235 1238\n",
      " 1242 1243 1250 1263 1264 1266 1272 1273 1276 1277 1278 1282 1285 1287\n",
      " 1289 1293 1303 1306 1313 1315 1317 1321 1324 1325 1326 1329 1333 1338\n",
      " 1342 1349 1350 1352 1353 1354 1359 1361 1363 1368 1376 1386 1387 1394\n",
      " 1400 1406 1417 1421 1432 1434 1441 1446 1449 1451 1452 1456 1467 1469\n",
      " 1476 1478 1486 1490 1491 1500 1505 1510 1511 1518 1532 1536 1537 1539\n",
      " 1545 1557 1564 1570 1572 1576 1578 1586 1589 1591 1607 1608 1614 1616\n",
      " 1648 1661 1663 1665 1672 1682 1685 1696 1698 1710 1713 1720 1724 1725\n",
      " 1731 1734 1738 1741 1744 1753 1754 1755 1757 1760 1761 1762 1766 1774\n",
      " 1775 1776 1781 1784 1787 1794 1802 1803 1807 1809 1811 1815 1821 1824\n",
      " 1834 1844 1850 1856 1857 1858 1865 1870 1872 1884 1889 1892 1893 1896\n",
      " 1897 1900 1902 1906 1907 1912 1915 1916 1917 1918 1923 1925 1931 1932\n",
      " 1934 1962 1970 1975 1980 1981 1993 1998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3749 - acc: 0.7622\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3495 - acc: 0.8308\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3311 - acc: 0.8449\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3126 - acc: 0.8570\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3104 - acc: 0.8581\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.3037 - acc: 0.8609\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2987 - acc: 0.8633\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2960 - acc: 0.8650\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2939 - acc: 0.8665\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2904 - acc: 0.8680\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2921 - acc: 0.8666\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2868 - acc: 0.8701\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2830 - acc: 0.8717\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2823 - acc: 0.8716\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2798 - acc: 0.8735\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2785 - acc: 0.8749\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2751 - acc: 0.8758\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2723 - acc: 0.8786\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2688 - acc: 0.8805\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2657 - acc: 0.8820\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2621 - acc: 0.8830\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2583 - acc: 0.8856\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2546 - acc: 0.8868\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2532 - acc: 0.8882\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2478 - acc: 0.8914\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2416 - acc: 0.8931\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2365 - acc: 0.8958\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2315 - acc: 0.8988\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2279 - acc: 0.8999\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2229 - acc: 0.9032\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2179 - acc: 0.9054\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2128 - acc: 0.9081\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2125 - acc: 0.9074\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2069 - acc: 0.9102\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2019 - acc: 0.9133\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1991 - acc: 0.9146\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1945 - acc: 0.9165\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1934 - acc: 0.9172\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1910 - acc: 0.9179\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1862 - acc: 0.9205\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1809 - acc: 0.9227\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1775 - acc: 0.9241\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1746 - acc: 0.9254\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1734 - acc: 0.9258\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1689 - acc: 0.9279\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1650 - acc: 0.9295\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1633 - acc: 0.9305\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1607 - acc: 0.9315\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1599 - acc: 0.9315\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1574 - acc: 0.9330\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1540 - acc: 0.9347\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1514 - acc: 0.9353\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1501 - acc: 0.9357\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1479 - acc: 0.9369\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1456 - acc: 0.9375\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1439 - acc: 0.9385\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1408 - acc: 0.9397\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1397 - acc: 0.9402\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1385 - acc: 0.9409\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1368 - acc: 0.9416\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1350 - acc: 0.9425\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1335 - acc: 0.9433\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1314 - acc: 0.9439\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1290 - acc: 0.9452\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1289 - acc: 0.9451\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1277 - acc: 0.9461\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1274 - acc: 0.9461\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1247 - acc: 0.9471\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1224 - acc: 0.9482\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1215 - acc: 0.9487\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1209 - acc: 0.9488\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1205 - acc: 0.9489\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1191 - acc: 0.9496\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1172 - acc: 0.9503\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1166 - acc: 0.9496\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1144 - acc: 0.9510\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1150 - acc: 0.9510\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1140 - acc: 0.9516\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1134 - acc: 0.9520\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1116 - acc: 0.9527\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1106 - acc: 0.9526\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1102 - acc: 0.9530\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1088 - acc: 0.9538\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1077 - acc: 0.9544\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1068 - acc: 0.9542\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1060 - acc: 0.9551\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1050 - acc: 0.9556\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1050 - acc: 0.9548\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1044 - acc: 0.9547\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1049 - acc: 0.9543\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1032 - acc: 0.9556\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1010 - acc: 0.9565\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1031 - acc: 0.9554\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1019 - acc: 0.9567\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0999 - acc: 0.9573\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0989 - acc: 0.9569\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0975 - acc: 0.9576\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0975 - acc: 0.9585\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0959 - acc: 0.9584\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0955 - acc: 0.9589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.71     41673\n",
      "           1       0.66      0.61      0.63     21983\n",
      "           2       0.73      0.74      0.73     38828\n",
      "\n",
      "    accuracy                           0.70    102484\n",
      "   macro avg       0.69      0.69      0.69    102484\n",
      "weighted avg       0.70      0.70      0.70    102484\n",
      "\n",
      "Acur√°cia\n",
      "0.6865456401715323\n",
      "Precisao\n",
      "0.6996876506616977\n",
      "Recall\n",
      "0.7004995901799306\n",
      "F1\n",
      "0.6997714317960639\n",
      "[[29788  4212  7673]\n",
      " [ 5507 13331  3145]\n",
      " [ 7524  2633 28671]]\n",
      "TRAIN: [   0    1    3 ... 1997 1998 1999] TEST: [   2    6   10   19   20   22   32   33   34   42   48   74   77   85\n",
      "   87   92   94   95   99  101  105  108  109  111  115  119  128  131\n",
      "  136  138  158  162  170  172  176  177  187  188  195  196  199  202\n",
      "  209  211  212  213  216  220  229  231  238  239  242  246  264  265\n",
      "  266  268  269  272  277  282  283  290  293  303  311  314  317  322\n",
      "  337  338  341  346  355  356  369  371  372  373  401  403  412  414\n",
      "  417  420  425  427  441  444  446  451  461  462  464  467  476  481\n",
      "  483  490  497  498  503  505  521  526  528  529  531  535  538  542\n",
      "  545  556  565  566  573  576  589  592  596  597  606  608  617  619\n",
      "  624  626  630  631  638  641  642  645  660  662  669  678  680  699\n",
      "  708  709  712  716  718  725  743  744  752  755  764  765  770  778\n",
      "  781  783  788  791  792  804  808  812  821  831  834  835  842  849\n",
      "  850  851  859  863  867  871  879  882  885  888  893  898  903  910\n",
      "  922  924  931  936  947  950  953  957  960  961  962  966  969  978\n",
      "  984  988  989  993  996 1010 1020 1023 1024 1025 1030 1031 1035 1038\n",
      " 1041 1042 1043 1044 1045 1047 1050 1057 1061 1062 1063 1065 1072 1084\n",
      " 1087 1098 1099 1106 1110 1123 1138 1145 1148 1153 1155 1162 1164 1169\n",
      " 1170 1172 1177 1183 1190 1194 1199 1207 1210 1211 1233 1237 1261 1265\n",
      " 1269 1270 1283 1288 1290 1291 1295 1296 1298 1300 1309 1331 1339 1355\n",
      " 1356 1357 1360 1362 1370 1371 1377 1378 1379 1383 1388 1404 1409 1424\n",
      " 1427 1429 1430 1437 1440 1442 1445 1447 1457 1458 1460 1465 1473 1480\n",
      " 1485 1489 1495 1498 1504 1506 1522 1525 1526 1527 1534 1548 1555 1558\n",
      " 1559 1560 1562 1563 1567 1568 1573 1579 1580 1583 1592 1593 1597 1602\n",
      " 1613 1619 1620 1622 1629 1630 1633 1636 1638 1646 1650 1658 1666 1668\n",
      " 1670 1671 1679 1680 1684 1692 1693 1699 1700 1703 1704 1707 1716 1726\n",
      " 1727 1737 1745 1751 1752 1765 1772 1779 1786 1788 1790 1791 1804 1818\n",
      " 1822 1826 1835 1837 1843 1848 1851 1855 1861 1862 1863 1868 1869 1871\n",
      " 1875 1885 1887 1890 1903 1905 1928 1930 1935 1942 1944 1948 1950 1952\n",
      " 1955 1959 1965 1972 1976 1978 1988 1992]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.3766 - acc: 0.7676\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.3498 - acc: 0.8336\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3288 - acc: 0.8472\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3132 - acc: 0.8558\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3071 - acc: 0.8600\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3034 - acc: 0.8617\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3009 - acc: 0.8628\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2965 - acc: 0.8653\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2956 - acc: 0.8660\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2924 - acc: 0.8682\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2906 - acc: 0.8688\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2866 - acc: 0.8710\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2855 - acc: 0.8720\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2845 - acc: 0.8722\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2808 - acc: 0.8735\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2784 - acc: 0.8752\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2776 - acc: 0.8756\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2747 - acc: 0.8775\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2693 - acc: 0.8806\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2674 - acc: 0.8809\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2620 - acc: 0.8836\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2569 - acc: 0.8867\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2552 - acc: 0.8878\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2529 - acc: 0.8886\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2455 - acc: 0.8928\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2415 - acc: 0.8946\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2372 - acc: 0.8971\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2333 - acc: 0.8984\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2289 - acc: 0.9013\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.2233 - acc: 0.9035\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.2198 - acc: 0.9045\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2157 - acc: 0.9062\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2102 - acc: 0.9093\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.2102 - acc: 0.9086\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.2068 - acc: 0.9113\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.2009 - acc: 0.9137\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1960 - acc: 0.9160\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1910 - acc: 0.9165\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1866 - acc: 0.9198\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1866 - acc: 0.9196\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1834 - acc: 0.9212\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1785 - acc: 0.9229\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1752 - acc: 0.9251\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1728 - acc: 0.9264\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1705 - acc: 0.9269\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1682 - acc: 0.9283\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1641 - acc: 0.9300\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1635 - acc: 0.9300\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 30s 18ms/sample - loss: 0.1610 - acc: 0.9309\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1583 - acc: 0.9318\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1552 - acc: 0.9334\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1525 - acc: 0.9349\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1508 - acc: 0.9350\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1480 - acc: 0.9368\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1473 - acc: 0.9363\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1467 - acc: 0.9372\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1422 - acc: 0.9387\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1414 - acc: 0.9394\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1391 - acc: 0.9410\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1376 - acc: 0.9416\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1352 - acc: 0.9420\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1342 - acc: 0.9430\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1330 - acc: 0.9424\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1326 - acc: 0.9431\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1323 - acc: 0.9425\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1302 - acc: 0.9447\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1267 - acc: 0.9459\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1254 - acc: 0.9469\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.1244 - acc: 0.9470\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.1231 - acc: 0.9466\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1222 - acc: 0.9471\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.1211 - acc: 0.9480\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1191 - acc: 0.9480\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1192 - acc: 0.9480\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1191 - acc: 0.9489\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1166 - acc: 0.9505\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1151 - acc: 0.9507\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1142 - acc: 0.9510\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1132 - acc: 0.9519\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1126 - acc: 0.9519\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1109 - acc: 0.9524\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1122 - acc: 0.9524\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1099 - acc: 0.9536\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1102 - acc: 0.9527\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1092 - acc: 0.9532\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1087 - acc: 0.9539\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1067 - acc: 0.9551\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1062 - acc: 0.9549\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1048 - acc: 0.9549\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1050 - acc: 0.9545\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1045 - acc: 0.9555\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1034 - acc: 0.9557\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.1020 - acc: 0.9571\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.1011 - acc: 0.9570\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1000 - acc: 0.9573\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1003 - acc: 0.9559\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.0992 - acc: 0.9575\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.0994 - acc: 0.9569\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.0983 - acc: 0.9571\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.0972 - acc: 0.9566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70     40947\n",
      "           1       0.65      0.62      0.64     22530\n",
      "           2       0.73      0.74      0.73     36233\n",
      "\n",
      "    accuracy                           0.70     99710\n",
      "   macro avg       0.69      0.69      0.69     99710\n",
      "weighted avg       0.70      0.70      0.70     99710\n",
      "\n",
      "Acur√°cia\n",
      "0.6903441666073719\n",
      "Precisao\n",
      "0.6998244317704262\n",
      "Recall\n",
      "0.700300872530338\n",
      "F1\n",
      "0.6999690972234701\n",
      "[[29112  4907  6928]\n",
      " [ 5634 14081  2815]\n",
      " [ 6998  2601 26634]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1998] TEST: [   3   12   13   14   24   29   30   35   47   53   58   62   65   70\n",
      "   82   89   96   97  104  106  107  112  113  114  130  143  146  149\n",
      "  156  163  164  167  169  171  175  182  186  191  197  207  222  236\n",
      "  237  240  245  249  256  257  274  276  294  296  301  304  306  315\n",
      "  318  319  321  326  335  348  351  357  358  361  374  378  381  383\n",
      "  389  393  394  395  406  410  413  418  424  430  433  435  437  448\n",
      "  459  460  466  468  475  479  484  486  491  494  495  496  509  515\n",
      "  516  518  522  523  525  536  539  546  547  550  553  554  555  558\n",
      "  559  567  579  580  586  594  595  601  605  610  612  616  629  633\n",
      "  634  635  637  651  661  666  672  685  692  694  696  697  698  710\n",
      "  717  720  721  726  728  731  740  741  750  754  760  762  763  767\n",
      "  769  779  780  784  797  798  801  813  815  819  822  827  833  836\n",
      "  840  841  843  844  853  860  866  870  887  899  913  918  926  932\n",
      "  934  937  938  943  944  945  952  954  958  959  964  965  968  976\n",
      "  977  979  985  991  999 1002 1004 1006 1008 1012 1019 1029 1032 1034\n",
      " 1039 1053 1054 1058 1064 1068 1077 1079 1080 1081 1085 1086 1089 1090\n",
      " 1100 1108 1109 1111 1113 1116 1122 1125 1129 1130 1133 1139 1144 1159\n",
      " 1167 1179 1180 1181 1184 1189 1191 1192 1198 1200 1208 1212 1213 1215\n",
      " 1219 1220 1225 1226 1227 1228 1231 1232 1240 1245 1249 1255 1268 1271\n",
      " 1274 1275 1279 1294 1302 1310 1312 1314 1319 1322 1330 1340 1344 1345\n",
      " 1358 1364 1369 1372 1373 1381 1384 1385 1391 1392 1403 1408 1410 1415\n",
      " 1425 1428 1443 1463 1464 1466 1468 1472 1474 1475 1477 1488 1501 1502\n",
      " 1503 1507 1508 1509 1513 1514 1519 1523 1528 1556 1569 1571 1585 1587\n",
      " 1588 1596 1601 1609 1610 1615 1627 1631 1639 1641 1642 1644 1645 1649\n",
      " 1653 1654 1660 1664 1683 1686 1690 1697 1701 1706 1709 1712 1718 1728\n",
      " 1739 1743 1746 1749 1756 1758 1763 1767 1770 1777 1785 1792 1795 1798\n",
      " 1810 1812 1820 1840 1841 1849 1852 1854 1874 1877 1881 1883 1888 1891\n",
      " 1894 1904 1908 1910 1919 1921 1927 1937 1938 1949 1953 1957 1958 1960\n",
      " 1971 1974 1979 1982 1983 1985 1991 1999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.3760 - acc: 0.6464\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3473 - acc: 0.8310\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.3321 - acc: 0.8448\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.3143 - acc: 0.8570\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.3049 - acc: 0.8608\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.3014 - acc: 0.8621\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2980 - acc: 0.8640\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2985 - acc: 0.8642\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2968 - acc: 0.8643\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2919 - acc: 0.8669\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 30s 18ms/sample - loss: 0.2919 - acc: 0.8670\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2883 - acc: 0.8692\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.2843 - acc: 0.8710\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.2817 - acc: 0.8725\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2811 - acc: 0.8729\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 30s 18ms/sample - loss: 0.2780 - acc: 0.8746\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2753 - acc: 0.8763\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.2711 - acc: 0.8784\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.2687 - acc: 0.8795\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2665 - acc: 0.8803\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.2600 - acc: 0.8840\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2586 - acc: 0.8848\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2576 - acc: 0.8859\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.2492 - acc: 0.8897\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 30s 18ms/sample - loss: 0.2433 - acc: 0.8914\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2406 - acc: 0.8928\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.2411 - acc: 0.8935\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2339 - acc: 0.8971\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2282 - acc: 0.8991\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.2231 - acc: 0.8994\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2190 - acc: 0.9028\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.2147 - acc: 0.9065\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.2116 - acc: 0.9062\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.2068 - acc: 0.9104\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.2046 - acc: 0.9095\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.2024 - acc: 0.9107\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1934 - acc: 0.9149\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1920 - acc: 0.9139\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1892 - acc: 0.9154\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1902 - acc: 0.9166\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1829 - acc: 0.9206\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 31s 19ms/sample - loss: 0.1795 - acc: 0.9221\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1759 - acc: 0.9238\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 30s 18ms/sample - loss: 0.1744 - acc: 0.9240\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1706 - acc: 0.9265\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1672 - acc: 0.9277\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1676 - acc: 0.9261\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 31s 19ms/sample - loss: 0.1636 - acc: 0.9269\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 30s 18ms/sample - loss: 0.1606 - acc: 0.9311\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1623 - acc: 0.9295\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.1586 - acc: 0.9304\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1608 - acc: 0.9292\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1540 - acc: 0.9309\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1504 - acc: 0.9327\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1469 - acc: 0.9347\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1462 - acc: 0.9361\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1446 - acc: 0.9358\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1414 - acc: 0.9384\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.1395 - acc: 0.9388\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1386 - acc: 0.9382\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1363 - acc: 0.9385\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.1380 - acc: 0.9376\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1360 - acc: 0.9372\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1322 - acc: 0.9399\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1310 - acc: 0.9417\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1333 - acc: 0.9409\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1290 - acc: 0.9436\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1278 - acc: 0.9436\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1268 - acc: 0.9441\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1249 - acc: 0.9452\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1230 - acc: 0.9455\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.1216 - acc: 0.9462\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1203 - acc: 0.9468\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1196 - acc: 0.9458\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1191 - acc: 0.9456\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1182 - acc: 0.9476\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 28s 18ms/sample - loss: 0.1172 - acc: 0.9458\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1148 - acc: 0.9485\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.1139 - acc: 0.9488\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1134 - acc: 0.9490\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1124 - acc: 0.9502\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1124 - acc: 0.9490\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1096 - acc: 0.9515\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1090 - acc: 0.9520\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1081 - acc: 0.9516\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1072 - acc: 0.9524\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1076 - acc: 0.9518\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1063 - acc: 0.9522\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.1081 - acc: 0.9514\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 30s 18ms/sample - loss: 0.1063 - acc: 0.9527\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1032 - acc: 0.9546\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1056 - acc: 0.9531\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.1080 - acc: 0.9523\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 30s 18ms/sample - loss: 0.1028 - acc: 0.9544\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1018 - acc: 0.9548\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.1002 - acc: 0.9550\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.0996 - acc: 0.9550\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.0990 - acc: 0.9558\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 30s 19ms/sample - loss: 0.0974 - acc: 0.9569\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.0986 - acc: 0.9566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71     41964\n",
      "           1       0.67      0.62      0.65     23441\n",
      "           2       0.73      0.76      0.74     37576\n",
      "\n",
      "    accuracy                           0.71    102981\n",
      "   macro avg       0.70      0.70      0.70    102981\n",
      "weighted avg       0.71      0.71      0.71    102981\n",
      "\n",
      "Acur√°cia\n",
      "0.697945059419577\n",
      "Precisao\n",
      "0.7082447717901545\n",
      "Recall\n",
      "0.7091502315961197\n",
      "F1\n",
      "0.7083958682100541\n",
      "[[30033  4517  7414]\n",
      " [ 5847 14576  3018]\n",
      " [ 6543  2613 28420]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_36 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_37 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_38 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_39 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_40 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_41 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_42 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_43 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_44 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 2,031,003\n",
      "Trainable params: 2,031,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.69196967 0.66028936 0.68654564 0.69034417 0.69794506]\n",
      "0.685418777389949\n",
      "Precision total\n",
      "[0.70378976 0.67364243 0.69968765 0.69982443 0.70824477]\n",
      "0.6970378087899194\n",
      "Recalls total\n",
      "[0.70462653 0.67469289 0.70049959 0.70030087 0.70915023]\n",
      "0.6978540245717407\n",
      "F1 total\n",
      "[0.70382905 0.67328193 0.69977143 0.6999691  0.70839587]\n",
      "0.6970494759493732\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
