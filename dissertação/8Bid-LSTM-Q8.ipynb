{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 08:30:30.391682  3912 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 08:30:30.396646  3912 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 08:30:30.397644  3912 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 08:30:30.398641  3912 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [  18   23   28   36   39   43   46   48   52   54   57   78   80   87\n",
      "  104  105  108  113  114  117  128  131  139  142  144  153  156  168\n",
      "  174  178  179  180  183  184  186  189  190  195  201  206  209  210\n",
      "  231  236  241  254  255  260  268  269  273  276  277  282  284  287\n",
      "  288  296  303  304  321  327  340  344  369  381  382  385  387  389\n",
      "  396  399  401  407  415  421  426  439  444  449  452  453  459  464\n",
      "  470  472  475  485  486  487  491  499  504  505  507  509  510  515\n",
      "  519  524  535  538  550  556  560  562  568  569  573  575  578  584\n",
      "  586  588  589  592  606  611  614  630  635  637  638  649  656  657\n",
      "  658  660  662  664  668  671  677  681  684  685  691  692  704  706\n",
      "  716  719  728  732  737  744  748  761  765  766  767  769  770  771\n",
      "  774  776  783  789  795  799  803  805  829  831  838  839  851  852\n",
      "  853  856  863  868  874  877  883  904  906  908  909  913  918  926\n",
      "  948  951  953  961  962  969  970  971  973  981  982  988  989  993\n",
      "  996  999 1000 1004 1007 1017 1018 1026 1027 1030 1031 1032 1037 1042\n",
      " 1043 1045 1047 1048 1051 1063 1064 1067 1068 1071 1073 1074 1079 1083\n",
      " 1088 1098 1100 1103 1105 1107 1108 1114 1116 1118 1119 1131 1132 1133\n",
      " 1136 1142 1153 1154 1158 1171 1173 1177 1181 1182 1186 1187 1195 1198\n",
      " 1204 1209 1214 1221 1224 1228 1231 1234 1256 1258 1261 1268 1269 1281\n",
      " 1296 1297 1305 1309 1314 1322 1324 1328 1335 1340 1346 1347 1351 1357\n",
      " 1360 1381 1389 1392 1398 1400 1408 1410 1416 1417 1418 1419 1429 1432\n",
      " 1436 1464 1469 1470 1478 1485 1490 1495 1503 1518 1522 1523 1529 1531\n",
      " 1532 1536 1537 1538 1545 1553 1556 1561 1575 1578 1579 1582 1587 1588\n",
      " 1590 1591 1592 1594 1601 1609 1615 1616 1617 1629 1631 1635 1671 1676\n",
      " 1677 1686 1687 1692 1699 1710 1718 1729 1730 1736 1741 1743 1745 1747\n",
      " 1761 1775 1776 1784 1785 1793 1796 1798 1802 1804 1808 1813 1817 1822\n",
      " 1827 1830 1832 1835 1836 1843 1851 1854 1859 1865 1869 1871 1876 1879\n",
      " 1880 1882 1891 1895 1905 1911 1912 1926 1929 1933 1936 1939 1942 1948\n",
      " 1951 1957 1965 1967 1968 1976 1984 1996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 08:30:32.587787  3912 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.6358 - acc: 0.1239\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6237 - acc: 0.1206\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6136 - acc: 0.1290\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5909 - acc: 0.1420\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5674 - acc: 0.1548\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5502 - acc: 0.1620\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5439 - acc: 0.1641\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5233 - acc: 0.1745\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5165 - acc: 0.1773\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5134 - acc: 0.1785\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5101 - acc: 0.1799\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5051 - acc: 0.1822\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5025 - acc: 0.1836\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5011 - acc: 0.1843\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4955 - acc: 0.1864\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4939 - acc: 0.1868\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4907 - acc: 0.1883\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4873 - acc: 0.1897\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4846 - acc: 0.1912\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4823 - acc: 0.1921\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4834 - acc: 0.1914\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4825 - acc: 0.1914\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4741 - acc: 0.1949\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4711 - acc: 0.1961\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4692 - acc: 0.1964\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4637 - acc: 0.1992\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4591 - acc: 0.2004\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4554 - acc: 0.2021\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4494 - acc: 0.2044\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4431 - acc: 0.2066\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4431 - acc: 0.2063\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4383 - acc: 0.2081\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4334 - acc: 0.2096\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4275 - acc: 0.2116\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4216 - acc: 0.2138\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4149 - acc: 0.2162\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4097 - acc: 0.2177\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4049 - acc: 0.2192\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4008 - acc: 0.2204\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3933 - acc: 0.2229\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3909 - acc: 0.2240\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3832 - acc: 0.2263\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3823 - acc: 0.2266\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3779 - acc: 0.2279\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3723 - acc: 0.2299\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3678 - acc: 0.2313\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3628 - acc: 0.2329\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3569 - acc: 0.2350\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3564 - acc: 0.2348\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3524 - acc: 0.2360\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3493 - acc: 0.2372\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3481 - acc: 0.2375\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3426 - acc: 0.2393\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3387 - acc: 0.2408\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3338 - acc: 0.2420\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3306 - acc: 0.2432\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3274 - acc: 0.2443\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3254 - acc: 0.2450\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3214 - acc: 0.2462\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3170 - acc: 0.2477\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3137 - acc: 0.2485\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3103 - acc: 0.2496\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3093 - acc: 0.2499\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3059 - acc: 0.2512\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3028 - acc: 0.2522\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3013 - acc: 0.2525\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2984 - acc: 0.2534\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2984 - acc: 0.2537\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2947 - acc: 0.2550\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2927 - acc: 0.2556\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2903 - acc: 0.2564\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2876 - acc: 0.2573\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2866 - acc: 0.2575\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2840 - acc: 0.2584\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2815 - acc: 0.2594\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2797 - acc: 0.2597\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2760 - acc: 0.2612\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2765 - acc: 0.2608\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2739 - acc: 0.2618\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2728 - acc: 0.2623\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2694 - acc: 0.2634\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2686 - acc: 0.2637\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2651 - acc: 0.2650\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2658 - acc: 0.2649\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2636 - acc: 0.2654\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2628 - acc: 0.2656\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2611 - acc: 0.2664\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2580 - acc: 0.2674\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2567 - acc: 0.2680\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2548 - acc: 0.2688\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2529 - acc: 0.2691\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2515 - acc: 0.2697\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2499 - acc: 0.2703\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2494 - acc: 0.2702\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2487 - acc: 0.2707\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2488 - acc: 0.2708\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2456 - acc: 0.2716\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2449 - acc: 0.2721\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2432 - acc: 0.2726\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2417 - acc: 0.2734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.03      0.06      1277\n",
      "           1       0.62      0.67      0.64     21328\n",
      "           2       0.37      0.33      0.35      3983\n",
      "           3       0.73      0.77      0.75     35472\n",
      "           4       0.45      0.27      0.34       748\n",
      "           5       0.45      0.54      0.49     21131\n",
      "           6       0.41      0.14      0.21      9291\n",
      "           7       0.39      0.40      0.40     11891\n",
      "\n",
      "    accuracy                           0.58    105121\n",
      "   macro avg       0.51      0.39      0.40    105121\n",
      "weighted avg       0.57      0.58      0.56    105121\n",
      "\n",
      "Acurácia\n",
      "0.3940936791975221\n",
      "Precisao\n",
      "0.5697194021095976\n",
      "Recall\n",
      "0.5762026616946185\n",
      "F1\n",
      "0.5620931482303724\n",
      "[[   42   238    41   168     1   583    57   147]\n",
      " [    3 14202   250  2382    23  3431   153   884]\n",
      " [    1   447  1310   789     3   811    85   537]\n",
      " [    2  2473   667 27296   149  2880   210  1795]\n",
      " [    0    76     8   354   202    50     6    52]\n",
      " [    8  3235   523  2709    23 11390   825  2418]\n",
      " [    3  1110   282  1200    13  3644  1320  1719]\n",
      " [    2  1091   507  2314    31  2608   529  4809]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [   3    8    9   15   16   17   25   27   35   49   56   59   60   71\n",
      "   73   79   81   84   85   92   93   96  103  107  111  112  116  125\n",
      "  126  148  159  169  173  182  192  194  198  200  207  212  225  227\n",
      "  237  247  248  250  261  264  281  283  290  297  298  299  301  302\n",
      "  307  309  312  313  316  323  324  328  335  339  342  349  350  354\n",
      "  355  360  361  366  367  368  371  372  409  410  419  420  424  428\n",
      "  430  434  437  445  446  456  458  460  474  478  483  492  496  498\n",
      "  503  508  511  512  513  521  525  529  531  532  543  548  551  552\n",
      "  553  555  574  579  582  587  598  600  601  603  610  613  616  620\n",
      "  632  648  650  652  654  659  665  670  673  686  687  693  698  699\n",
      "  702  703  705  707  708  712  714  723  726  727  734  738  740  741\n",
      "  751  755  762  773  778  779  780  786  787  792  793  796  804  807\n",
      "  808  809  818  820  825  828  842  845  849  854  858  865  878  882\n",
      "  885  888  889  890  891  898  907  912  916  921  927  928  929  931\n",
      "  932  933  941  949  957  965  976  980  984  998 1002 1008 1009 1014\n",
      " 1022 1029 1035 1036 1038 1041 1050 1052 1055 1057 1060 1075 1077 1081\n",
      " 1084 1086 1091 1094 1096 1097 1101 1111 1112 1134 1141 1146 1156 1162\n",
      " 1166 1174 1176 1180 1183 1192 1193 1199 1206 1212 1222 1223 1225 1226\n",
      " 1230 1233 1243 1244 1245 1253 1260 1265 1274 1277 1280 1282 1288 1290\n",
      " 1293 1298 1304 1307 1308 1316 1317 1320 1327 1342 1343 1356 1369 1370\n",
      " 1371 1375 1382 1387 1395 1401 1403 1404 1409 1411 1428 1430 1431 1435\n",
      " 1448 1451 1453 1456 1460 1477 1479 1487 1494 1499 1500 1504 1509 1511\n",
      " 1513 1517 1526 1530 1535 1540 1546 1550 1559 1560 1567 1568 1576 1589\n",
      " 1599 1603 1612 1614 1622 1625 1633 1639 1646 1659 1664 1675 1680 1688\n",
      " 1696 1700 1703 1705 1708 1714 1719 1721 1727 1739 1742 1746 1748 1758\n",
      " 1760 1762 1766 1767 1773 1781 1782 1790 1797 1801 1803 1814 1816 1823\n",
      " 1824 1826 1828 1833 1834 1838 1844 1846 1848 1858 1864 1873 1878 1881\n",
      " 1888 1893 1894 1907 1908 1909 1914 1920 1922 1931 1941 1944 1952 1955\n",
      " 1958 1971 1980 1983 1985 1990 1995 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.6419 - acc: 0.1260\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6343 - acc: 0.1223\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6290 - acc: 0.1246\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6287 - acc: 0.1244\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6271 - acc: 0.1253\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6265 - acc: 0.1253\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6261 - acc: 0.1254\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6257 - acc: 0.1255\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6238 - acc: 0.1262\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6056 - acc: 0.1392\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5865 - acc: 0.1471\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5704 - acc: 0.1566\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5636 - acc: 0.1602\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5501 - acc: 0.1663\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5416 - acc: 0.1698\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5325 - acc: 0.1745\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5242 - acc: 0.1790\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5220 - acc: 0.1801\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5172 - acc: 0.1825\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5121 - acc: 0.1844\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5095 - acc: 0.1854\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5063 - acc: 0.1871\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5071 - acc: 0.1861\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5019 - acc: 0.1891\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5008 - acc: 0.1892\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4955 - acc: 0.1913\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4980 - acc: 0.1903\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4924 - acc: 0.1927\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4887 - acc: 0.1943\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4863 - acc: 0.1953\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4834 - acc: 0.1966\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4797 - acc: 0.1984\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4763 - acc: 0.1991\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4737 - acc: 0.2005\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4735 - acc: 0.2007\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4698 - acc: 0.2021\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4653 - acc: 0.2036\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4621 - acc: 0.2049\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4589 - acc: 0.2061\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4558 - acc: 0.2072\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4555 - acc: 0.2072\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4491 - acc: 0.2092\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4454 - acc: 0.2109\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4398 - acc: 0.2125\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4340 - acc: 0.2147\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4331 - acc: 0.2150\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4284 - acc: 0.2167\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4239 - acc: 0.2182\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4206 - acc: 0.2193\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4148 - acc: 0.2213\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4135 - acc: 0.2216\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4061 - acc: 0.2242\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4011 - acc: 0.2256\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4002 - acc: 0.2261\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3950 - acc: 0.2274\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3900 - acc: 0.2292\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3861 - acc: 0.2304\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3809 - acc: 0.2321\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3780 - acc: 0.2329\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3745 - acc: 0.2342\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3735 - acc: 0.2343\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3718 - acc: 0.2351\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3627 - acc: 0.2378\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3572 - acc: 0.2394\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3555 - acc: 0.2400\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3523 - acc: 0.2413\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3503 - acc: 0.2416\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3475 - acc: 0.2426\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3411 - acc: 0.2447\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3380 - acc: 0.2458\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3379 - acc: 0.2457\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3315 - acc: 0.2477\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3304 - acc: 0.2480\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3265 - acc: 0.2493\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3265 - acc: 0.2493\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3215 - acc: 0.2509\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3185 - acc: 0.2517\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3141 - acc: 0.2531\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3101 - acc: 0.2547\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3109 - acc: 0.2539\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3060 - acc: 0.2556\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3048 - acc: 0.2563\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3020 - acc: 0.2572\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3006 - acc: 0.2573\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2971 - acc: 0.2587\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2974 - acc: 0.2585\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2940 - acc: 0.2599\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2950 - acc: 0.2594\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2903 - acc: 0.2608\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2872 - acc: 0.2620\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2851 - acc: 0.2627\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2832 - acc: 0.2635\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2816 - acc: 0.2639\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2778 - acc: 0.2650\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2787 - acc: 0.2652\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2756 - acc: 0.2660\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2723 - acc: 0.2668\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2702 - acc: 0.2680\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2708 - acc: 0.2676\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2680 - acc: 0.2683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.03      0.06      1281\n",
      "           1       0.61      0.67      0.64     22615\n",
      "           2       0.34      0.22      0.27      3762\n",
      "           3       0.70      0.75      0.72     32078\n",
      "           4       0.39      0.30      0.34       533\n",
      "           5       0.43      0.53      0.47     20040\n",
      "           6       0.39      0.12      0.18      8692\n",
      "           7       0.40      0.38      0.39     11415\n",
      "\n",
      "    accuracy                           0.56    100416\n",
      "   macro avg       0.50      0.38      0.38    100416\n",
      "weighted avg       0.55      0.56      0.54    100416\n",
      "\n",
      "Acurácia\n",
      "0.3753872389035599\n",
      "Precisao\n",
      "0.5489025363495098\n",
      "Recall\n",
      "0.5587655353728489\n",
      "F1\n",
      "0.5413461088269991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   41   260    48   189     2   583    40   118]\n",
      " [    1 15089   191  2821    25  3468   193   827]\n",
      " [    0   410   840   869     6   986    80   571]\n",
      " [    0  2856   410 23902   135  3049   175  1551]\n",
      " [    0    49    11   222   161    50     5    35]\n",
      " [   12  3492   412  2835    48 10689   627  1925]\n",
      " [    0  1258   206  1187    20  3581  1024  1416]\n",
      " [    2  1197   340  2335    15  2649   514  4363]]\n",
      "TRAIN: [   1    3    4 ... 1996 1997 1998] TEST: [   0    2   12   13   19   21   24   26   30   37   51   61   66   94\n",
      "   98   99  101  106  118  119  129  130  132  133  134  135  137  143\n",
      "  147  149  151  157  161  162  185  187  188  203  214  215  216  219\n",
      "  220  224  234  235  239  244  251  262  265  270  272  280  285  286\n",
      "  289  294  300  314  317  318  325  329  330  345  346  351  356  358\n",
      "  363  365  370  378  380  386  391  392  393  394  395  398  413  418\n",
      "  429  435  436  443  448  450  454  457  462  471  473  476  482  500\n",
      "  502  528  534  539  540  541  546  557  561  563  564  565  567  570\n",
      "  572  576  581  583  585  590  595  599  604  617  622  625  626  627\n",
      "  641  644  645  646  653  655  666  669  672  675  676  679  689  695\n",
      "  700  701  711  720  724  725  729  733  735  739  742  743  747  750\n",
      "  756  757  758  772  775  784  802  810  812  814  827  843  846  847\n",
      "  848  850  860  861  862  876  879  884  899  902  910  915  917  919\n",
      "  922  923  924  925  930  936  937  939  944  950  956  968  972  974\n",
      "  992  995 1003 1015 1020 1025 1033 1039 1044 1058 1059 1061 1065 1066\n",
      " 1069 1087 1095 1120 1121 1124 1125 1127 1138 1139 1140 1143 1145 1148\n",
      " 1149 1150 1151 1155 1165 1172 1185 1188 1190 1197 1200 1201 1205 1208\n",
      " 1218 1227 1235 1237 1241 1242 1246 1251 1255 1267 1271 1275 1283 1287\n",
      " 1291 1295 1301 1312 1319 1321 1323 1330 1333 1339 1341 1344 1345 1353\n",
      " 1359 1367 1374 1376 1378 1384 1385 1388 1391 1397 1406 1407 1412 1415\n",
      " 1420 1434 1438 1439 1441 1459 1466 1473 1474 1480 1488 1489 1491 1493\n",
      " 1501 1508 1512 1514 1516 1520 1521 1524 1539 1547 1549 1562 1569 1570\n",
      " 1571 1572 1573 1577 1584 1593 1596 1598 1602 1604 1605 1608 1610 1613\n",
      " 1618 1621 1624 1626 1627 1636 1637 1641 1644 1650 1654 1661 1667 1672\n",
      " 1685 1689 1702 1711 1717 1720 1722 1723 1726 1749 1752 1753 1754 1759\n",
      " 1774 1777 1778 1787 1791 1795 1800 1805 1806 1807 1819 1829 1840 1842\n",
      " 1845 1850 1852 1860 1861 1862 1863 1870 1872 1883 1884 1886 1887 1892\n",
      " 1897 1898 1903 1904 1913 1923 1935 1937 1938 1940 1943 1953 1961 1969\n",
      " 1970 1974 1975 1978 1982 1988 1993 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.6206 - acc: 0.1400\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5732 - acc: 0.1564\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5523 - acc: 0.1662\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5319 - acc: 0.1749\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5197 - acc: 0.1795\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5129 - acc: 0.1826\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5073 - acc: 0.1851\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5067 - acc: 0.1848\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5009 - acc: 0.1883\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4958 - acc: 0.1902\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4928 - acc: 0.1915\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4901 - acc: 0.1923\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4858 - acc: 0.1939\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4810 - acc: 0.1958\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4753 - acc: 0.1981\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4735 - acc: 0.1987\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4642 - acc: 0.2023\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4588 - acc: 0.2040\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4528 - acc: 0.2063\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4468 - acc: 0.2087\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4380 - acc: 0.2117\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4312 - acc: 0.2139\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4263 - acc: 0.2155\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4200 - acc: 0.2174\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4151 - acc: 0.2191\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4071 - acc: 0.2217\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4017 - acc: 0.2235\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3957 - acc: 0.2255\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3889 - acc: 0.2279\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3816 - acc: 0.2303\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3792 - acc: 0.2304\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3722 - acc: 0.2328\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3671 - acc: 0.2343\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3612 - acc: 0.2364\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3589 - acc: 0.2371\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3537 - acc: 0.2388\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3482 - acc: 0.2404\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3433 - acc: 0.2423\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3384 - acc: 0.2436\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3368 - acc: 0.2441\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3335 - acc: 0.2449\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3277 - acc: 0.2470\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3257 - acc: 0.2476\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3221 - acc: 0.2489\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3183 - acc: 0.2501\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3142 - acc: 0.2513\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3110 - acc: 0.2521\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3086 - acc: 0.2534\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3055 - acc: 0.2543\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3028 - acc: 0.2549\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3003 - acc: 0.2559\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2982 - acc: 0.2567\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2955 - acc: 0.2576\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2913 - acc: 0.2588\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2917 - acc: 0.2588\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2892 - acc: 0.2597\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2872 - acc: 0.2604\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2842 - acc: 0.2614\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2860 - acc: 0.2607\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2805 - acc: 0.2626\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2765 - acc: 0.2639\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2737 - acc: 0.2649\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2725 - acc: 0.2652\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2702 - acc: 0.2661\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2684 - acc: 0.2667\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2673 - acc: 0.2672\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2665 - acc: 0.2674\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2641 - acc: 0.2684\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2623 - acc: 0.2689\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2612 - acc: 0.2696\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2607 - acc: 0.2697\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2576 - acc: 0.2707\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2540 - acc: 0.2719\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2541 - acc: 0.2717\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2524 - acc: 0.2724\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2499 - acc: 0.2734\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2485 - acc: 0.2737\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2476 - acc: 0.2744\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2477 - acc: 0.2739\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2471 - acc: 0.2743\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2425 - acc: 0.2759\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2416 - acc: 0.2763\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2405 - acc: 0.2770\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2400 - acc: 0.2772\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2408 - acc: 0.2768\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2374 - acc: 0.2779\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2369 - acc: 0.2782\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2348 - acc: 0.2788\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2344 - acc: 0.2788\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2321 - acc: 0.2797\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2315 - acc: 0.2801\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2325 - acc: 0.2797\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2298 - acc: 0.2807\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2290 - acc: 0.2810\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2274 - acc: 0.2817\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2278 - acc: 0.2813\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2254 - acc: 0.2827\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2245 - acc: 0.2826\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2240 - acc: 0.2829\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2218 - acc: 0.2838\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.04      0.08      1223\n",
      "           1       0.63      0.63      0.63     21679\n",
      "           2       0.35      0.27      0.31      4192\n",
      "           3       0.69      0.77      0.73     32223\n",
      "           4       0.41      0.27      0.33       623\n",
      "           5       0.45      0.54      0.49     20995\n",
      "           6       0.37      0.16      0.23      9130\n",
      "           7       0.41      0.39      0.40     11550\n",
      "\n",
      "    accuracy                           0.56    101615\n",
      "   macro avg       0.50      0.38      0.40    101615\n",
      "weighted avg       0.55      0.56      0.55    101615\n",
      "\n",
      "Acurácia\n",
      "0.38486298867640384\n",
      "Precisao\n",
      "0.5504736874946211\n",
      "Recall\n",
      "0.5627318801358068\n",
      "F1\n",
      "0.5480453520110679\n",
      "[[   53   270    35   183     1   491    71   119]\n",
      " [    5 13671   225  2973    41  3488   353   923]\n",
      " [    0   444  1128   999    11   917   145   548]\n",
      " [    0  2149   512 24891   139  2679   294  1559]\n",
      " [    0    62    15   278   171    56     8    33]\n",
      " [   16  3105   511  3036    20 11302  1045  1960]\n",
      " [    5  1100   284  1289     2  3560  1485  1405]\n",
      " [    2  1026   474  2259    28  2615   665  4481]]\n",
      "TRAIN: [   0    1    2 ... 1996 1998 1999] TEST: [   4   11   20   29   31   32   33   45   53   55   58   62   63   64\n",
      "   68   70   75   76   86   88   90   91  115  120  123  145  146  154\n",
      "  155  163  164  165  171  175  177  181  193  196  199  204  205  213\n",
      "  217  222  223  228  230  232  238  242  243  249  252  256  258  259\n",
      "  263  267  279  292  293  295  305  306  319  320  326  337  338  341\n",
      "  347  348  357  359  362  364  373  374  376  377  379  383  384  388\n",
      "  390  400  402  403  405  406  411  414  422  425  427  431  440  463\n",
      "  465  466  468  477  479  484  489  494  506  514  518  520  523  530\n",
      "  542  558  566  571  577  580  607  609  612  618  619  623  628  636\n",
      "  642  647  667  678  683  690  697  710  713  715  717  721  745  749\n",
      "  752  753  763  781  785  788  790  794  801  806  811  815  817  819\n",
      "  823  826  830  833  835  836  841  844  855  857  864  866  869  872\n",
      "  873  875  886  887  894  897  900  901  905  911  935  940  943  945\n",
      "  946  958  959  964  966  975  983  986  987  990  997 1005 1010 1011\n",
      " 1013 1016 1021 1024 1040 1053 1056 1062 1072 1080 1082 1090 1092 1093\n",
      " 1099 1102 1104 1115 1117 1123 1126 1128 1135 1144 1157 1159 1160 1161\n",
      " 1163 1164 1167 1169 1175 1179 1189 1191 1194 1196 1202 1203 1207 1210\n",
      " 1211 1215 1217 1219 1239 1254 1257 1266 1276 1292 1294 1303 1306 1310\n",
      " 1313 1325 1326 1329 1331 1332 1334 1336 1337 1348 1349 1354 1361 1363\n",
      " 1364 1365 1373 1380 1386 1390 1402 1405 1413 1422 1424 1427 1442 1443\n",
      " 1444 1446 1447 1449 1450 1452 1454 1457 1458 1463 1467 1468 1476 1482\n",
      " 1492 1497 1506 1507 1510 1515 1525 1527 1528 1534 1541 1542 1544 1548\n",
      " 1551 1552 1554 1558 1563 1565 1574 1581 1583 1585 1595 1607 1619 1630\n",
      " 1632 1638 1643 1647 1648 1649 1651 1652 1657 1658 1668 1669 1679 1682\n",
      " 1683 1684 1690 1693 1694 1695 1698 1704 1706 1707 1713 1715 1716 1725\n",
      " 1731 1735 1738 1740 1744 1756 1763 1764 1768 1769 1770 1771 1780 1788\n",
      " 1792 1794 1799 1809 1810 1818 1837 1839 1841 1866 1877 1890 1899 1900\n",
      " 1901 1902 1906 1917 1919 1927 1928 1932 1934 1945 1949 1950 1954 1959\n",
      " 1962 1963 1972 1979 1981 1986 1989 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6143 - acc: 0.1375\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.5671 - acc: 0.1558\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.5464 - acc: 0.1642\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5219 - acc: 0.1749\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5152 - acc: 0.1777\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5089 - acc: 0.1805\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5021 - acc: 0.1833\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.5009 - acc: 0.1838\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.4929 - acc: 0.1875\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4929 - acc: 0.1872\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.4877 - acc: 0.1897\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.4827 - acc: 0.1914\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4777 - acc: 0.1936\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.4768 - acc: 0.1938\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.4720 - acc: 0.1959\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4662 - acc: 0.1980\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4644 - acc: 0.1987\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.4570 - acc: 0.2011\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.4490 - acc: 0.2042\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4430 - acc: 0.2063\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4413 - acc: 0.2072\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4343 - acc: 0.2091\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4270 - acc: 0.2118\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.4217 - acc: 0.2135\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4134 - acc: 0.2158\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4112 - acc: 0.2168\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4038 - acc: 0.2195\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3972 - acc: 0.2218\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3929 - acc: 0.2232\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3839 - acc: 0.2260\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3822 - acc: 0.2265\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3731 - acc: 0.2293\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3695 - acc: 0.2306\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3647 - acc: 0.2319\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3606 - acc: 0.2335\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3552 - acc: 0.2348\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3509 - acc: 0.2362\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3447 - acc: 0.2382\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3414 - acc: 0.2391\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3362 - acc: 0.2409\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3330 - acc: 0.2419\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3299 - acc: 0.2429\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3269 - acc: 0.2445\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3254 - acc: 0.2445\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3213 - acc: 0.2458\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3160 - acc: 0.2476\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3134 - acc: 0.2483\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3076 - acc: 0.2503\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3068 - acc: 0.2508\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3043 - acc: 0.2511\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3001 - acc: 0.2527\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2999 - acc: 0.2526\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2970 - acc: 0.2535\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2949 - acc: 0.2542\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2911 - acc: 0.2555\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2861 - acc: 0.2573\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2848 - acc: 0.2576\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2825 - acc: 0.2587\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2806 - acc: 0.2591\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2785 - acc: 0.2601\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2768 - acc: 0.2603\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2741 - acc: 0.2614\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2721 - acc: 0.2623\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2702 - acc: 0.2628\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2674 - acc: 0.2639\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2652 - acc: 0.2644\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2644 - acc: 0.2648\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2628 - acc: 0.2653\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2602 - acc: 0.2661\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2594 - acc: 0.2664\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2580 - acc: 0.2669\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2563 - acc: 0.2678\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2568 - acc: 0.2675\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2565 - acc: 0.2677\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2520 - acc: 0.2692\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2509 - acc: 0.2695\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2477 - acc: 0.2705\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2467 - acc: 0.2709\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2448 - acc: 0.2717\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2427 - acc: 0.2726\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2423 - acc: 0.2728\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2412 - acc: 0.2729\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2383 - acc: 0.2738\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2396 - acc: 0.2734\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2377 - acc: 0.2741\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2354 - acc: 0.2752\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2325 - acc: 0.2761\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2345 - acc: 0.2755\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2336 - acc: 0.2754\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2305 - acc: 0.2768\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2295 - acc: 0.2774\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2286 - acc: 0.2774\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2287 - acc: 0.2777\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2263 - acc: 0.2787\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2251 - acc: 0.2788\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2258 - acc: 0.2785\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2240 - acc: 0.2792\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2246 - acc: 0.2791\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2216 - acc: 0.2803\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2206 - acc: 0.2806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.05      0.09      1231\n",
      "           1       0.61      0.66      0.64     21938\n",
      "           2       0.33      0.31      0.32      4056\n",
      "           3       0.70      0.76      0.73     35160\n",
      "           4       0.49      0.20      0.29       916\n",
      "           5       0.46      0.50      0.48     21283\n",
      "           6       0.37      0.19      0.25      9333\n",
      "           7       0.39      0.39      0.39     11887\n",
      "\n",
      "    accuracy                           0.56    105804\n",
      "   macro avg       0.50      0.38      0.40    105804\n",
      "weighted avg       0.55      0.56      0.55    105804\n",
      "\n",
      "Acurácia\n",
      "0.38301443428267007\n",
      "Precisao\n",
      "0.5527866456165093\n",
      "Recall\n",
      "0.5642225246682545\n",
      "F1\n",
      "0.5519131625998394\n",
      "[[   62   252    30   188     1   483    74   141]\n",
      " [    5 14513   334  2813    31  2922   351   969]\n",
      " [    0   438  1264   909     4   763   117   561]\n",
      " [    1  2867   680 26570    97  2667   430  1848]\n",
      " [    0   117     7   446   185    74    15    72]\n",
      " [   22  3338   625  3037    25 10664  1340  2232]\n",
      " [    6  1201   302  1364    12  3161  1745  1542]\n",
      " [    6  1045   556  2614    19  2274   679  4694]]\n",
      "TRAIN: [   0    2    3 ... 1997 1998 1999] TEST: [   1    5    6    7   10   14   22   34   38   40   41   42   44   47\n",
      "   50   65   67   69   72   74   77   82   83   89   95   97  100  102\n",
      "  109  110  121  122  124  127  136  138  140  141  150  152  158  160\n",
      "  166  167  170  172  176  191  197  202  208  211  218  221  226  229\n",
      "  233  240  245  246  253  257  266  271  274  275  278  291  308  310\n",
      "  311  315  322  331  332  333  334  336  343  352  353  375  397  404\n",
      "  408  412  416  417  423  432  433  438  441  442  447  451  455  461\n",
      "  467  469  480  481  488  490  493  495  497  501  516  517  522  526\n",
      "  527  533  536  537  544  545  547  549  554  559  591  593  594  596\n",
      "  597  602  605  608  615  621  624  629  631  633  634  639  640  643\n",
      "  651  661  663  674  680  682  688  694  696  709  718  722  730  731\n",
      "  736  746  754  759  760  764  768  777  782  791  797  798  800  813\n",
      "  816  821  822  824  832  834  837  840  859  867  870  871  880  881\n",
      "  892  893  895  896  903  914  920  934  938  942  947  952  954  955\n",
      "  960  963  967  977  978  979  985  991  994 1001 1006 1012 1019 1023\n",
      " 1028 1034 1046 1049 1054 1070 1076 1078 1085 1089 1106 1109 1110 1113\n",
      " 1122 1129 1130 1137 1147 1152 1168 1170 1178 1184 1213 1216 1220 1229\n",
      " 1232 1236 1238 1240 1247 1248 1249 1250 1252 1259 1262 1263 1264 1270\n",
      " 1272 1273 1278 1279 1284 1285 1286 1289 1299 1300 1302 1311 1315 1318\n",
      " 1338 1350 1352 1355 1358 1362 1366 1368 1372 1377 1379 1383 1393 1394\n",
      " 1396 1399 1414 1421 1423 1425 1426 1433 1437 1440 1445 1455 1461 1462\n",
      " 1465 1471 1472 1475 1481 1483 1484 1486 1496 1498 1502 1505 1519 1533\n",
      " 1543 1555 1557 1564 1566 1580 1586 1597 1600 1606 1611 1620 1623 1628\n",
      " 1634 1640 1642 1645 1653 1655 1656 1660 1662 1663 1665 1666 1670 1673\n",
      " 1674 1678 1681 1691 1697 1701 1709 1712 1724 1728 1732 1733 1734 1737\n",
      " 1750 1751 1755 1757 1765 1772 1779 1783 1786 1789 1811 1812 1815 1820\n",
      " 1821 1825 1831 1847 1849 1853 1855 1856 1857 1867 1868 1874 1875 1885\n",
      " 1889 1896 1910 1915 1916 1918 1921 1924 1925 1930 1946 1947 1956 1960\n",
      " 1964 1966 1973 1977 1987 1991 1992 1994]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6208 - acc: 0.1375\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5767 - acc: 0.1531\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5542 - acc: 0.1639\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5293 - acc: 0.1745\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5184 - acc: 0.1795\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5140 - acc: 0.1813\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5099 - acc: 0.1831\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.5057 - acc: 0.1847\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4998 - acc: 0.1872\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4991 - acc: 0.1875\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4933 - acc: 0.1902\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4900 - acc: 0.1911\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4860 - acc: 0.1932\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4796 - acc: 0.1956\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4742 - acc: 0.1979\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4708 - acc: 0.1991\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4666 - acc: 0.2007\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4597 - acc: 0.2035\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4512 - acc: 0.2065\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4489 - acc: 0.2069\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4406 - acc: 0.2104\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4353 - acc: 0.2126\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4285 - acc: 0.2140\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4228 - acc: 0.2160\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4160 - acc: 0.2182\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4088 - acc: 0.2207\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.4021 - acc: 0.2230\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3960 - acc: 0.2249\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3931 - acc: 0.2256\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3843 - acc: 0.2287\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3772 - acc: 0.2308\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3713 - acc: 0.2324\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3669 - acc: 0.2338\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3617 - acc: 0.2359\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3610 - acc: 0.2360\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3530 - acc: 0.2384\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3499 - acc: 0.2391\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3455 - acc: 0.2407\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3406 - acc: 0.2418\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3351 - acc: 0.2439\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3317 - acc: 0.2450\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3275 - acc: 0.2464\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3266 - acc: 0.2466\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3229 - acc: 0.2477\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3177 - acc: 0.2489\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3157 - acc: 0.2498\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3123 - acc: 0.2510\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3095 - acc: 0.2518\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3097 - acc: 0.2518\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3045 - acc: 0.2534\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3011 - acc: 0.2547\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2967 - acc: 0.2561\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2950 - acc: 0.2565\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2924 - acc: 0.2577\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2892 - acc: 0.2587\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2876 - acc: 0.2592\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2847 - acc: 0.2601\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2835 - acc: 0.2605\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2794 - acc: 0.2617\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2780 - acc: 0.2624\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2762 - acc: 0.2629\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2745 - acc: 0.2636\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2729 - acc: 0.2641\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2713 - acc: 0.2644\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2680 - acc: 0.2659\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2669 - acc: 0.2662\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2647 - acc: 0.2670\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2623 - acc: 0.2680\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2613 - acc: 0.2681\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2590 - acc: 0.2691\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2583 - acc: 0.2695\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2566 - acc: 0.2699\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2550 - acc: 0.2704\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2531 - acc: 0.2709\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2506 - acc: 0.2718\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2500 - acc: 0.2721\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2490 - acc: 0.2722\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2470 - acc: 0.2730\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2441 - acc: 0.2743\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2445 - acc: 0.2742\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2422 - acc: 0.2748\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2424 - acc: 0.2751\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2397 - acc: 0.2757\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2383 - acc: 0.2764\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2391 - acc: 0.2761\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2369 - acc: 0.2768\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2352 - acc: 0.2776\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2345 - acc: 0.2778\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2326 - acc: 0.2784\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2311 - acc: 0.2790\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2313 - acc: 0.2789\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2322 - acc: 0.2787\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2284 - acc: 0.2801\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2269 - acc: 0.2806\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2275 - acc: 0.2802\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2244 - acc: 0.2814\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2245 - acc: 0.2816\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2241 - acc: 0.2817\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2217 - acc: 0.2824\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.2206 - acc: 0.2827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.05      0.09      1252\n",
      "           1       0.60      0.69      0.64     21372\n",
      "           2       0.35      0.28      0.31      3878\n",
      "           3       0.73      0.74      0.74     34210\n",
      "           4       0.38      0.21      0.27       697\n",
      "           5       0.45      0.53      0.49     20636\n",
      "           6       0.36      0.19      0.25      9142\n",
      "           7       0.40      0.39      0.40     11665\n",
      "\n",
      "    accuracy                           0.57    102852\n",
      "   macro avg       0.49      0.38      0.40    102852\n",
      "weighted avg       0.56      0.57      0.56    102852\n",
      "\n",
      "Acurácia\n",
      "0.38427417981596257\n",
      "Precisao\n",
      "0.5600337124993879\n",
      "Recall\n",
      "0.5684673122545016\n",
      "F1\n",
      "0.5573231146965777\n",
      "[[   59   274    27   137     1   540    81   133]\n",
      " [   11 14684   234  2127    20  3159   356   781]\n",
      " [    0   411  1070   828    16   877   125   551]\n",
      " [    0  3248   509 25280   158  2745   404  1866]\n",
      " [    0    65    10   327   149    76    12    58]\n",
      " [   19  3297   482  2607    13 10898  1221  2099]\n",
      " [    7  1217   265  1117     6  3430  1723  1377]\n",
      " [    2  1238   419  2092    25  2466   818  4605]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_32 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_35 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_36 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_37 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_38 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_39 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 1,790,408\n",
      "Trainable params: 1,790,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.3940936791975221, 0.3753872389035599, 0.38486298867640384, 0.38301443428267007, 0.38427417981596257]\n",
      "0.3843265041752237\n",
      "Precision total\n",
      "[0.5697194021095976, 0.5489025363495098, 0.5504736874946211, 0.5527866456165093, 0.5600337124993879]\n",
      "0.5563831968139251\n",
      "Recalls total\n",
      "[0.5762026616946185, 0.5587655353728489, 0.5627318801358068, 0.5642225246682545, 0.5684673122545016]\n",
      "0.566077982825206\n",
      "F1 total\n",
      "[0.5620931482303724, 0.5413461088269991, 0.5480453520110679, 0.5519131625998394, 0.5573231146965777]\n",
      "0.5521441772729713\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accuq8)\n",
    "a = np.array(accuq8)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "p = np.array(precisionsq8)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "r = np.array(recallsq8)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1q8)\n",
    "f = np.array(f1q8)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
