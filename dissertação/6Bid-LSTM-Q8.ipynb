{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 17:25:47.946565  2168 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 17:25:47.952528  2168 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 17:25:47.954522  2168 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 17:25:47.955520  2168 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1996 1997 1998] TEST: [   7   10   12   21   23   32   42   47   51   54   63   70   77   83\n",
      "   88  105  108  113  114  125  126  134  135  136  141  142  148  149\n",
      "  150  158  164  183  191  192  195  201  206  211  217  219  224  233\n",
      "  234  243  245  247  249  250  255  256  268  271  273  274  283  291\n",
      "  295  300  302  305  307  319  326  328  337  340  349  375  376  378\n",
      "  385  387  388  389  390  392  393  398  410  417  421  429  431  434\n",
      "  438  441  443  449  459  463  464  465  470  472  478  480  482  485\n",
      "  497  503  506  509  513  518  519  520  527  528  534  538  539  544\n",
      "  554  559  560  563  569  571  578  583  584  588  591  592  603  615\n",
      "  618  624  625  627  634  643  645  652  653  656  659  660  662  665\n",
      "  673  679  688  694  702  704  706  720  721  723  729  738  739  743\n",
      "  746  749  759  764  766  768  773  783  790  791  793  796  798  801\n",
      "  806  807  812  817  819  830  833  836  839  840  845  849  865  869\n",
      "  874  881  889  893  899  901  911  914  915  917  923  924  928  933\n",
      "  941  943  946  950  955  961  966  968  972  989  990  996  997 1004\n",
      " 1010 1011 1012 1034 1041 1045 1049 1050 1053 1054 1060 1065 1079 1081\n",
      " 1090 1092 1096 1097 1103 1112 1113 1114 1117 1138 1155 1163 1171 1176\n",
      " 1185 1195 1199 1204 1206 1213 1218 1239 1243 1251 1252 1257 1259 1269\n",
      " 1271 1273 1278 1281 1285 1286 1289 1291 1299 1300 1307 1312 1319 1324\n",
      " 1327 1342 1360 1363 1368 1374 1378 1379 1380 1381 1382 1384 1388 1394\n",
      " 1400 1401 1412 1415 1417 1422 1424 1430 1433 1435 1438 1445 1447 1450\n",
      " 1455 1463 1464 1465 1466 1474 1482 1489 1490 1492 1498 1499 1505 1524\n",
      " 1526 1529 1530 1535 1536 1541 1543 1544 1545 1557 1558 1564 1565 1568\n",
      " 1576 1584 1588 1590 1595 1604 1609 1613 1618 1621 1638 1641 1658 1666\n",
      " 1675 1681 1682 1683 1684 1686 1691 1710 1713 1720 1721 1722 1724 1725\n",
      " 1732 1736 1743 1744 1746 1756 1771 1774 1776 1786 1788 1791 1793 1804\n",
      " 1810 1819 1822 1833 1835 1836 1838 1839 1840 1844 1846 1849 1867 1871\n",
      " 1872 1874 1879 1883 1900 1901 1904 1905 1920 1928 1930 1938 1941 1943\n",
      " 1944 1947 1968 1972 1973 1979 1986 1999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 17:25:49.761718  2168 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.6173 - acc: 0.1405\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5704 - acc: 0.1577\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5431 - acc: 0.1692\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5233 - acc: 0.1769\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5155 - acc: 0.1803\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5094 - acc: 0.1831\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5071 - acc: 0.1844\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5021 - acc: 0.1865\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4974 - acc: 0.1886\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4933 - acc: 0.1903\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4901 - acc: 0.1917\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4863 - acc: 0.1925\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4814 - acc: 0.1949\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4795 - acc: 0.1956\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4727 - acc: 0.1980\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4691 - acc: 0.1996\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4618 - acc: 0.2023\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4568 - acc: 0.2043\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4496 - acc: 0.2070\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4438 - acc: 0.2086\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4436 - acc: 0.2089\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4312 - acc: 0.2133\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4244 - acc: 0.2152\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4179 - acc: 0.2171\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4141 - acc: 0.2189\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4064 - acc: 0.2212\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4000 - acc: 0.2233\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3934 - acc: 0.2256\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3879 - acc: 0.2272\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3825 - acc: 0.2292\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3769 - acc: 0.2306\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3696 - acc: 0.2333\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3662 - acc: 0.2342\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3608 - acc: 0.2356\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3584 - acc: 0.2366\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3545 - acc: 0.2380\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3466 - acc: 0.2406\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3435 - acc: 0.2414\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3407 - acc: 0.2420\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3348 - acc: 0.2445\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3311 - acc: 0.2455\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3285 - acc: 0.2464\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3246 - acc: 0.2475\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3193 - acc: 0.2492\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3163 - acc: 0.2503\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3131 - acc: 0.2511\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3129 - acc: 0.2513\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3053 - acc: 0.2537\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3032 - acc: 0.2544\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3022 - acc: 0.2550\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2971 - acc: 0.2565\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2975 - acc: 0.2563\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2941 - acc: 0.2577\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2914 - acc: 0.2585\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2874 - acc: 0.2597\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2885 - acc: 0.2593\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2824 - acc: 0.2614\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2805 - acc: 0.2620\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2786 - acc: 0.2627\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2784 - acc: 0.2631\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2749 - acc: 0.2638\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2727 - acc: 0.2649\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2698 - acc: 0.2659\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2697 - acc: 0.2657\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2674 - acc: 0.2667\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2671 - acc: 0.2667\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2646 - acc: 0.2678\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2621 - acc: 0.2682\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2602 - acc: 0.2694\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2581 - acc: 0.2699\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2555 - acc: 0.2710\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2548 - acc: 0.2711\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2539 - acc: 0.2715\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2536 - acc: 0.2712\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2526 - acc: 0.2718\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2494 - acc: 0.2728\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2467 - acc: 0.2741\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2460 - acc: 0.2741\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2443 - acc: 0.2750\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2455 - acc: 0.2745\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2432 - acc: 0.2752\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2413 - acc: 0.2760\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2401 - acc: 0.2763\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2399 - acc: 0.2762\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2379 - acc: 0.2770\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2355 - acc: 0.2779\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2349 - acc: 0.2781\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2334 - acc: 0.2787\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2357 - acc: 0.2781\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2326 - acc: 0.2792\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2318 - acc: 0.2792\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2306 - acc: 0.2799\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2288 - acc: 0.2805\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2282 - acc: 0.2808\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2263 - acc: 0.2814\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2251 - acc: 0.2819\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2247 - acc: 0.2819\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2242 - acc: 0.2819\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2239 - acc: 0.2823\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2217 - acc: 0.2829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.03      0.06      1235\n",
      "           1       0.58      0.67      0.62     21358\n",
      "           2       0.29      0.31      0.30      3819\n",
      "           3       0.72      0.73      0.73     34006\n",
      "           4       0.41      0.20      0.27       673\n",
      "           5       0.45      0.49      0.47     20894\n",
      "           6       0.36      0.16      0.22      9099\n",
      "           7       0.39      0.40      0.39     11693\n",
      "\n",
      "    accuracy                           0.56    102777\n",
      "   macro avg       0.47      0.38      0.38    102777\n",
      "weighted avg       0.55      0.56      0.54    102777\n",
      "\n",
      "Acurácia\n",
      "0.3754708295099102\n",
      "Precisao\n",
      "0.5464038536010872\n",
      "Recall\n",
      "0.5553966354339979\n",
      "F1\n",
      "0.5438318811152377\n",
      "[[   37   255    47   150     1   531    70   144]\n",
      " [    6 14351   370  2396    26  2901   337   971]\n",
      " [    0   508  1200   751     5   710    99   546]\n",
      " [    4  3112   778 24889   116  2746   344  2017]\n",
      " [    0    93    11   297   135    73     6    58]\n",
      " [   14  3740   714  2770    15 10332  1142  2167]\n",
      " [    8  1385   345  1217    11  3211  1466  1456]\n",
      " [    1  1213   638  2131    23  2364   651  4672]]\n",
      "TRAIN: [   0    1    2 ... 1996 1998 1999] TEST: [  25   26   28   36   38   45   48   49   50   57   61   65   72   73\n",
      "   75   81   91  103  104  112  119  123  128  131  138  145  162  163\n",
      "  169  176  181  182  184  186  188  210  216  218  222  227  230  236\n",
      "  242  248  254  258  259  261  262  263  265  277  279  280  286  287\n",
      "  290  292  297  298  301  304  311  321  322  324  325  332  333  334\n",
      "  341  353  354  367  377  381  383  395  403  406  418  420  423  424\n",
      "  430  439  440  442  446  451  452  453  455  456  460  461  473  475\n",
      "  481  486  500  505  512  517  526  531  537  548  549  553  561  566\n",
      "  570  582  589  597  598  600  605  607  613  617  626  628  631  632\n",
      "  635  637  639  647  651  655  657  666  667  681  685  686  687  689\n",
      "  697  701  705  707  709  717  722  724  732  733  741  772  774  776\n",
      "  779  780  784  788  789  797  799  804  810  813  818  820  823  824\n",
      "  826  827  835  847  851  855  856  866  878  888  894  907  910  918\n",
      "  919  925  931  935  936  937  939  952  953  957  962  969  970  975\n",
      "  980  983  987  991  992 1002 1014 1019 1021 1025 1029 1032 1043 1046\n",
      " 1056 1061 1062 1064 1070 1080 1082 1084 1086 1102 1104 1106 1120 1121\n",
      " 1127 1130 1135 1141 1144 1146 1149 1150 1160 1161 1162 1172 1175 1182\n",
      " 1192 1197 1201 1202 1207 1210 1216 1217 1227 1235 1236 1237 1238 1255\n",
      " 1256 1258 1260 1264 1266 1270 1276 1280 1283 1294 1296 1311 1317 1323\n",
      " 1328 1330 1338 1344 1346 1347 1348 1352 1355 1362 1365 1372 1376 1385\n",
      " 1390 1395 1406 1407 1408 1410 1416 1419 1423 1428 1431 1434 1437 1439\n",
      " 1458 1459 1460 1461 1470 1473 1475 1479 1483 1488 1495 1497 1504 1506\n",
      " 1515 1520 1531 1533 1538 1546 1554 1560 1567 1569 1578 1581 1593 1601\n",
      " 1605 1614 1615 1630 1635 1637 1640 1642 1645 1653 1656 1667 1668 1669\n",
      " 1672 1707 1709 1727 1730 1734 1740 1749 1753 1755 1758 1767 1768 1772\n",
      " 1773 1775 1778 1781 1783 1785 1787 1789 1790 1812 1821 1823 1827 1829\n",
      " 1837 1842 1851 1852 1853 1870 1878 1889 1891 1894 1899 1906 1908 1911\n",
      " 1913 1916 1919 1927 1929 1933 1936 1939 1951 1952 1953 1957 1959 1962\n",
      " 1967 1971 1978 1980 1981 1983 1991 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.6176 - acc: 0.1421\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5695 - acc: 0.1587\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5429 - acc: 0.1702\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5265 - acc: 0.1769\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5197 - acc: 0.1800\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5152 - acc: 0.1820\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5093 - acc: 0.1843\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5044 - acc: 0.1864\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4990 - acc: 0.1889\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4981 - acc: 0.1891\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4926 - acc: 0.1915\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4909 - acc: 0.1921\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4844 - acc: 0.1948\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4807 - acc: 0.1963\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4787 - acc: 0.1975\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4707 - acc: 0.2003\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4687 - acc: 0.2007\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4671 - acc: 0.2013\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4578 - acc: 0.2047\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4518 - acc: 0.2071\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4468 - acc: 0.2086\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4428 - acc: 0.2101\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4347 - acc: 0.2129\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4290 - acc: 0.2149\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4248 - acc: 0.2161\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4166 - acc: 0.2192\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4117 - acc: 0.2206\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4026 - acc: 0.2238\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3974 - acc: 0.2254\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3921 - acc: 0.2271\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3886 - acc: 0.2281\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3842 - acc: 0.2294\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3745 - acc: 0.2326\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3726 - acc: 0.2335\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3685 - acc: 0.2345\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3620 - acc: 0.2367\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3575 - acc: 0.2385\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3536 - acc: 0.2394\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3534 - acc: 0.2393\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3442 - acc: 0.2427\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3411 - acc: 0.2432\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3370 - acc: 0.2445\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3350 - acc: 0.2453\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3303 - acc: 0.2465\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3256 - acc: 0.2482\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3222 - acc: 0.2498\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3194 - acc: 0.2502\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3175 - acc: 0.2509\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3145 - acc: 0.2521\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3153 - acc: 0.2520\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3090 - acc: 0.2540\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3056 - acc: 0.2549\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3017 - acc: 0.2561\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2986 - acc: 0.2573\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2969 - acc: 0.2578\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2945 - acc: 0.2586\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2913 - acc: 0.2595\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2900 - acc: 0.2601\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2876 - acc: 0.2608\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2851 - acc: 0.2614\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2823 - acc: 0.2628\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2820 - acc: 0.2628\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2806 - acc: 0.2631\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2770 - acc: 0.2643\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2748 - acc: 0.2652\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2725 - acc: 0.2662\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2715 - acc: 0.2661\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2697 - acc: 0.2668\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2659 - acc: 0.2684\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2643 - acc: 0.2685\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2650 - acc: 0.2685\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2636 - acc: 0.2690\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2611 - acc: 0.2699\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2609 - acc: 0.2701\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2605 - acc: 0.2702\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2564 - acc: 0.2715\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2558 - acc: 0.2722\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2555 - acc: 0.2720\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2523 - acc: 0.2730\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2505 - acc: 0.2736\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2484 - acc: 0.2747\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2469 - acc: 0.2751\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2482 - acc: 0.2743\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2467 - acc: 0.2751\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2426 - acc: 0.2767\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2420 - acc: 0.2767\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2420 - acc: 0.2767\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2404 - acc: 0.2773\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2393 - acc: 0.2777\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2377 - acc: 0.2784\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2394 - acc: 0.2776\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2381 - acc: 0.2784\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2354 - acc: 0.2792\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2331 - acc: 0.2800\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2330 - acc: 0.2798\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2328 - acc: 0.2803\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2327 - acc: 0.2801\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2300 - acc: 0.2809\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2288 - acc: 0.2815\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2297 - acc: 0.2810\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.04      0.08      1219\n",
      "           1       0.62      0.64      0.63     20742\n",
      "           2       0.34      0.29      0.31      3960\n",
      "           3       0.70      0.79      0.74     33911\n",
      "           4       0.48      0.28      0.35       653\n",
      "           5       0.47      0.51      0.49     20746\n",
      "           6       0.39      0.17      0.24      8826\n",
      "           7       0.40      0.40      0.40     11448\n",
      "\n",
      "    accuracy                           0.57    101505\n",
      "   macro avg       0.52      0.39      0.41    101505\n",
      "weighted avg       0.56      0.57      0.56    101505\n",
      "\n",
      "Acurácia\n",
      "0.3912221198140546\n",
      "Precisao\n",
      "0.5619392467641846\n",
      "Recall\n",
      "0.573971725530762\n",
      "F1\n",
      "0.559247163044368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   49   246    42   187     3   489    73   130]\n",
      " [    3 13371   260  2876    21  3002   285   924]\n",
      " [    4   437  1143   975    16   715   109   561]\n",
      " [    1  2212   555 26703   105  2315   262  1758]\n",
      " [    0    46    17   341   182    27     7    33]\n",
      " [    6  3119   568  3149    20 10672  1066  2146]\n",
      " [    2  1024   305  1343    12  3230  1515  1395]\n",
      " [    2  1054   515  2412    21  2255   563  4626]]\n",
      "TRAIN: [   0    2    4 ... 1995 1997 1999] TEST: [   1    3    9   15   17   20   24   29   30   34   37   40   41   44\n",
      "   46   53   67   68   74   84   85   98  107  111  120  130  132  143\n",
      "  147  151  155  160  173  178  179  187  198  202  204  205  208  215\n",
      "  220  221  225  226  240  244  246  251  257  276  285  303  314  315\n",
      "  329  339  342  350  351  357  363  366  379  380  386  401  408  419\n",
      "  422  425  428  433  436  437  445  450  457  462  471  477  479  483\n",
      "  488  492  498  499  502  507  516  521  523  524  525  529  530  540\n",
      "  542  543  551  557  562  565  568  572  577  579  581  587  608  611\n",
      "  620  621  622  623  633  636  644  649  650  654  671  672  675  676\n",
      "  690  695  696  713  716  719  726  727  734  740  744  747  750  754\n",
      "  755  757  760  770  771  782  809  811  814  816  822  828  831  832\n",
      "  834  837  842  844  852  854  870  871  872  873  879  883  885  890\n",
      "  895  896  897  902  903  904  922  940  942  945  947  949  956  958\n",
      "  960  976  978  981  985  986  993  995  999 1008 1013 1016 1022 1027\n",
      " 1030 1033 1035 1036 1038 1039 1044 1055 1059 1071 1072 1076 1087 1088\n",
      " 1094 1098 1099 1108 1115 1125 1128 1129 1134 1139 1140 1157 1159 1164\n",
      " 1165 1167 1179 1184 1188 1196 1198 1200 1203 1208 1209 1211 1219 1220\n",
      " 1221 1223 1231 1233 1234 1240 1241 1246 1247 1249 1253 1261 1262 1268\n",
      " 1274 1287 1292 1293 1297 1301 1302 1306 1313 1314 1315 1316 1318 1322\n",
      " 1326 1329 1336 1339 1340 1343 1350 1354 1357 1366 1375 1383 1387 1389\n",
      " 1393 1399 1402 1403 1411 1413 1425 1426 1429 1432 1441 1443 1449 1451\n",
      " 1452 1454 1471 1478 1480 1484 1491 1501 1502 1503 1512 1516 1517 1519\n",
      " 1523 1534 1537 1542 1547 1553 1570 1571 1572 1573 1574 1580 1585 1587\n",
      " 1589 1591 1592 1594 1596 1600 1607 1608 1612 1619 1626 1627 1634 1639\n",
      " 1643 1646 1648 1650 1652 1655 1657 1659 1662 1665 1673 1674 1677 1678\n",
      " 1679 1687 1695 1697 1698 1705 1712 1715 1716 1717 1728 1733 1741 1750\n",
      " 1752 1760 1761 1797 1802 1803 1808 1809 1813 1815 1820 1830 1831 1845\n",
      " 1859 1860 1862 1868 1869 1895 1921 1923 1925 1931 1942 1945 1946 1950\n",
      " 1958 1963 1976 1982 1985 1989 1996 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.6177 - acc: 0.1370\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5704 - acc: 0.1529\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5501 - acc: 0.1627\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5271 - acc: 0.1724\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5170 - acc: 0.1761\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5114 - acc: 0.1792\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5056 - acc: 0.1814\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5016 - acc: 0.1831\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4949 - acc: 0.1855\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4929 - acc: 0.1868\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4884 - acc: 0.1887\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4879 - acc: 0.1887\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4843 - acc: 0.1904\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4776 - acc: 0.1929\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4724 - acc: 0.1951\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4683 - acc: 0.1966\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4656 - acc: 0.1976\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4589 - acc: 0.2001\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4554 - acc: 0.2013\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4462 - acc: 0.2048\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4445 - acc: 0.2050\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4340 - acc: 0.2091\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4268 - acc: 0.2118\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4211 - acc: 0.2135\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4144 - acc: 0.2167\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4102 - acc: 0.2179\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4021 - acc: 0.2204\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3958 - acc: 0.2216\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3914 - acc: 0.2235\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3854 - acc: 0.2257\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3796 - acc: 0.2274\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3742 - acc: 0.2289\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3680 - acc: 0.2308\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3594 - acc: 0.2333\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3564 - acc: 0.2344\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3538 - acc: 0.2351\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3495 - acc: 0.2366\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3424 - acc: 0.2391\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3382 - acc: 0.2399\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3361 - acc: 0.2409\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3339 - acc: 0.2414\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3281 - acc: 0.2436\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3234 - acc: 0.2451\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3188 - acc: 0.2463\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3157 - acc: 0.2472\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3122 - acc: 0.2487\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3093 - acc: 0.2496\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3050 - acc: 0.2509\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3039 - acc: 0.2513\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3019 - acc: 0.2520\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2971 - acc: 0.2535\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2946 - acc: 0.2545\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2930 - acc: 0.2550\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2901 - acc: 0.2557\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2944 - acc: 0.2545\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2870 - acc: 0.2568\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2815 - acc: 0.2586\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2790 - acc: 0.2599\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2770 - acc: 0.2603\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2760 - acc: 0.2606\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2736 - acc: 0.2615\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2703 - acc: 0.2626\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2681 - acc: 0.2631\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2676 - acc: 0.2635\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2642 - acc: 0.2646\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2636 - acc: 0.2649\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2611 - acc: 0.2658\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2586 - acc: 0.2666\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2565 - acc: 0.2673\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2554 - acc: 0.2680\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2531 - acc: 0.2688\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2526 - acc: 0.2690\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2514 - acc: 0.2691\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2498 - acc: 0.2700\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2480 - acc: 0.2705\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2460 - acc: 0.2710\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2452 - acc: 0.2714\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2444 - acc: 0.2716\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2430 - acc: 0.2721\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2425 - acc: 0.2723\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2410 - acc: 0.2732\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2397 - acc: 0.2735\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2384 - acc: 0.2739\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2375 - acc: 0.2741\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2344 - acc: 0.2753\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2329 - acc: 0.2760\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2334 - acc: 0.2756\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2323 - acc: 0.2762\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2313 - acc: 0.2763\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2314 - acc: 0.2765\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2298 - acc: 0.2772\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2269 - acc: 0.2784\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2266 - acc: 0.2784\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2260 - acc: 0.2786\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2253 - acc: 0.2786\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2228 - acc: 0.2797\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2232 - acc: 0.2794\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2220 - acc: 0.2799\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2201 - acc: 0.2806\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2202 - acc: 0.2805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.05      0.09      1282\n",
      "           1       0.61      0.69      0.65     23233\n",
      "           2       0.32      0.27      0.29      4244\n",
      "           3       0.72      0.74      0.73     34575\n",
      "           4       0.41      0.30      0.34       663\n",
      "           5       0.45      0.50      0.48     20964\n",
      "           6       0.36      0.19      0.25      9261\n",
      "           7       0.40      0.41      0.40     11826\n",
      "\n",
      "    accuracy                           0.57    106048\n",
      "   macro avg       0.48      0.39      0.40    106048\n",
      "weighted avg       0.56      0.57      0.56    106048\n",
      "\n",
      "Acurácia\n",
      "0.39211172261178406\n",
      "Precisao\n",
      "0.556405114533617\n",
      "Recall\n",
      "0.5662247284248643\n",
      "F1\n",
      "0.5552144438997556\n",
      "[[   63   278    41   180     2   512    71   135]\n",
      " [    6 15935   268  2531    33  3185   371   904]\n",
      " [    1   485  1135   935    13   870   172   633]\n",
      " [    3  2857   676 25599   145  2745   453  2097]\n",
      " [    0   112    10   225   196    38    11    71]\n",
      " [   24  3691   557  2756    31 10565  1259  2081]\n",
      " [    9  1372   291  1205    16  3185  1719  1464]\n",
      " [    3  1301   540  2081    41  2311   714  4835]]\n",
      "TRAIN: [   0    1    3 ... 1997 1998 1999] TEST: [   2    6    8   14   16   22   27   31   39   43   59   60   64   69\n",
      "   76   80   82   87   90   93  101  115  116  117  118  124  129  144\n",
      "  152  153  157  161  165  166  167  168  172  174  185  190  193  196\n",
      "  199  200  212  213  214  228  231  232  235  237  239  253  260  266\n",
      "  270  272  275  278  281  294  296  306  310  312  313  316  317  320\n",
      "  327  330  335  336  344  345  347  359  360  362  369  370  382  396\n",
      "  404  405  409  411  414  415  427  444  454  466  476  484  489  493\n",
      "  494  496  501  514  532  533  536  547  550  552  555  558  567  575\n",
      "  576  586  590  593  595  596  602  606  609  610  614  629  640  642\n",
      "  646  658  663  664  670  677  678  680  682  684  691  693  698  699\n",
      "  708  711  715  725  730  736  742  751  752  756  758  762  775  785\n",
      "  786  794  795  800  802  803  805  815  821  825  829  841  843  846\n",
      "  853  859  861  862  867  880  884  886  898  905  908  909  913  920\n",
      "  921  927  930  934  938  964  965  967  984  994 1000 1015 1020 1023\n",
      " 1026 1031 1040 1048 1051 1057 1063 1067 1069 1074 1075 1085 1091 1095\n",
      " 1105 1109 1111 1124 1132 1133 1137 1142 1143 1148 1152 1154 1156 1158\n",
      " 1166 1168 1170 1174 1178 1180 1181 1183 1187 1189 1191 1194 1205 1212\n",
      " 1222 1226 1229 1230 1244 1248 1254 1265 1267 1272 1275 1279 1282 1295\n",
      " 1304 1309 1320 1334 1335 1337 1358 1361 1367 1373 1377 1391 1392 1397\n",
      " 1398 1405 1414 1418 1427 1436 1442 1444 1456 1457 1462 1468 1469 1472\n",
      " 1477 1486 1493 1494 1500 1507 1509 1510 1513 1514 1525 1527 1528 1540\n",
      " 1549 1552 1556 1559 1561 1562 1566 1579 1582 1586 1606 1610 1611 1617\n",
      " 1623 1624 1628 1631 1636 1649 1651 1654 1660 1661 1663 1664 1676 1689\n",
      " 1690 1692 1693 1696 1700 1701 1702 1703 1704 1714 1718 1719 1723 1726\n",
      " 1729 1731 1735 1738 1739 1742 1747 1751 1754 1757 1759 1765 1769 1782\n",
      " 1792 1800 1801 1806 1807 1814 1816 1824 1826 1832 1834 1843 1847 1848\n",
      " 1856 1857 1858 1861 1863 1865 1873 1875 1876 1881 1884 1887 1892 1896\n",
      " 1902 1903 1909 1910 1914 1918 1922 1924 1934 1937 1940 1948 1955 1956\n",
      " 1961 1964 1969 1974 1975 1992 1993 1994]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.6191 - acc: 0.1428\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5689 - acc: 0.1584\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5448 - acc: 0.1685\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5269 - acc: 0.1759\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5175 - acc: 0.1803\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5106 - acc: 0.1833\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5057 - acc: 0.1853\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5047 - acc: 0.1862\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4977 - acc: 0.1887\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4944 - acc: 0.1903\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4918 - acc: 0.1912\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4869 - acc: 0.1931\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4855 - acc: 0.1938\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4789 - acc: 0.1966\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4731 - acc: 0.1988\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4714 - acc: 0.1996\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4628 - acc: 0.2026\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4585 - acc: 0.2041\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4518 - acc: 0.2068\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4469 - acc: 0.2087\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4391 - acc: 0.2112\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4348 - acc: 0.2128\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4286 - acc: 0.2151\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4184 - acc: 0.2180\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4113 - acc: 0.2209\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4048 - acc: 0.2228\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3986 - acc: 0.2248\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3924 - acc: 0.2265\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3870 - acc: 0.2283\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3809 - acc: 0.2304\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3763 - acc: 0.2317\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3689 - acc: 0.2341\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3669 - acc: 0.2347\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3622 - acc: 0.2360\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3614 - acc: 0.2369\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3525 - acc: 0.2394\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3446 - acc: 0.2418\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3424 - acc: 0.2424\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3388 - acc: 0.2436\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3344 - acc: 0.2449\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3324 - acc: 0.2456\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3264 - acc: 0.2475\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3226 - acc: 0.2490\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3176 - acc: 0.2508\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3147 - acc: 0.2515\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3125 - acc: 0.2520\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3102 - acc: 0.2530\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3087 - acc: 0.2536\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3063 - acc: 0.2543\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2994 - acc: 0.2567\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2968 - acc: 0.2574\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2936 - acc: 0.2583\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2922 - acc: 0.2588\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2897 - acc: 0.2595\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2870 - acc: 0.2605\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2844 - acc: 0.2617\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2855 - acc: 0.2612\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2820 - acc: 0.2624\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2788 - acc: 0.2634\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2766 - acc: 0.2643\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2744 - acc: 0.2649\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2717 - acc: 0.2660\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2701 - acc: 0.2666\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2685 - acc: 0.2669\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2668 - acc: 0.2677\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2663 - acc: 0.2676\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2642 - acc: 0.2685\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2621 - acc: 0.2692\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2606 - acc: 0.2699\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2579 - acc: 0.2704\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2555 - acc: 0.2715\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2555 - acc: 0.2717\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2540 - acc: 0.2720\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2524 - acc: 0.2727\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2511 - acc: 0.2731\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2487 - acc: 0.2740\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2483 - acc: 0.2740\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2455 - acc: 0.2751\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2456 - acc: 0.2748\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2441 - acc: 0.2755\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2435 - acc: 0.2757\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2416 - acc: 0.2765\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2390 - acc: 0.2774\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2399 - acc: 0.2771\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2375 - acc: 0.2780\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2366 - acc: 0.2784\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2368 - acc: 0.2784\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2345 - acc: 0.2790\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2338 - acc: 0.2794\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2315 - acc: 0.2800\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2307 - acc: 0.2803\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2299 - acc: 0.2806\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2292 - acc: 0.2807\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2280 - acc: 0.2814\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2283 - acc: 0.2812\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2266 - acc: 0.2819\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2251 - acc: 0.2824\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2243 - acc: 0.2827\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2229 - acc: 0.2834\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2220 - acc: 0.2837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.03      0.06      1236\n",
      "           1       0.61      0.65      0.63     21810\n",
      "           2       0.32      0.28      0.30      3818\n",
      "           3       0.71      0.76      0.73     33145\n",
      "           4       0.49      0.26      0.34       875\n",
      "           5       0.44      0.53      0.48     20260\n",
      "           6       0.37      0.16      0.23      9183\n",
      "           7       0.40      0.38      0.39     11667\n",
      "\n",
      "    accuracy                           0.56    101994\n",
      "   macro avg       0.50      0.38      0.39    101994\n",
      "weighted avg       0.55      0.56      0.55    101994\n",
      "\n",
      "Acurácia\n",
      "0.38188467744489263\n",
      "Precisao\n",
      "0.5520270768973159\n",
      "Recall\n",
      "0.5616212718395199\n",
      "F1\n",
      "0.547937538565673\n",
      "[[   37   252    33   205     1   523    66   119]\n",
      " [    9 14110   315  2655    30  3471   348   872]\n",
      " [    0   431  1070   785     6   868   109   549]\n",
      " [    0  2578   527 25139   151  2691   306  1753]\n",
      " [    0    86    20   390   231    79    18    51]\n",
      " [    6  3164   571  2787    15 10726  1002  1989]\n",
      " [    3  1282   273  1305    19  3472  1486  1343]\n",
      " [    0  1160   553  2252    20  2552   647  4483]]\n",
      "TRAIN: [   1    2    3 ... 1997 1998 1999] TEST: [   0    4    5   11   13   18   19   33   35   52   55   56   58   62\n",
      "   66   71   78   79   86   89   92   94   95   96   97   99  100  102\n",
      "  106  109  110  121  122  127  133  137  139  140  146  154  156  159\n",
      "  170  171  175  177  180  189  194  197  203  207  209  223  229  238\n",
      "  241  252  264  267  269  282  284  288  289  293  299  308  309  318\n",
      "  323  331  338  343  346  348  352  355  356  358  361  364  365  368\n",
      "  371  372  373  374  384  391  394  397  399  400  402  407  412  413\n",
      "  416  426  432  435  447  448  458  467  468  469  474  487  490  491\n",
      "  495  504  508  510  511  515  522  535  541  545  546  556  564  573\n",
      "  574  580  585  594  599  601  604  612  616  619  630  638  641  648\n",
      "  661  668  669  674  683  692  700  703  710  712  714  718  728  731\n",
      "  735  737  745  748  753  761  763  765  767  769  777  778  781  787\n",
      "  792  808  838  848  850  857  858  860  863  864  868  875  876  877\n",
      "  882  887  891  892  900  906  912  916  926  929  932  944  948  951\n",
      "  954  959  963  971  973  974  977  979  982  988  998 1001 1003 1005\n",
      " 1006 1007 1009 1017 1018 1024 1028 1037 1042 1047 1052 1058 1066 1068\n",
      " 1073 1077 1078 1083 1089 1093 1100 1101 1107 1110 1116 1118 1119 1122\n",
      " 1123 1126 1131 1136 1145 1147 1151 1153 1169 1173 1177 1186 1190 1193\n",
      " 1214 1215 1224 1225 1228 1232 1242 1245 1250 1263 1277 1284 1288 1290\n",
      " 1298 1303 1305 1308 1310 1321 1325 1331 1332 1333 1341 1345 1349 1351\n",
      " 1353 1356 1359 1364 1369 1370 1371 1386 1396 1404 1409 1420 1421 1440\n",
      " 1446 1448 1453 1467 1476 1481 1485 1487 1496 1508 1511 1518 1521 1522\n",
      " 1532 1539 1548 1550 1551 1555 1563 1575 1577 1583 1597 1598 1599 1602\n",
      " 1603 1616 1620 1622 1625 1629 1632 1633 1644 1647 1670 1671 1680 1685\n",
      " 1688 1694 1699 1706 1708 1711 1737 1745 1748 1762 1763 1764 1766 1770\n",
      " 1777 1779 1780 1784 1794 1795 1796 1798 1799 1805 1811 1817 1818 1825\n",
      " 1828 1841 1850 1854 1855 1864 1866 1877 1880 1882 1885 1886 1888 1890\n",
      " 1893 1897 1898 1907 1912 1915 1917 1926 1932 1935 1949 1954 1960 1965\n",
      " 1966 1970 1977 1984 1987 1988 1990 1995]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.6168 - acc: 0.1408\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5695 - acc: 0.1565\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5454 - acc: 0.1668\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5244 - acc: 0.1756\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5131 - acc: 0.1806\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5067 - acc: 0.1835\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5027 - acc: 0.1853\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4984 - acc: 0.1870\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4954 - acc: 0.1886\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4930 - acc: 0.1897\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4874 - acc: 0.1917\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4833 - acc: 0.1939\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4793 - acc: 0.1954\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4752 - acc: 0.1965\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4710 - acc: 0.1983\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4680 - acc: 0.1992\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4600 - acc: 0.2020\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4552 - acc: 0.2037\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4483 - acc: 0.2067\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4437 - acc: 0.2084\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4357 - acc: 0.2107\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4312 - acc: 0.2125\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4247 - acc: 0.2152\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4144 - acc: 0.2184\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4090 - acc: 0.2209\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4082 - acc: 0.2205\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.4007 - acc: 0.2237\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3935 - acc: 0.2257\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3870 - acc: 0.2280\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3823 - acc: 0.2295\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3759 - acc: 0.2307\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3701 - acc: 0.2329\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3658 - acc: 0.2337\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3596 - acc: 0.2358\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3541 - acc: 0.2377\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3518 - acc: 0.2383\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3452 - acc: 0.2402\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3412 - acc: 0.2416\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3387 - acc: 0.2424\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3365 - acc: 0.2433\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3295 - acc: 0.2456\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3278 - acc: 0.2460\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3237 - acc: 0.2471\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3200 - acc: 0.2483\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3161 - acc: 0.2496\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3129 - acc: 0.2505\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3122 - acc: 0.2511\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3087 - acc: 0.2518\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3043 - acc: 0.2536\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2994 - acc: 0.2551\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2986 - acc: 0.2552\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2968 - acc: 0.2560\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2936 - acc: 0.2571\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2907 - acc: 0.2584\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2879 - acc: 0.2586\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2846 - acc: 0.2604\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2821 - acc: 0.2608\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2819 - acc: 0.2608\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2797 - acc: 0.2616\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2772 - acc: 0.2623\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2747 - acc: 0.2634\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2733 - acc: 0.2641\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2709 - acc: 0.2648\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2680 - acc: 0.2656\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2658 - acc: 0.2666\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2653 - acc: 0.2667\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2643 - acc: 0.2670\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2630 - acc: 0.2676\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2596 - acc: 0.2687\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2590 - acc: 0.2689\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2562 - acc: 0.2700\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2548 - acc: 0.2703\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2529 - acc: 0.2711\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2505 - acc: 0.2718\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2537 - acc: 0.2709\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2524 - acc: 0.2714\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2478 - acc: 0.2732\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2463 - acc: 0.2735\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2443 - acc: 0.2741\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2434 - acc: 0.2744\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2428 - acc: 0.2745\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2406 - acc: 0.2755\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2395 - acc: 0.2760\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2386 - acc: 0.2764\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2365 - acc: 0.2770\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2357 - acc: 0.2774\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2341 - acc: 0.2779\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2336 - acc: 0.2779\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2320 - acc: 0.2786\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2317 - acc: 0.2786\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2321 - acc: 0.2787\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2310 - acc: 0.2792\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2279 - acc: 0.2803\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2277 - acc: 0.2803\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2260 - acc: 0.2809\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2263 - acc: 0.2806\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2237 - acc: 0.2817\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2220 - acc: 0.2822\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2226 - acc: 0.2818\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2212 - acc: 0.2825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.04      0.07      1292\n",
      "           1       0.63      0.65      0.64     21789\n",
      "           2       0.39      0.32      0.35      4030\n",
      "           3       0.71      0.76      0.73     33506\n",
      "           4       0.45      0.26      0.33       653\n",
      "           5       0.46      0.54      0.50     21221\n",
      "           6       0.38      0.20      0.27      9219\n",
      "           7       0.41      0.40      0.40     11774\n",
      "\n",
      "    accuracy                           0.57    103484\n",
      "   macro avg       0.53      0.40      0.41    103484\n",
      "weighted avg       0.56      0.57      0.56    103484\n",
      "\n",
      "Acurácia\n",
      "0.39663500158079695\n",
      "Precisao\n",
      "0.5646524426108128\n",
      "Recall\n",
      "0.5717695489157745\n",
      "F1\n",
      "0.5602676941300178\n",
      "[[   47   211    39   201     3   586    68   137]\n",
      " [    0 14132   237  2901    22  3181   364   952]\n",
      " [    0   399  1297   850     0   832   157   495]\n",
      " [    0  2394   522 25435   135  2868   360  1792]\n",
      " [    0    81     6   256   171    65    17    57]\n",
      " [   10  3040   493  2862    26 11508  1313  1969]\n",
      " [    2  1124   264  1171     8  3406  1879  1365]\n",
      " [    2   983   476  2359    14  2512   728  4700]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_24 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_25 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 1,307,208\n",
      "Trainable params: 1,307,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.3754708295099102, 0.3912221198140546, 0.39211172261178406, 0.38188467744489263, 0.39663500158079695]\n",
      "0.38746487019228765\n",
      "Precision total\n",
      "[0.5464038536010872, 0.5619392467641846, 0.556405114533617, 0.5520270768973159, 0.5646524426108128]\n",
      "0.5562855468814035\n",
      "Recalls total\n",
      "[0.5553966354339979, 0.573971725530762, 0.5662247284248643, 0.5616212718395199, 0.5717695489157745]\n",
      "0.5657967820289836\n",
      "F1 total\n",
      "[0.5438318811152377, 0.559247163044368, 0.5552144438997556, 0.547937538565673, 0.5602676941300178]\n",
      "0.5532997441510105\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accuq8)\n",
    "a = np.array(accuq8)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "p = np.array(precisionsq8)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "r = np.array(recallsq8)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1q8)\n",
    "f = np.array(f1q8)\n",
    "print(f.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
