{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNGRU, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0823 23:51:01.072951  2976 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0823 23:51:01.078912  2976 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0823 23:51:01.079908  2976 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0823 23:51:01.080906  2976 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    2    4 ... 1994 1997 1999] TEST: [   1    3    7   10   11   15   21   24   25   31   32   34   37   38\n",
      "   41   44   45   47   51   54   61   62   64   66   70   74   77   80\n",
      "   88   96   98   99  102  105  111  123  128  130  134  136  137  144\n",
      "  151  158  160  166  168  181  186  188  189  195  196  199  202  223\n",
      "  233  239  241  269  273  277  280  293  295  311  312  313  315  316\n",
      "  317  328  330  333  336  337  340  344  346  351  353  354  356  358\n",
      "  362  365  371  374  377  379  392  402  412  414  416  417  418  426\n",
      "  435  446  461  462  466  467  481  482  485  496  499  501  508  516\n",
      "  532  541  544  549  552  554  561  566  569  579  581  587  602  615\n",
      "  622  624  628  629  631  632  636  644  654  662  665  670  671  677\n",
      "  683  691  696  706  712  715  716  737  740  744  747  749  754  762\n",
      "  771  775  778  781  784  786  791  792  793  802  809  813  830  841\n",
      "  849  853  856  866  869  878  881  886  893  895  899  903  905  922\n",
      "  928  929  932  936  938  940  946  960  966  969  972  973  975  980\n",
      "  985  986  991  995  999 1003 1005 1010 1013 1017 1018 1019 1027 1037\n",
      " 1038 1041 1059 1062 1066 1067 1084 1091 1093 1094 1101 1106 1123 1128\n",
      " 1134 1139 1160 1167 1178 1180 1183 1187 1190 1209 1214 1217 1222 1223\n",
      " 1226 1232 1233 1235 1242 1244 1246 1251 1256 1258 1262 1268 1270 1276\n",
      " 1277 1278 1280 1300 1310 1329 1330 1331 1337 1340 1341 1355 1357 1363\n",
      " 1367 1369 1372 1374 1379 1380 1386 1389 1393 1395 1399 1403 1405 1409\n",
      " 1411 1420 1422 1428 1434 1453 1463 1465 1469 1473 1476 1477 1496 1510\n",
      " 1514 1516 1517 1520 1525 1526 1527 1531 1542 1549 1556 1559 1560 1564\n",
      " 1565 1567 1575 1580 1595 1605 1617 1627 1628 1632 1634 1639 1643 1650\n",
      " 1656 1660 1665 1669 1681 1683 1687 1688 1699 1708 1709 1711 1717 1718\n",
      " 1720 1728 1729 1735 1736 1738 1739 1741 1748 1761 1762 1766 1767 1785\n",
      " 1789 1800 1809 1818 1820 1824 1830 1835 1839 1843 1844 1847 1852 1853\n",
      " 1856 1858 1862 1863 1872 1874 1877 1878 1881 1882 1887 1892 1898 1904\n",
      " 1909 1910 1916 1923 1932 1935 1938 1941 1943 1948 1949 1953 1954 1967\n",
      " 1968 1981 1984 1985 1989 1995 1996 1998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 23:51:02.357494  2976 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.5771 - acc: 0.1604\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5294 - acc: 0.1684\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5151 - acc: 0.1759\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5108 - acc: 0.1779\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5062 - acc: 0.1802\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5015 - acc: 0.1818\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4993 - acc: 0.1824\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4930 - acc: 0.1855\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4909 - acc: 0.1864\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4869 - acc: 0.1883\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4817 - acc: 0.1903\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4798 - acc: 0.1914\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4763 - acc: 0.1929\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4722 - acc: 0.1944\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4709 - acc: 0.1950\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4708 - acc: 0.1952\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4622 - acc: 0.1988\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4604 - acc: 0.1992\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4528 - acc: 0.2025\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4453 - acc: 0.2052\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4402 - acc: 0.2072\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4364 - acc: 0.2086\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4322 - acc: 0.2100\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4290 - acc: 0.2111\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4240 - acc: 0.2129\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4201 - acc: 0.2145\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4160 - acc: 0.2156\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4132 - acc: 0.2167\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4096 - acc: 0.2178\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4044 - acc: 0.2196\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4012 - acc: 0.2209\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3983 - acc: 0.2215\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3950 - acc: 0.2226\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3906 - acc: 0.2241\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3879 - acc: 0.2246\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3860 - acc: 0.2257\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3815 - acc: 0.2271\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3788 - acc: 0.2278\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3754 - acc: 0.2289\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3726 - acc: 0.2299\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3701 - acc: 0.2305\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3683 - acc: 0.2313\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3643 - acc: 0.2325\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3609 - acc: 0.2336\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3590 - acc: 0.2341\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3553 - acc: 0.2352\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3536 - acc: 0.2358\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3515 - acc: 0.2367\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3485 - acc: 0.2375\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3463 - acc: 0.2382\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3443 - acc: 0.2386\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3421 - acc: 0.2394\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3403 - acc: 0.2402\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3381 - acc: 0.2409\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3372 - acc: 0.2410\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3352 - acc: 0.2416\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3310 - acc: 0.2433\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3307 - acc: 0.2436\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3288 - acc: 0.2440\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3272 - acc: 0.2445\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3246 - acc: 0.2452\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3241 - acc: 0.2455\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3214 - acc: 0.2464\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3198 - acc: 0.2468\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3186 - acc: 0.2470\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3176 - acc: 0.2476\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3170 - acc: 0.2476\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3145 - acc: 0.2485\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3132 - acc: 0.2494\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3112 - acc: 0.2497\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3121 - acc: 0.2496\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3074 - acc: 0.2509\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3069 - acc: 0.2513\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3051 - acc: 0.2519\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3034 - acc: 0.2522\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3041 - acc: 0.2521\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3019 - acc: 0.2528\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.3012 - acc: 0.2531\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2997 - acc: 0.2534\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2972 - acc: 0.2543\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2968 - acc: 0.2545\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2952 - acc: 0.2551\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2946 - acc: 0.2552\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2931 - acc: 0.2557\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2925 - acc: 0.2561\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2916 - acc: 0.2562\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2902 - acc: 0.2568\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2901 - acc: 0.2567\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2889 - acc: 0.2570\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2875 - acc: 0.2575\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2869 - acc: 0.2576\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2846 - acc: 0.2586\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2843 - acc: 0.2587\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2839 - acc: 0.2590\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2825 - acc: 0.2592\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2821 - acc: 0.2597\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2813 - acc: 0.2596\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2800 - acc: 0.2602\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2796 - acc: 0.2603\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2789 - acc: 0.2607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.01      1269\n",
      "           1       0.62      0.71      0.66     21997\n",
      "           2       0.35      0.26      0.30      4224\n",
      "           3       0.75      0.80      0.77     35819\n",
      "           4       0.46      0.23      0.30       761\n",
      "           5       0.48      0.53      0.50     21643\n",
      "           6       0.37      0.13      0.19      9396\n",
      "           7       0.41      0.43      0.42     12113\n",
      "\n",
      "    accuracy                           0.59    107222\n",
      "   macro avg       0.55      0.39      0.39    107222\n",
      "weighted avg       0.58      0.59      0.57    107222\n",
      "\n",
      "Acur√°cia\n",
      "0.38577975627475\n",
      "Precisao\n",
      "0.5798417927204089\n",
      "Recall\n",
      "0.5918001902594616\n",
      "F1\n",
      "0.5730373819827018\n",
      "[[    6   315    46   171     0   534    62   135]\n",
      " [    0 15527   225  2169     8  2988   237   843]\n",
      " [    0   479  1082   980     4   869   110   700]\n",
      " [    0  2038   532 28727   153  2336   235  1798]\n",
      " [    0    62    12   383   174    78    11    41]\n",
      " [    0  4005   475  2439     8 11579   928  2209]\n",
      " [    0  1424   261  1228    12  3607  1209  1655]\n",
      " [    0  1312   473  2389    22  2289   478  5150]]\n",
      "TRAIN: [   0    1    2 ... 1995 1996 1998] TEST: [   8   12   16   20   33   35   40   56   67   68   79   81   82   89\n",
      "   91   92   94  106  110  115  118  125  132  146  148  149  150  155\n",
      "  156  169  171  172  174  176  177  187  193  197  198  206  208  226\n",
      "  236  243  245  247  251  253  256  268  270  281  284  285  286  289\n",
      "  290  292  296  301  302  306  307  324  331  335  338  349  350  360\n",
      "  361  369  372  384  391  394  395  401  405  407  410  411  419  422\n",
      "  423  433  434  436  437  440  459  464  469  472  474  475  480  488\n",
      "  489  490  492  498  500  504  505  507  519  522  528  539  543  557\n",
      "  563  567  572  573  576  578  583  590  591  592  597  601  603  605\n",
      "  606  609  612  623  627  630  634  639  653  656  661  669  673  678\n",
      "  681  689  690  697  701  703  710  717  720  721  724  732  738  743\n",
      "  745  758  760  761  765  777  779  795  796  800  801  804  811  822\n",
      "  827  828  829  831  835  839  843  852  857  860  867  868  871  891\n",
      "  892  896  897  898  900  904  918  919  937  947  950  951  955  956\n",
      "  959  961  964  971  976  978  979  981  983  988  989  992  996 1001\n",
      " 1012 1015 1021 1022 1035 1039 1042 1054 1063 1075 1096 1109 1114 1120\n",
      " 1131 1140 1141 1150 1154 1156 1158 1166 1168 1169 1188 1200 1211 1212\n",
      " 1215 1219 1227 1228 1229 1238 1248 1249 1252 1257 1260 1263 1264 1269\n",
      " 1279 1281 1282 1283 1291 1292 1294 1301 1304 1308 1327 1328 1333 1334\n",
      " 1346 1349 1352 1361 1362 1364 1365 1370 1375 1410 1415 1418 1425 1429\n",
      " 1444 1446 1450 1457 1458 1468 1470 1478 1483 1486 1488 1492 1493 1494\n",
      " 1495 1498 1502 1504 1505 1508 1511 1528 1529 1534 1535 1540 1541 1543\n",
      " 1548 1558 1562 1569 1570 1577 1578 1583 1587 1594 1599 1603 1608 1609\n",
      " 1614 1615 1618 1636 1637 1644 1646 1649 1654 1655 1664 1667 1670 1679\n",
      " 1682 1693 1706 1707 1710 1719 1721 1725 1737 1745 1747 1754 1756 1758\n",
      " 1763 1765 1773 1781 1788 1790 1791 1793 1795 1797 1807 1808 1810 1813\n",
      " 1819 1821 1822 1823 1831 1842 1851 1857 1864 1869 1871 1876 1883 1885\n",
      " 1893 1894 1895 1896 1900 1903 1906 1907 1911 1913 1917 1921 1936 1944\n",
      " 1950 1957 1959 1961 1966 1972 1997 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5865 - acc: 0.1620\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5329 - acc: 0.1748\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5232 - acc: 0.1795\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5155 - acc: 0.1827\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5117 - acc: 0.1840\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5094 - acc: 0.1846\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5026 - acc: 0.1879\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4991 - acc: 0.1898\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4944 - acc: 0.1920\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4980 - acc: 0.1905\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.4898 - acc: 0.1935\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4887 - acc: 0.1942\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4836 - acc: 0.1966\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4824 - acc: 0.1968\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4799 - acc: 0.1978\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4749 - acc: 0.2000\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4695 - acc: 0.2024\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4623 - acc: 0.2048\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4554 - acc: 0.2075\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4488 - acc: 0.2099\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4455 - acc: 0.2116\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4415 - acc: 0.2129\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4371 - acc: 0.2142\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4337 - acc: 0.2154\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4309 - acc: 0.2165\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4278 - acc: 0.2175\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4217 - acc: 0.2195\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4181 - acc: 0.2210\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4145 - acc: 0.2221\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4101 - acc: 0.2235\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4087 - acc: 0.2240\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4031 - acc: 0.2259\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4031 - acc: 0.2257\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3977 - acc: 0.2277\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3930 - acc: 0.2290\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3880 - acc: 0.2307\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3851 - acc: 0.2319\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3834 - acc: 0.2326\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3803 - acc: 0.2332\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3767 - acc: 0.2347\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3736 - acc: 0.2354\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3715 - acc: 0.2359\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3677 - acc: 0.2372\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3640 - acc: 0.2385\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3614 - acc: 0.2393\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3605 - acc: 0.2397\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3567 - acc: 0.2407\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3541 - acc: 0.2417\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3523 - acc: 0.2421\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3496 - acc: 0.2430\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3471 - acc: 0.2438\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3444 - acc: 0.2447\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3433 - acc: 0.2451\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3420 - acc: 0.2455\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3407 - acc: 0.2457\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3377 - acc: 0.2470\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3360 - acc: 0.2474\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3328 - acc: 0.2485\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3312 - acc: 0.2488\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3303 - acc: 0.2492\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3274 - acc: 0.2503\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3251 - acc: 0.2507\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3250 - acc: 0.2511\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3227 - acc: 0.2517\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3215 - acc: 0.2520\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3195 - acc: 0.2527\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3183 - acc: 0.2535\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3154 - acc: 0.2543\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3160 - acc: 0.2538\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3131 - acc: 0.2549\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3122 - acc: 0.2552\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3115 - acc: 0.2555\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3106 - acc: 0.2557\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3084 - acc: 0.2565\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3065 - acc: 0.2569\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3048 - acc: 0.2577\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3049 - acc: 0.2578\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3045 - acc: 0.2578\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3022 - acc: 0.2587\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3016 - acc: 0.2587\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2988 - acc: 0.2597\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2980 - acc: 0.2601\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2982 - acc: 0.2599\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2959 - acc: 0.2607\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2947 - acc: 0.2610\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2953 - acc: 0.2610\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2940 - acc: 0.2611\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2932 - acc: 0.2617\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2907 - acc: 0.2622\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2907 - acc: 0.2622\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2897 - acc: 0.2628\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2879 - acc: 0.2635\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2862 - acc: 0.2640\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2865 - acc: 0.2638\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2856 - acc: 0.2642\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2839 - acc: 0.2649\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2834 - acc: 0.2650\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2831 - acc: 0.2650\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2816 - acc: 0.2655\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2816 - acc: 0.2658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.00      0.01      1187\n",
      "           1       0.66      0.70      0.68     22533\n",
      "           2       0.37      0.31      0.34      3802\n",
      "           3       0.75      0.82      0.78     31188\n",
      "           4       0.50      0.28      0.36       615\n",
      "           5       0.48      0.56      0.51     20577\n",
      "           6       0.37      0.13      0.19      9206\n",
      "           7       0.42      0.42      0.42     11376\n",
      "\n",
      "    accuracy                           0.60    100484\n",
      "   macro avg       0.49      0.40      0.41    100484\n",
      "weighted avg       0.58      0.60      0.58    100484\n",
      "\n",
      "Acur√°cia\n",
      "0.40290224713797396\n",
      "Precisao\n",
      "0.580257621312462\n",
      "Recall\n",
      "0.5986027626288762\n",
      "F1\n",
      "0.5802167284207054\n",
      "[[    5   266    32   132     0   564    49   139]\n",
      " [    4 15787   222  1908    29  3466   281   836]\n",
      " [    0   447  1172   823     2   736    93   529]\n",
      " [    0  1566   475 25547    93  1941   214  1352]\n",
      " [    0    58    27   233   174    53    14    56]\n",
      " [    1  3479   473  2208    13 11502   862  2039]\n",
      " [    0  1310   276  1139    13  3637  1203  1628]\n",
      " [    2  1064   487  2183    24  2306   550  4760]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   1    2    3 ... 1997 1998 1999] TEST: [   0   13   23   28   36   55   57   60   63   72   78   84   86   87\n",
      "   95  103  104  109  116  135  138  139  140  141  143  145  152  154\n",
      "  157  159  167  175  179  182  191  192  200  201  203  207  213  218\n",
      "  224  227  228  230  231  238  240  246  255  258  260  261  263  274\n",
      "  282  283  291  297  298  299  304  320  321  323  329  342  348  355\n",
      "  359  370  376  378  381  388  397  425  427  441  443  445  448  452\n",
      "  458  463  470  477  479  483  491  502  503  512  517  520  523  527\n",
      "  531  533  534  535  537  540  542  570  588  593  616  618  640  641\n",
      "  642  643  646  649  650  652  655  659  660  663  667  679  680  685\n",
      "  687  688  692  693  694  700  705  718  726  727  728  730  731  741\n",
      "  742  748  755  756  759  770  773  774  776  783  787  789  794  803\n",
      "  805  807  816  819  825  826  834  836  846  847  850  858  872  875\n",
      "  877  882  883  884  888  890  901  902  910  913  915  920  926  931\n",
      "  941  943  948  953  958  962  965  968  977  984 1000 1004 1006 1008\n",
      " 1009 1016 1028 1030 1032 1036 1043 1046 1050 1053 1055 1057 1060 1068\n",
      " 1070 1074 1076 1083 1087 1102 1104 1111 1113 1117 1132 1135 1138 1142\n",
      " 1143 1146 1147 1148 1149 1155 1157 1170 1171 1174 1182 1186 1189 1192\n",
      " 1197 1201 1203 1204 1207 1210 1213 1216 1221 1225 1230 1237 1245 1247\n",
      " 1253 1255 1259 1288 1290 1298 1309 1313 1314 1315 1317 1320 1323 1325\n",
      " 1332 1336 1348 1351 1354 1371 1383 1387 1388 1392 1394 1396 1397 1413\n",
      " 1417 1423 1435 1437 1442 1447 1452 1454 1459 1461 1464 1472 1474 1479\n",
      " 1481 1485 1490 1491 1501 1506 1507 1509 1512 1513 1522 1524 1530 1539\n",
      " 1544 1546 1551 1552 1553 1561 1563 1572 1579 1582 1585 1586 1591 1597\n",
      " 1600 1602 1616 1625 1630 1635 1638 1642 1647 1652 1653 1658 1666 1676\n",
      " 1691 1692 1697 1698 1701 1705 1713 1731 1740 1742 1743 1744 1749 1751\n",
      " 1759 1770 1772 1775 1776 1779 1784 1798 1801 1803 1805 1814 1815 1816\n",
      " 1827 1829 1838 1841 1849 1854 1855 1868 1875 1888 1890 1891 1897 1902\n",
      " 1908 1912 1914 1915 1918 1919 1920 1924 1925 1928 1930 1933 1937 1946\n",
      " 1951 1952 1955 1965 1970 1974 1978 1988]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.5799 - acc: 0.1610\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5297 - acc: 0.1704\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5164 - acc: 0.1773\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5107 - acc: 0.1794\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5078 - acc: 0.1807\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5027 - acc: 0.1833\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4990 - acc: 0.1846\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4963 - acc: 0.1855\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4914 - acc: 0.1876\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4891 - acc: 0.1891\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4838 - acc: 0.1912\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4806 - acc: 0.1927\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4763 - acc: 0.1947\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4734 - acc: 0.1957\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4691 - acc: 0.1974\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4674 - acc: 0.1981\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4616 - acc: 0.2005\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4542 - acc: 0.2036\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4477 - acc: 0.2057\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4431 - acc: 0.2078\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4373 - acc: 0.2097\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4342 - acc: 0.2109\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4283 - acc: 0.2133\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4266 - acc: 0.2135\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4256 - acc: 0.2137\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4182 - acc: 0.2164\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4138 - acc: 0.2179\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4096 - acc: 0.2190\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4080 - acc: 0.2198\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4021 - acc: 0.2219\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3995 - acc: 0.2228\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3962 - acc: 0.2237\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3931 - acc: 0.2249\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3883 - acc: 0.2268\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3850 - acc: 0.2276\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3822 - acc: 0.2282\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3805 - acc: 0.2291\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3766 - acc: 0.2302\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3720 - acc: 0.2316\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3707 - acc: 0.2319\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3670 - acc: 0.2330\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3645 - acc: 0.2341\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3620 - acc: 0.2350\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3595 - acc: 0.2358\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.3559 - acc: 0.2370\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3530 - acc: 0.2377\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.3512 - acc: 0.2384\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3496 - acc: 0.2387\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3463 - acc: 0.2399\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3455 - acc: 0.2401\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3424 - acc: 0.2414\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3397 - acc: 0.2422\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3377 - acc: 0.2429\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3357 - acc: 0.2436\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3334 - acc: 0.2443\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3328 - acc: 0.2443\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3314 - acc: 0.2448\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3280 - acc: 0.2458\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3263 - acc: 0.2467\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3250 - acc: 0.2467\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3219 - acc: 0.2480\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3207 - acc: 0.2482\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3195 - acc: 0.2487\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3175 - acc: 0.2492\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3165 - acc: 0.2496\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3155 - acc: 0.2502\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3134 - acc: 0.2506\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3120 - acc: 0.2513\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3101 - acc: 0.2516\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3085 - acc: 0.2525\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3087 - acc: 0.2523\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3063 - acc: 0.2532\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3057 - acc: 0.2531\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3035 - acc: 0.2538\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3019 - acc: 0.2544\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3004 - acc: 0.2552\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3004 - acc: 0.2550\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2982 - acc: 0.2554\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2979 - acc: 0.2556\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2973 - acc: 0.2563\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2949 - acc: 0.2568\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2937 - acc: 0.2576\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2937 - acc: 0.2573\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2924 - acc: 0.2578\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2905 - acc: 0.2584\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2897 - acc: 0.2584\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2887 - acc: 0.2590\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2872 - acc: 0.2594\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2869 - acc: 0.2597\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2857 - acc: 0.2599\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2844 - acc: 0.2607\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2833 - acc: 0.2606\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2828 - acc: 0.2609\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2818 - acc: 0.2614\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2807 - acc: 0.2617\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2801 - acc: 0.2618\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2794 - acc: 0.2622\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2793 - acc: 0.2622\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2778 - acc: 0.2630\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2768 - acc: 0.2631\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.01      0.02      1298\n",
      "           1       0.63      0.69      0.66     21077\n",
      "           2       0.34      0.27      0.30      3910\n",
      "           3       0.76      0.81      0.78     36112\n",
      "           4       0.65      0.26      0.37       811\n",
      "           5       0.46      0.55      0.50     20865\n",
      "           6       0.39      0.11      0.17      9222\n",
      "           7       0.41      0.43      0.42     11884\n",
      "\n",
      "    accuracy                           0.60    105179\n",
      "   macro avg       0.53      0.39      0.40    105179\n",
      "weighted avg       0.58      0.60      0.58    105179\n",
      "\n",
      "Acur√°cia\n",
      "0.3907555082866264\n",
      "Precisao\n",
      "0.5819719537995791\n",
      "Recall\n",
      "0.5955181167343291\n",
      "F1\n",
      "0.5765737073482682\n",
      "[[   11   288    32   149     1   630    39   148]\n",
      " [    2 14512   270  2068    12  3127   184   902]\n",
      " [    0   428  1054   853     2   816    82   675]\n",
      " [    0  1845   501 29234    57  2494   190  1791]\n",
      " [    0    53     5   393   210    96     4    50]\n",
      " [    3  3601   492  2463    14 11498   701  2093]\n",
      " [    2  1263   296  1224    15  3724  1038  1660]\n",
      " [    1  1076   453  2331    12  2480   452  5079]]\n",
      "TRAIN: [   0    1    3 ... 1997 1998 1999] TEST: [   2    6    9   22   26   30   42   53   59   69   73   75   76   85\n",
      "   93   97  107  108  112  113  114  120  122  126  142  163  178  180\n",
      "  183  184  204  205  210  212  219  221  222  225  232  235  237  242\n",
      "  244  248  249  250  254  257  262  267  271  276  278  279  300  303\n",
      "  308  318  319  322  326  332  339  341  347  352  363  373  375  383\n",
      "  385  387  390  393  403  404  406  408  409  415  421  424  429  432\n",
      "  439  444  447  451  453  454  455  457  465  473  478  486  487  493\n",
      "  494  506  509  514  515  521  526  536  545  551  553  555  558  559\n",
      "  564  568  571  574  575  577  585  586  594  595  599  600  604  608\n",
      "  613  614  619  621  626  637  638  645  651  658  664  666  668  674\n",
      "  695  702  711  713  722  723  725  733  735  739  746  750  753  763\n",
      "  767  768  769  772  780  782  788  798  799  810  815  818  820  821\n",
      "  823  824  832  838  840  855  865  870  873  874  879  880  885  887\n",
      "  894  906  907  909  911  916  917  927  930  933  934  935  945  954\n",
      "  963  967  994  998 1002 1014 1023 1024 1026 1031 1045 1051 1072 1073\n",
      " 1077 1078 1079 1082 1086 1097 1099 1110 1115 1118 1122 1125 1126 1130\n",
      " 1151 1152 1161 1162 1163 1172 1173 1176 1177 1184 1185 1191 1193 1194\n",
      " 1195 1196 1206 1220 1224 1231 1234 1236 1240 1241 1243 1250 1261 1267\n",
      " 1271 1272 1274 1284 1285 1295 1296 1297 1302 1305 1306 1316 1324 1326\n",
      " 1338 1345 1347 1356 1360 1366 1368 1373 1376 1377 1381 1382 1384 1385\n",
      " 1390 1391 1398 1402 1404 1416 1421 1430 1431 1432 1439 1443 1448 1449\n",
      " 1455 1456 1460 1466 1475 1497 1500 1503 1515 1519 1521 1523 1537 1545\n",
      " 1547 1550 1554 1555 1557 1566 1568 1571 1573 1574 1581 1590 1592 1593\n",
      " 1596 1601 1604 1606 1607 1611 1612 1613 1619 1629 1631 1633 1640 1645\n",
      " 1648 1662 1668 1671 1674 1675 1680 1686 1689 1694 1696 1700 1702 1704\n",
      " 1715 1722 1726 1730 1750 1752 1753 1755 1757 1760 1768 1780 1782 1783\n",
      " 1786 1792 1794 1796 1799 1802 1811 1812 1832 1834 1836 1837 1840 1845\n",
      " 1846 1860 1865 1870 1873 1879 1899 1901 1922 1926 1927 1931 1942 1945\n",
      " 1958 1973 1979 1980 1982 1991 1992 1994]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.5834 - acc: 0.1631\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5327 - acc: 0.1727\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5200 - acc: 0.1789\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5154 - acc: 0.1808\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5111 - acc: 0.1828\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5071 - acc: 0.1844\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5041 - acc: 0.1854\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4982 - acc: 0.1881\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4949 - acc: 0.1891\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4926 - acc: 0.1906\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4918 - acc: 0.1904\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4852 - acc: 0.1939\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4847 - acc: 0.1944\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4805 - acc: 0.1956\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4781 - acc: 0.1966\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4749 - acc: 0.1982\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4708 - acc: 0.1997\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4637 - acc: 0.2023\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4554 - acc: 0.2054\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4489 - acc: 0.2083\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4457 - acc: 0.2093\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4418 - acc: 0.2104\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4358 - acc: 0.2126\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4327 - acc: 0.2140\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4267 - acc: 0.2160\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4229 - acc: 0.2171\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4201 - acc: 0.2183\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4157 - acc: 0.2194\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4128 - acc: 0.2207\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4107 - acc: 0.2217\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4048 - acc: 0.2233\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4010 - acc: 0.2247\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3966 - acc: 0.2261\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3937 - acc: 0.2271\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3906 - acc: 0.2283\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3872 - acc: 0.2292\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3849 - acc: 0.2299\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3817 - acc: 0.2310\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3778 - acc: 0.2323\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3748 - acc: 0.2333\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3715 - acc: 0.2344\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3689 - acc: 0.2353\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3666 - acc: 0.2358\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3622 - acc: 0.2372\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3599 - acc: 0.2382\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3579 - acc: 0.2390\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3548 - acc: 0.2399\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3537 - acc: 0.2400\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3499 - acc: 0.2412\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3490 - acc: 0.2417\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3454 - acc: 0.2429\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3429 - acc: 0.2438\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3414 - acc: 0.2440\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3396 - acc: 0.2447\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3368 - acc: 0.2455\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3354 - acc: 0.2462\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3337 - acc: 0.2466\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3317 - acc: 0.2474\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3302 - acc: 0.2478\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3277 - acc: 0.2482\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3268 - acc: 0.2492\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3251 - acc: 0.2495\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3221 - acc: 0.2502\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3222 - acc: 0.2505\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3195 - acc: 0.2511\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3175 - acc: 0.2520\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3165 - acc: 0.2524\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3155 - acc: 0.2526\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3133 - acc: 0.2530\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3120 - acc: 0.2539\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3121 - acc: 0.2536\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3093 - acc: 0.2545\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3090 - acc: 0.2549\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3063 - acc: 0.2557\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3051 - acc: 0.2560\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3049 - acc: 0.2562\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3029 - acc: 0.2568\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3019 - acc: 0.2571\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3001 - acc: 0.2575\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2998 - acc: 0.2577\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2978 - acc: 0.2586\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2973 - acc: 0.2586\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2965 - acc: 0.2589\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2947 - acc: 0.2596\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2937 - acc: 0.2598\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2922 - acc: 0.2602\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2916 - acc: 0.2607\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2908 - acc: 0.2609\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2890 - acc: 0.2616\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2876 - acc: 0.2618\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2884 - acc: 0.2617\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2869 - acc: 0.2623\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2861 - acc: 0.2625\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2849 - acc: 0.2627\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2842 - acc: 0.2632\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2834 - acc: 0.2634\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2830 - acc: 0.2637\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2824 - acc: 0.2638\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2810 - acc: 0.2642\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2801 - acc: 0.2647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.01      0.01      1235\n",
      "           1       0.65      0.68      0.66     21792\n",
      "           2       0.34      0.31      0.32      4022\n",
      "           3       0.75      0.80      0.78     33448\n",
      "           4       0.45      0.29      0.35       677\n",
      "           5       0.46      0.55      0.50     20442\n",
      "           6       0.35      0.16      0.22      8853\n",
      "           7       0.42      0.42      0.42     11723\n",
      "\n",
      "    accuracy                           0.59    102192\n",
      "   macro avg       0.49      0.40      0.41    102192\n",
      "weighted avg       0.58      0.59      0.58    102192\n",
      "\n",
      "Acur√°cia\n",
      "0.4014853049446528\n",
      "Precisao\n",
      "0.5797863720337366\n",
      "Recall\n",
      "0.5935787537184907\n",
      "F1\n",
      "0.5794886290652267\n",
      "[[    9   247    33   150     2   578    68   148]\n",
      " [    1 14794   340  1904    35  3397   352   969]\n",
      " [    4   417  1245   899     2   776   118   561]\n",
      " [    1  1757   586 26863   138  2299   292  1512]\n",
      " [    0    64     6   318   194    54     8    33]\n",
      " [    3  3288   595  2247    25 11238  1080  1966]\n",
      " [    0  1066   319  1055    12  3525  1373  1503]\n",
      " [    1  1084   517  2197    22  2333   626  4943]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   4    5   14   17   18   19   27   29   39   43   46   48   49   50\n",
      "   52   58   65   71   83   90  100  101  117  119  121  124  127  129\n",
      "  131  133  147  153  161  162  164  165  170  173  185  190  194  209\n",
      "  211  214  215  216  217  220  229  234  252  259  264  265  266  272\n",
      "  275  287  288  294  305  309  310  314  325  327  334  343  345  357\n",
      "  364  366  367  368  380  382  386  389  396  398  399  400  413  420\n",
      "  428  430  431  438  442  449  450  456  460  468  471  476  484  495\n",
      "  497  510  511  513  518  524  525  529  530  538  546  547  548  550\n",
      "  556  560  562  565  580  582  584  589  596  598  607  610  611  617\n",
      "  620  625  633  635  647  648  657  672  675  676  682  684  686  698\n",
      "  699  704  707  708  709  714  719  729  734  736  751  752  757  764\n",
      "  766  785  790  797  806  808  812  814  817  833  837  842  844  845\n",
      "  848  851  854  859  861  862  863  864  876  889  908  912  914  921\n",
      "  923  924  925  939  942  944  949  952  957  970  974  982  987  990\n",
      "  993  997 1007 1011 1020 1025 1029 1033 1034 1040 1044 1047 1048 1049\n",
      " 1052 1056 1058 1061 1064 1065 1069 1071 1080 1081 1085 1088 1089 1090\n",
      " 1092 1095 1098 1100 1103 1105 1107 1108 1112 1116 1119 1121 1124 1127\n",
      " 1129 1133 1136 1137 1144 1145 1153 1159 1164 1165 1175 1179 1181 1198\n",
      " 1199 1202 1205 1208 1218 1239 1254 1265 1266 1273 1275 1286 1287 1289\n",
      " 1293 1299 1303 1307 1311 1312 1318 1319 1321 1322 1335 1339 1342 1343\n",
      " 1344 1350 1353 1358 1359 1378 1400 1401 1406 1407 1408 1412 1414 1419\n",
      " 1424 1426 1427 1433 1436 1438 1440 1441 1445 1451 1462 1467 1471 1480\n",
      " 1482 1484 1487 1489 1499 1518 1532 1533 1536 1538 1576 1584 1588 1589\n",
      " 1598 1610 1620 1621 1622 1623 1624 1626 1641 1651 1657 1659 1661 1663\n",
      " 1672 1673 1677 1678 1684 1685 1690 1695 1703 1712 1714 1716 1723 1724\n",
      " 1727 1732 1733 1734 1746 1764 1769 1771 1774 1777 1778 1787 1804 1806\n",
      " 1817 1825 1826 1828 1833 1848 1850 1859 1861 1866 1867 1880 1884 1886\n",
      " 1889 1905 1929 1934 1939 1940 1947 1956 1960 1962 1963 1964 1969 1971\n",
      " 1975 1976 1977 1983 1986 1987 1990 1993]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.5812 - acc: 0.1649\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5319 - acc: 0.1747\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5214 - acc: 0.1802\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5177 - acc: 0.1812\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5104 - acc: 0.1846\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5079 - acc: 0.1853\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5038 - acc: 0.1872\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5073 - acc: 0.1856\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4973 - acc: 0.1900\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4919 - acc: 0.1920\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4878 - acc: 0.1942\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4866 - acc: 0.1948\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4838 - acc: 0.1959\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4799 - acc: 0.1973\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4790 - acc: 0.1979\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4744 - acc: 0.1993\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4702 - acc: 0.2014\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4663 - acc: 0.2029\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4636 - acc: 0.2040\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4596 - acc: 0.2053\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4523 - acc: 0.2079\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4438 - acc: 0.2112\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4393 - acc: 0.2133\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4361 - acc: 0.2138\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4319 - acc: 0.2154\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4270 - acc: 0.2173\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4243 - acc: 0.2178\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4193 - acc: 0.2195\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4168 - acc: 0.2203\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4121 - acc: 0.2220\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4104 - acc: 0.2230\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4078 - acc: 0.2234\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4027 - acc: 0.2255\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3978 - acc: 0.2272\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3943 - acc: 0.2282\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3911 - acc: 0.2290\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3888 - acc: 0.2297\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3844 - acc: 0.2313\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3811 - acc: 0.2323\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3798 - acc: 0.2330\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3762 - acc: 0.2340\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3734 - acc: 0.2352\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3708 - acc: 0.2356\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3689 - acc: 0.2364\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3649 - acc: 0.2377\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3634 - acc: 0.2382\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3630 - acc: 0.2382\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3578 - acc: 0.2398\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3544 - acc: 0.2410\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3524 - acc: 0.2416\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3512 - acc: 0.2423\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3498 - acc: 0.2425\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3468 - acc: 0.2435\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3447 - acc: 0.2441\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3432 - acc: 0.2449\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3410 - acc: 0.2455\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3394 - acc: 0.2461\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3370 - acc: 0.2470\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3349 - acc: 0.2469\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3335 - acc: 0.2477\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3333 - acc: 0.2478\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3314 - acc: 0.2485\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3293 - acc: 0.2496\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3277 - acc: 0.2498\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3254 - acc: 0.2505\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3226 - acc: 0.2515\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3216 - acc: 0.2518\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3217 - acc: 0.2518\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3192 - acc: 0.2526\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3176 - acc: 0.2532\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3169 - acc: 0.2532\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3166 - acc: 0.2533\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3134 - acc: 0.2542\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3120 - acc: 0.2551\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.3108 - acc: 0.2554\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3082 - acc: 0.2563\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3084 - acc: 0.2559\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3075 - acc: 0.2562\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3062 - acc: 0.2570\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3045 - acc: 0.2574\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3036 - acc: 0.2576\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3039 - acc: 0.2577\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.3015 - acc: 0.2586\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3003 - acc: 0.2588\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2998 - acc: 0.2589\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2989 - acc: 0.2592\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2976 - acc: 0.2599\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2976 - acc: 0.2594\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2950 - acc: 0.2605\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2942 - acc: 0.2608\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2929 - acc: 0.2614\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2919 - acc: 0.2614\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2906 - acc: 0.2618\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2901 - acc: 0.2623\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2887 - acc: 0.2630\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2881 - acc: 0.2627\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2879 - acc: 0.2628\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2864 - acc: 0.2634\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2860 - acc: 0.2637\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2850 - acc: 0.2642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.00      1275\n",
      "           1       0.66      0.68      0.67     21533\n",
      "           2       0.35      0.31      0.33      3913\n",
      "           3       0.76      0.82      0.79     32576\n",
      "           4       0.56      0.30      0.39       653\n",
      "           5       0.48      0.56      0.52     20558\n",
      "           6       0.36      0.14      0.21      8911\n",
      "           7       0.41      0.44      0.42     11312\n",
      "\n",
      "    accuracy                           0.60    100731\n",
      "   macro avg       0.51      0.41      0.42    100731\n",
      "weighted avg       0.59      0.60      0.59    100731\n",
      "\n",
      "Acur√°cia\n",
      "0.40642401271868767\n",
      "Precisao\n",
      "0.5865986070068487\n",
      "Recall\n",
      "0.6011356980472744\n",
      "F1\n",
      "0.5853840553218973\n",
      "[[    2   286    37   122     0   626    73   129]\n",
      " [    0 14704   315  1871     7  3339   313   984]\n",
      " [    0   382  1196   789     4   813   106   623]\n",
      " [    0  1459   523 26772    95  1934   197  1596]\n",
      " [    0    38     9   301   196    49     9    51]\n",
      " [    2  3252   498  2219    15 11431   986  2155]\n",
      " [    0  1140   299  1069    14  3462  1285  1642]\n",
      " [    0   977   559  2027    17  2136   629  4967]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_20 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 799,608\n",
      "Trainable params: 799,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.38577975627475, 0.40290224713797396, 0.3907555082866264, 0.4014853049446528, 0.40642401271868767]\n",
      "0.3974693658725382\n",
      "Precision total\n",
      "[0.5798417927204089, 0.580257621312462, 0.5819719537995791, 0.5797863720337366, 0.5865986070068487]\n",
      "0.581691269374607\n",
      "Recalls total\n",
      "[0.5918001902594616, 0.5986027626288762, 0.5955181167343291, 0.5935787537184907, 0.6011356980472744]\n",
      "0.5961271042776863\n",
      "F1 total\n",
      "[0.5730373819827018, 0.5802167284207054, 0.5765737073482682, 0.5794886290652267, 0.5853840553218973]\n",
      "0.5789401004277599\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accuq8)\n",
    "a = np.array(accuq8)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "p = np.array(precisionsq8)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "r = np.array(recallsq8)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1q8)\n",
    "f = np.array(f1q8)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
