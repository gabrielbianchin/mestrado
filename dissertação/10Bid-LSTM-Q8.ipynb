{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 22:42:08.317436  9768 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 22:42:08.322394  9768 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 22:42:08.323392  9768 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 22:42:08.324389  9768 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   5    8    9   13   16   18   21   22   30   36   38   39   54   58\n",
      "   59   61   62   63   68   69   70   77   82   92   96   98  106  112\n",
      "  129  136  143  145  150  158  162  170  176  177  178  185  187  188\n",
      "  191  192  198  200  212  213  223  228  231  237  242  247  249  250\n",
      "  251  253  259  265  267  268  273  276  286  292  298  300  304  312\n",
      "  329  351  354  355  364  368  369  376  379  380  389  394  404  406\n",
      "  407  410  413  423  429  431  432  433  435  437  441  446  454  457\n",
      "  468  469  472  498  501  506  508  510  526  531  538  540  544  550\n",
      "  553  555  564  566  572  581  584  599  600  602  603  615  617  618\n",
      "  623  624  626  631  635  637  655  657  662  664  675  680  681  685\n",
      "  694  699  702  704  708  720  722  728  737  746  747  749  754  762\n",
      "  768  772  775  778  780  787  788  792  793  804  805  808  811  813\n",
      "  816  827  829  830  839  842  848  854  858  860  866  867  875  878\n",
      "  879  891  892  897  902  925  937  938  941  951  957  972  974  976\n",
      "  981  997 1000 1005 1012 1019 1021 1027 1032 1034 1038 1041 1043 1044\n",
      " 1049 1054 1062 1063 1065 1069 1075 1084 1086 1087 1097 1104 1107 1108\n",
      " 1109 1111 1115 1121 1122 1125 1129 1144 1146 1154 1163 1173 1175 1190\n",
      " 1197 1198 1202 1225 1233 1236 1241 1244 1247 1253 1258 1260 1265 1268\n",
      " 1272 1273 1275 1286 1290 1291 1301 1306 1309 1314 1317 1331 1333 1339\n",
      " 1347 1351 1373 1374 1375 1383 1389 1391 1393 1397 1400 1401 1409 1412\n",
      " 1413 1424 1426 1428 1431 1432 1433 1437 1443 1446 1448 1451 1457 1467\n",
      " 1471 1475 1483 1484 1487 1489 1506 1510 1512 1518 1522 1523 1525 1529\n",
      " 1536 1554 1576 1580 1583 1584 1589 1592 1595 1601 1607 1608 1609 1614\n",
      " 1616 1621 1622 1626 1627 1631 1641 1645 1647 1649 1654 1659 1669 1670\n",
      " 1673 1677 1679 1683 1688 1696 1702 1707 1717 1718 1742 1748 1751 1754\n",
      " 1755 1757 1758 1759 1764 1769 1773 1785 1789 1790 1795 1798 1812 1817\n",
      " 1828 1832 1834 1835 1842 1844 1859 1865 1869 1874 1876 1882 1884 1885\n",
      " 1886 1887 1893 1900 1905 1909 1915 1919 1923 1931 1934 1939 1940 1952\n",
      " 1955 1960 1973 1978 1979 1983 1987 1993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 22:42:11.176790  9768 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 31s 19ms/sample - loss: 0.6419 - acc: 0.1278\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6304 - acc: 0.1224\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6274 - acc: 0.1250\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6267 - acc: 0.1250\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6255 - acc: 0.1253\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6250 - acc: 0.1254\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6253 - acc: 0.1253\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1322\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5961 - acc: 0.1430\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5728 - acc: 0.1537\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5619 - acc: 0.1602\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5448 - acc: 0.1675\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5362 - acc: 0.1722\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5267 - acc: 0.1777\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5202 - acc: 0.1807\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5165 - acc: 0.1825\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5110 - acc: 0.1846\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5079 - acc: 0.1863\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5069 - acc: 0.1869\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5032 - acc: 0.1883\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4999 - acc: 0.1892\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4961 - acc: 0.1909\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4950 - acc: 0.1919\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4931 - acc: 0.1925\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4888 - acc: 0.1942\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4840 - acc: 0.1958\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4815 - acc: 0.1969\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4822 - acc: 0.1965\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4754 - acc: 0.1994\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4706 - acc: 0.2015\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4701 - acc: 0.2014\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4668 - acc: 0.2028\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4615 - acc: 0.2048\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4578 - acc: 0.2060\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4500 - acc: 0.2090\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4452 - acc: 0.2103\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4412 - acc: 0.2121\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4353 - acc: 0.2139\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4309 - acc: 0.2155\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4247 - acc: 0.2177\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4223 - acc: 0.2184\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4222 - acc: 0.2187\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4152 - acc: 0.2206\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4051 - acc: 0.2238\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3988 - acc: 0.2259\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3958 - acc: 0.2268\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3914 - acc: 0.2283\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3859 - acc: 0.2299\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3883 - acc: 0.2291\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3782 - acc: 0.2323\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3721 - acc: 0.2344\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3710 - acc: 0.2347\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3656 - acc: 0.2361\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3602 - acc: 0.2377\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3555 - acc: 0.2393\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3539 - acc: 0.2399\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3493 - acc: 0.2410\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3483 - acc: 0.2415\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3418 - acc: 0.2436\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3392 - acc: 0.2442\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3339 - acc: 0.2458\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3314 - acc: 0.2467\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3302 - acc: 0.2473\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3252 - acc: 0.2484\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3214 - acc: 0.2497\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3196 - acc: 0.2504\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3177 - acc: 0.2512\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3138 - acc: 0.2522\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3109 - acc: 0.2532\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3083 - acc: 0.2539\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3067 - acc: 0.2547\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3029 - acc: 0.2557\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3009 - acc: 0.2564\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2975 - acc: 0.2575\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2970 - acc: 0.2578\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2944 - acc: 0.2586\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2923 - acc: 0.2594\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2879 - acc: 0.2606\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2859 - acc: 0.2615\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2865 - acc: 0.2612\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2833 - acc: 0.2622\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2822 - acc: 0.2627\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2796 - acc: 0.2635\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2754 - acc: 0.2648\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2749 - acc: 0.2653\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2734 - acc: 0.2655\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2722 - acc: 0.2660\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2709 - acc: 0.2667\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2679 - acc: 0.2675\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2694 - acc: 0.2674\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2659 - acc: 0.2686\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2638 - acc: 0.2692\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2612 - acc: 0.2699\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2592 - acc: 0.2710\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2578 - acc: 0.2711\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2574 - acc: 0.2713\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2545 - acc: 0.2723\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2547 - acc: 0.2724\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2525 - acc: 0.2733\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2494 - acc: 0.2742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.04      0.07      1286\n",
      "           1       0.67      0.68      0.68     22166\n",
      "           2       0.32      0.31      0.32      3766\n",
      "           3       0.73      0.81      0.77     32092\n",
      "           4       0.45      0.29      0.35       685\n",
      "           5       0.45      0.55      0.50     20191\n",
      "           6       0.39      0.13      0.20      9041\n",
      "           7       0.41      0.39      0.40     11406\n",
      "\n",
      "    accuracy                           0.59    100633\n",
      "   macro avg       0.52      0.40      0.41    100633\n",
      "weighted avg       0.58      0.59      0.57    100633\n",
      "\n",
      "Acur√°cia\n",
      "0.40017190541695397\n",
      "Precisao\n",
      "0.5784424192514481\n",
      "Recall\n",
      "0.5897071537169716\n",
      "F1\n",
      "0.5728260266120115\n",
      "[[   48   295    39   166     1   583    41   113]\n",
      " [    8 15115   329  2267    26  3367   263   791]\n",
      " [    0   321  1160   846     2   854    73   510]\n",
      " [    0  1498   618 26014   138  2274   113  1437]\n",
      " [    0    68     6   323   196    42     3    47]\n",
      " [    7  3147   594  2415    30 11117   870  2011]\n",
      " [    2  1234   285  1165    17  3652  1203  1483]\n",
      " [    1   940   567  2206    26  2631   544  4491]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   4    7   14   15   19   28   32   35   40   41   45   50   51   53\n",
      "   55   60   76   81   83   87   88   97  104  107  117  122  126  127\n",
      "  130  137  140  142  144  148  160  161  164  169  171  173  175  196\n",
      "  201  202  203  207  209  210  211  222  224  226  227  240  254  255\n",
      "  261  264  274  284  295  296  303  310  314  316  322  324  325  333\n",
      "  334  337  340  341  343  346  348  350  365  366  367  371  372  373\n",
      "  374  375  381  390  391  399  402  405  408  409  415  416  418  420\n",
      "  422  425  427  430  439  447  451  452  455  462  463  464  471  479\n",
      "  480  484  485  486  489  491  503  517  522  524  542  545  549  557\n",
      "  563  567  568  569  570  571  573  590  592  593  594  595  596  598\n",
      "  604  614  627  628  632  636  644  645  648  650  651  652  663  669\n",
      "  679  686  696  700  701  717  721  725  729  738  751  756  777  794\n",
      "  802  806  814  817  826  835  836  851  853  862  870  880  893  895\n",
      "  896  903  905  906  907  912  913  916  918  921  922  927  929  942\n",
      "  944  946  947  950  958  961  965  968  975  978  979  987  991  996\n",
      " 1007 1016 1024 1030 1036 1037 1046 1051 1053 1055 1057 1060 1066 1067\n",
      " 1070 1077 1094 1101 1103 1110 1113 1117 1119 1128 1130 1132 1136 1137\n",
      " 1140 1145 1150 1159 1183 1185 1191 1193 1199 1205 1206 1215 1220 1235\n",
      " 1237 1240 1243 1245 1250 1254 1267 1274 1280 1281 1284 1285 1297 1311\n",
      " 1319 1320 1325 1335 1336 1340 1341 1343 1344 1352 1353 1354 1358 1362\n",
      " 1363 1366 1367 1368 1369 1378 1381 1384 1394 1402 1403 1414 1420 1422\n",
      " 1427 1440 1442 1445 1447 1458 1460 1461 1463 1473 1476 1482 1495 1496\n",
      " 1503 1504 1511 1516 1524 1526 1530 1533 1535 1538 1541 1542 1547 1556\n",
      " 1566 1567 1571 1572 1575 1587 1590 1598 1599 1600 1613 1624 1634 1650\n",
      " 1655 1660 1661 1668 1676 1682 1687 1708 1711 1715 1720 1727 1728 1731\n",
      " 1734 1737 1750 1766 1768 1774 1776 1779 1791 1793 1797 1802 1803 1811\n",
      " 1813 1816 1821 1825 1831 1836 1837 1840 1846 1847 1848 1849 1866 1868\n",
      " 1883 1891 1894 1901 1913 1914 1920 1928 1937 1943 1946 1948 1950 1953\n",
      " 1954 1956 1961 1966 1972 1982 1989 1990]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.6316 - acc: 0.1262\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6197 - acc: 0.1219\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6185 - acc: 0.1226\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6176 - acc: 0.1233\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6165 - acc: 0.1236\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6161 - acc: 0.1236\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6160 - acc: 0.1236\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6157 - acc: 0.1236\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6154 - acc: 0.1236\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6152 - acc: 0.1237\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6152 - acc: 0.1237\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6151 - acc: 0.1236\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6148 - acc: 0.1236\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6148 - acc: 0.1236\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6150 - acc: 0.1237\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6149 - acc: 0.1236\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6149 - acc: 0.1236\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6148 - acc: 0.1236\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6146 - acc: 0.1237\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6146 - acc: 0.1236\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6146 - acc: 0.1237\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6145 - acc: 0.1237\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6147 - acc: 0.1236\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6146 - acc: 0.1237\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6147 - acc: 0.1237\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6147 - acc: 0.1237\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6145 - acc: 0.1237\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6144 - acc: 0.1237\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6147 - acc: 0.1236\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6149 - acc: 0.1236\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6146 - acc: 0.1237\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6146 - acc: 0.1245\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6143 - acc: 0.1237\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6142 - acc: 0.1237\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6142 - acc: 0.1237\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6142 - acc: 0.1238\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6143 - acc: 0.1238\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6143 - acc: 0.1237\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6142 - acc: 0.1237\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6141 - acc: 0.1237\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6144 - acc: 0.1236\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6142 - acc: 0.1237\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6140 - acc: 0.1237\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6140 - acc: 0.1237\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6140 - acc: 0.1237\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6140 - acc: 0.1237\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6141 - acc: 0.1237\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6140 - acc: 0.1236\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6141 - acc: 0.1237\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1237\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1238\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6140 - acc: 0.1237\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1237\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1237\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1238\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1237\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1238\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1237\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1238\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1238\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1237\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1237\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1238\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1238\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1238\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1238\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1238\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1238\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1238\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1237\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1238\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1238\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6135 - acc: 0.1237\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6138 - acc: 0.1238\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1238\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1238\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1238\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1238\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6139 - acc: 0.1238\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1238\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1237\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6136 - acc: 0.1237\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6137 - acc: 0.1237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1249\n",
      "           1       0.39      0.02      0.04     22718\n",
      "           2       0.00      0.00      0.00      4232\n",
      "           3       0.32      0.99      0.49     33935\n",
      "           4       0.00      0.00      0.00       832\n",
      "           5       0.58      0.04      0.08     21585\n",
      "           6       0.00      0.00      0.00      9596\n",
      "           7       0.00      0.00      0.00     12069\n",
      "\n",
      "    accuracy                           0.33    106216\n",
      "   macro avg       0.16      0.13      0.08    106216\n",
      "weighted avg       0.31      0.33      0.18    106216\n",
      "\n",
      "Acur√°cia\n",
      "0.13150230690085424\n",
      "Precisao\n",
      "0.3057907368040687\n",
      "Recall\n",
      "0.32891466445733225\n",
      "F1\n",
      "0.18071996959574058\n",
      "[[    0    17     0  1211     0    21     0     0]\n",
      " [    0   469     0 21934     0   315     0     0]\n",
      " [    0    40     0  4152     0    40     0     0]\n",
      " [    0   254     0 33536     0   145     0     0]\n",
      " [    0     0     0   832     0     0     0     0]\n",
      " [    0   258     0 20396     0   931     0     0]\n",
      " [    0    80     0  9443     0    73     0     0]\n",
      " [    0    77     0 11917     0    75     0     0]]\n",
      "TRAIN: [   1    2    4 ... 1997 1998 1999] TEST: [   0    3    6   11   26   34   42   44   74   79   94  100  103  105\n",
      "  111  113  123  132  134  135  139  157  159  168  181  186  189  194\n",
      "  197  205  208  215  221  229  235  236  238  241  245  256  269  277\n",
      "  278  279  288  291  297  299  301  306  308  309  315  328  339  353\n",
      "  358  359  363  370  377  386  387  392  395  400  417  421  426  428\n",
      "  434  436  438  440  443  444  453  456  458  459  466  467  473  477\n",
      "  482  488  492  493  499  502  509  511  512  513  518  525  527  528\n",
      "  532  533  541  559  575  579  580  582  583  587  605  607  608  610\n",
      "  611  612  613  629  630  634  639  643  659  678  690  691  692  693\n",
      "  698  706  707  711  724  727  733  734  735  736  740  750  760  766\n",
      "  770  773  779  782  791  797  798  818  819  822  825  828  832  840\n",
      "  841  844  847  861  864  869  873  874  876  877  881  884  885  888\n",
      "  899  901  908  909  911  914  915  917  919  923  926  934  943  949\n",
      "  954  956  966  967  973  982  983  984  994 1002 1004 1008 1018 1022\n",
      " 1025 1040 1079 1095 1100 1105 1114 1118 1120 1123 1133 1151 1152 1156\n",
      " 1157 1160 1167 1169 1171 1174 1178 1179 1180 1181 1186 1187 1194 1196\n",
      " 1203 1208 1209 1213 1216 1217 1222 1224 1226 1238 1239 1242 1246 1252\n",
      " 1256 1257 1271 1276 1279 1282 1293 1294 1299 1300 1305 1313 1318 1322\n",
      " 1323 1330 1334 1338 1349 1350 1355 1372 1377 1379 1387 1395 1399 1404\n",
      " 1406 1415 1417 1418 1419 1429 1430 1435 1436 1438 1444 1465 1466 1477\n",
      " 1488 1493 1494 1497 1502 1505 1515 1517 1521 1528 1531 1539 1548 1550\n",
      " 1552 1555 1557 1558 1560 1562 1563 1565 1568 1569 1573 1579 1582 1588\n",
      " 1593 1594 1603 1617 1620 1629 1630 1633 1638 1642 1644 1646 1653 1658\n",
      " 1662 1666 1671 1672 1675 1685 1689 1690 1692 1694 1710 1713 1716 1719\n",
      " 1721 1725 1726 1730 1733 1741 1743 1745 1746 1747 1756 1760 1763 1765\n",
      " 1780 1781 1783 1799 1800 1801 1806 1809 1810 1822 1826 1838 1843 1851\n",
      " 1852 1853 1854 1855 1860 1861 1864 1872 1875 1878 1881 1892 1898 1899\n",
      " 1902 1903 1906 1907 1921 1925 1926 1927 1929 1932 1935 1936 1938 1962\n",
      " 1971 1976 1977 1980 1981 1988 1991 1992]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.6233 - acc: 0.1328\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5806 - acc: 0.1494\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5612 - acc: 0.1598\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.5435 - acc: 0.1673\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5253 - acc: 0.1752\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.5206 - acc: 0.1773\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.5130 - acc: 0.1810\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5084 - acc: 0.1827\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5033 - acc: 0.1848\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4984 - acc: 0.1867\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4952 - acc: 0.1879\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4940 - acc: 0.1886\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4886 - acc: 0.1906\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4857 - acc: 0.1919\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4800 - acc: 0.1941\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4757 - acc: 0.1956\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4718 - acc: 0.1971\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4693 - acc: 0.1980\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4629 - acc: 0.2006\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4571 - acc: 0.2034\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4520 - acc: 0.2043\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 26s 17ms/sample - loss: 0.4456 - acc: 0.2073\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4424 - acc: 0.2081\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4372 - acc: 0.2118\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4304 - acc: 0.2128\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4240 - acc: 0.2147\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4141 - acc: 0.2172\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4103 - acc: 0.2186\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4030 - acc: 0.2213\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3972 - acc: 0.2231\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3913 - acc: 0.2245\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3855 - acc: 0.2267\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3818 - acc: 0.2277\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3762 - acc: 0.2298\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3730 - acc: 0.2310\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3665 - acc: 0.2324\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3607 - acc: 0.2342\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3615 - acc: 0.2341\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3542 - acc: 0.2362\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3485 - acc: 0.2383\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3435 - acc: 0.2396\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3394 - acc: 0.2410\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3351 - acc: 0.2424\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3334 - acc: 0.2430\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3316 - acc: 0.2435\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3275 - acc: 0.2448\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3256 - acc: 0.2452\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3211 - acc: 0.2470\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3177 - acc: 0.2478\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3170 - acc: 0.2479\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3110 - acc: 0.2502\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3066 - acc: 0.2517\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3035 - acc: 0.2526\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3041 - acc: 0.2521\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3003 - acc: 0.2539\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2984 - acc: 0.2545\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2962 - acc: 0.2552\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2944 - acc: 0.2558\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2900 - acc: 0.2572\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2882 - acc: 0.2575\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2849 - acc: 0.2591\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2841 - acc: 0.2591\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2809 - acc: 0.2600\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2769 - acc: 0.2616\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2751 - acc: 0.2621\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2767 - acc: 0.2616\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2738 - acc: 0.2626\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2730 - acc: 0.2629\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2691 - acc: 0.2641\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2681 - acc: 0.2645\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2678 - acc: 0.2645\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2642 - acc: 0.2662\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2618 - acc: 0.2667\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2593 - acc: 0.2679\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2609 - acc: 0.2673\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2583 - acc: 0.2681\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2568 - acc: 0.2686\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2545 - acc: 0.2692\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2528 - acc: 0.2700\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2516 - acc: 0.2705\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2491 - acc: 0.2712\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2475 - acc: 0.2719\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2454 - acc: 0.2723\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2449 - acc: 0.2729\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2455 - acc: 0.2725\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2424 - acc: 0.2737\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2409 - acc: 0.2746\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2409 - acc: 0.2743\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2413 - acc: 0.2743\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2366 - acc: 0.2760\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2364 - acc: 0.2757\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2366 - acc: 0.2759\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2335 - acc: 0.2771\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2320 - acc: 0.2778\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2311 - acc: 0.2779\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2307 - acc: 0.2777\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2298 - acc: 0.2783\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2287 - acc: 0.2787\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2277 - acc: 0.2791\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2277 - acc: 0.2792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.06      0.11      1216\n",
      "           1       0.58      0.67      0.62     21321\n",
      "           2       0.30      0.29      0.30      4086\n",
      "           3       0.73      0.71      0.72     35287\n",
      "           4       0.47      0.29      0.36       678\n",
      "           5       0.44      0.52      0.48     20771\n",
      "           6       0.39      0.16      0.23      9204\n",
      "           7       0.38      0.40      0.39     11774\n",
      "\n",
      "    accuracy                           0.56    104337\n",
      "   macro avg       0.49      0.39      0.40    104337\n",
      "weighted avg       0.55      0.56      0.55    104337\n",
      "\n",
      "Acur√°cia\n",
      "0.38899528619781787\n",
      "Precisao\n",
      "0.5535203867730304\n",
      "Recall\n",
      "0.5559197600084342\n",
      "F1\n",
      "0.5462962995575783\n",
      "[[   75   258    40   166     1   487    57   132]\n",
      " [   11 14309   383  2301    36  3088   298   895]\n",
      " [    0   476  1191   843    11   875    94   596]\n",
      " [    1  3597   671 25134   115  3283   311  2175]\n",
      " [    0    83    24   261   196    68     8    38]\n",
      " [   26  3482   632  2446    19 10899   924  2343]\n",
      " [    5  1316   373  1076    10  3405  1480  1539]\n",
      " [    3  1160   595  2080    28  2546   643  4719]]\n",
      "TRAIN: [   0    3    4 ... 1992 1993 1998] TEST: [   1    2   17   25   27   29   31   47   49   57   65   67   78   80\n",
      "   85   86   90   93   95  102  109  110  115  118  119  120  124  125\n",
      "  131  147  151  152  154  166  172  182  183  199  206  214  217  220\n",
      "  230  233  248  252  257  260  263  270  272  283  285  287  289  302\n",
      "  305  320  321  327  331  332  336  349  352  356  360  361  362  378\n",
      "  385  393  396  401  411  414  442  445  448  450  460  470  474  475\n",
      "  478  483  487  495  497  500  504  516  521  529  530  534  537  543\n",
      "  546  547  548  552  554  556  558  561  576  577  588  589  591  606\n",
      "  616  619  621  625  633  638  640  653  658  660  665  667  670  671\n",
      "  672  673  677  682  687  697  705  710  712  715  716  718  723  730\n",
      "  731  732  741  742  743  744  745  752  753  758  761  763  765  771\n",
      "  774  776  781  784  785  786  789  796  799  800  803  807  809  810\n",
      "  812  815  821  824  833  834  843  845  846  852  855  859  865  868\n",
      "  872  886  887  889  894  900  910  928  930  932  933  936  939  952\n",
      "  971  986  988  989  990  992 1003 1009 1011 1015 1017 1026 1028 1039\n",
      " 1042 1047 1052 1056 1059 1068 1073 1074 1085 1090 1091 1096 1099 1126\n",
      " 1147 1149 1155 1158 1164 1165 1168 1172 1176 1182 1192 1195 1200 1201\n",
      " 1204 1207 1210 1211 1214 1218 1219 1221 1228 1230 1231 1232 1251 1261\n",
      " 1263 1266 1269 1270 1277 1287 1288 1289 1295 1296 1298 1303 1308 1312\n",
      " 1315 1316 1324 1326 1328 1329 1346 1348 1356 1359 1360 1361 1365 1370\n",
      " 1380 1382 1386 1388 1390 1398 1407 1408 1441 1452 1455 1456 1459 1462\n",
      " 1469 1470 1485 1490 1491 1492 1498 1500 1520 1527 1544 1545 1546 1549\n",
      " 1551 1561 1570 1581 1585 1586 1597 1602 1604 1605 1606 1612 1615 1618\n",
      " 1623 1625 1628 1632 1635 1637 1640 1648 1652 1656 1657 1663 1667 1674\n",
      " 1678 1684 1686 1697 1698 1699 1704 1705 1709 1712 1723 1724 1732 1736\n",
      " 1744 1752 1767 1770 1777 1778 1782 1786 1794 1807 1808 1814 1818 1819\n",
      " 1820 1823 1824 1829 1830 1833 1839 1845 1850 1856 1863 1871 1873 1888\n",
      " 1889 1896 1897 1904 1911 1912 1917 1930 1933 1941 1942 1944 1957 1958\n",
      " 1965 1970 1985 1994 1995 1996 1997 1999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.6346 - acc: 0.1275\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6250 - acc: 0.1212\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6234 - acc: 0.1230\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6217 - acc: 0.1236\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6256 - acc: 0.1446\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6231 - acc: 0.1230\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6220 - acc: 0.1232\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6216 - acc: 0.1232\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6215 - acc: 0.1234\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6206 - acc: 0.1234\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6208 - acc: 0.1234\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6206 - acc: 0.1234\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6205 - acc: 0.1234\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6205 - acc: 0.1234\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6204 - acc: 0.1234\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6204 - acc: 0.1236\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6201 - acc: 0.1237\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6200 - acc: 0.1241\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6210 - acc: 0.1234\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6211 - acc: 0.1228\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6207 - acc: 0.1230\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6203 - acc: 0.1231\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6203 - acc: 0.1233\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6202 - acc: 0.1234\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6200 - acc: 0.1234\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6200 - acc: 0.1234\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6199 - acc: 0.1234\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6200 - acc: 0.1235\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6201 - acc: 0.1233\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6201 - acc: 0.1233\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6199 - acc: 0.1234\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6198 - acc: 0.1234\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6200 - acc: 0.1235\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6198 - acc: 0.1235\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6199 - acc: 0.1234\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6198 - acc: 0.1234\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6199 - acc: 0.1234\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6198 - acc: 0.1235\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6198 - acc: 0.1234\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6196 - acc: 0.1235\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6196 - acc: 0.1235\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6195 - acc: 0.1235\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6195 - acc: 0.1235\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6195 - acc: 0.1234\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6196 - acc: 0.1235\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1234\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1236\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6608 - acc: 0.1226\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6203 - acc: 0.1233\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6196 - acc: 0.1234\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6195 - acc: 0.1235\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6195 - acc: 0.1235\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1234\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6194 - acc: 0.1235\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6191 - acc: 0.1235\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1235\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6189 - acc: 0.1235\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6195 - acc: 0.1235\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6191 - acc: 0.1235\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6191 - acc: 0.1235\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6193 - acc: 0.1234\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6191 - acc: 0.1235\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6190 - acc: 0.1235\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6190 - acc: 0.1235\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6192 - acc: 0.1235\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6191 - acc: 0.1234\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6189 - acc: 0.1235\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6191 - acc: 0.1235\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6191 - acc: 0.1235\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6191 - acc: 0.1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1354\n",
      "           1       0.32      0.02      0.05     21179\n",
      "           2       0.00      0.00      0.00      4064\n",
      "           3       0.34      0.99      0.50     34368\n",
      "           4       0.00      0.00      0.00       603\n",
      "           5       0.70      0.04      0.08     21177\n",
      "           6       0.00      0.00      0.00      8989\n",
      "           7       0.00      0.00      0.00     11698\n",
      "\n",
      "    accuracy                           0.34    103432\n",
      "   macro avg       0.17      0.13      0.08    103432\n",
      "weighted avg       0.32      0.34      0.19    103432\n",
      "\n",
      "Acur√°cia\n",
      "0.13117469479562668\n",
      "Precisao\n",
      "0.32080741850927075\n",
      "Recall\n",
      "0.34054257869904864\n",
      "F1\n",
      "0.19132318609369187\n",
      "[[    0    23     0  1313     0    18     0     0]\n",
      " [    0   513     0 20520     0   146     0     0]\n",
      " [    0    53     0  3993     0    18     0     0]\n",
      " [    0   397     0 33870     0   101     0     0]\n",
      " [    0     0     0   603     0     0     0     0]\n",
      " [    0   378     0 19959     0   840     0     0]\n",
      " [    0   132     0  8821     0    36     0     0]\n",
      " [    0   104     0 11553     0    41     0     0]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [  10   12   20   23   24   33   37   43   46   48   52   56   64   66\n",
      "   71   72   73   75   84   89   91   99  101  108  114  116  121  128\n",
      "  133  138  141  146  149  153  155  156  163  165  167  174  179  180\n",
      "  184  190  193  195  204  216  218  219  225  232  234  239  243  244\n",
      "  246  258  262  266  271  275  280  281  282  290  293  294  307  311\n",
      "  313  317  318  319  323  326  330  335  338  342  344  345  347  357\n",
      "  382  383  384  388  397  398  403  412  419  424  449  461  465  476\n",
      "  481  490  494  496  505  507  514  515  519  520  523  535  536  539\n",
      "  551  560  562  565  574  578  585  586  597  601  609  620  622  641\n",
      "  642  646  647  649  654  656  661  666  668  674  676  683  684  688\n",
      "  689  695  703  709  713  714  719  726  739  748  755  757  759  764\n",
      "  767  769  783  790  795  801  820  823  831  837  838  849  850  856\n",
      "  857  863  871  882  883  890  898  904  920  924  931  935  940  945\n",
      "  948  953  955  959  960  962  963  964  969  970  977  980  985  993\n",
      "  995  998  999 1001 1006 1010 1013 1014 1020 1023 1029 1031 1033 1035\n",
      " 1045 1048 1050 1058 1061 1064 1071 1072 1076 1078 1080 1081 1082 1083\n",
      " 1088 1089 1092 1093 1098 1102 1106 1112 1116 1124 1127 1131 1134 1135\n",
      " 1138 1139 1141 1142 1143 1148 1153 1161 1162 1166 1170 1177 1184 1188\n",
      " 1189 1212 1223 1227 1229 1234 1248 1249 1255 1259 1262 1264 1278 1283\n",
      " 1292 1302 1304 1307 1310 1321 1327 1332 1337 1342 1345 1357 1364 1371\n",
      " 1376 1385 1392 1396 1405 1410 1411 1416 1421 1423 1425 1434 1439 1449\n",
      " 1450 1453 1454 1464 1468 1472 1474 1478 1479 1480 1481 1486 1499 1501\n",
      " 1507 1508 1509 1513 1514 1519 1532 1534 1537 1540 1543 1553 1559 1564\n",
      " 1574 1577 1578 1591 1596 1610 1611 1619 1636 1639 1643 1651 1664 1665\n",
      " 1680 1681 1691 1693 1695 1700 1701 1703 1706 1714 1722 1729 1735 1738\n",
      " 1739 1740 1749 1753 1761 1762 1771 1772 1775 1784 1787 1788 1792 1796\n",
      " 1804 1805 1815 1827 1841 1857 1858 1862 1867 1870 1877 1879 1880 1890\n",
      " 1895 1908 1910 1916 1918 1922 1924 1945 1947 1949 1951 1959 1963 1964\n",
      " 1967 1968 1969 1974 1975 1984 1986 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.6418 - acc: 0.1227\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6298 - acc: 0.1209\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6291 - acc: 0.1228\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6278 - acc: 0.1207\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6263 - acc: 0.1237\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6255 - acc: 0.1240\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6253 - acc: 0.1239\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6253 - acc: 0.1242\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6248 - acc: 0.1241\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6245 - acc: 0.1242\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6244 - acc: 0.1243\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6247 - acc: 0.1242\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6243 - acc: 0.1243\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6242 - acc: 0.1244\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6247 - acc: 0.1232\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6240 - acc: 0.1247\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6252 - acc: 0.1241\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6240 - acc: 0.1242\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6245 - acc: 0.1243\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6238 - acc: 0.1243\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.6209 - acc: 0.1279\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5930 - acc: 0.1437\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5761 - acc: 0.1510\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5626 - acc: 0.1596\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5496 - acc: 0.1657\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5394 - acc: 0.1703\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5293 - acc: 0.1757\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5303 - acc: 0.1757\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5183 - acc: 0.1810\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5150 - acc: 0.1828\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5138 - acc: 0.1836\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5080 - acc: 0.1861\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5072 - acc: 0.1867\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5032 - acc: 0.1884\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4970 - acc: 0.1909\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4960 - acc: 0.1909\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4951 - acc: 0.1915\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4896 - acc: 0.1936\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4858 - acc: 0.1949\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4862 - acc: 0.1950\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4805 - acc: 0.1971\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4794 - acc: 0.1976\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4749 - acc: 0.1994\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4717 - acc: 0.2007\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4694 - acc: 0.2017\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4628 - acc: 0.2040\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4611 - acc: 0.2045\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4542 - acc: 0.2067\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4524 - acc: 0.2074\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4499 - acc: 0.2081\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4439 - acc: 0.2107\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4376 - acc: 0.2127\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4324 - acc: 0.2143\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4284 - acc: 0.2160\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4249 - acc: 0.2171\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4200 - acc: 0.2185\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4166 - acc: 0.2196\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4110 - acc: 0.2214\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.4055 - acc: 0.2232\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3991 - acc: 0.2250\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3968 - acc: 0.2260\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3957 - acc: 0.2267\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3911 - acc: 0.2278\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3860 - acc: 0.2294\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3793 - acc: 0.2317\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3780 - acc: 0.2318\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3699 - acc: 0.2346\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3682 - acc: 0.2352\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3642 - acc: 0.2361\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3585 - acc: 0.2381\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3558 - acc: 0.2391\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3567 - acc: 0.2388\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3487 - acc: 0.2410\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3477 - acc: 0.2414\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3451 - acc: 0.2423\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3386 - acc: 0.2441\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3356 - acc: 0.2448\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3332 - acc: 0.2457\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3301 - acc: 0.2468\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3283 - acc: 0.2473\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3244 - acc: 0.2485\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3210 - acc: 0.2493\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3172 - acc: 0.2506\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3173 - acc: 0.2506\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3160 - acc: 0.2510\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3140 - acc: 0.2518\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3098 - acc: 0.2530\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3083 - acc: 0.2536\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3050 - acc: 0.2548\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3027 - acc: 0.2556\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3001 - acc: 0.2563\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2970 - acc: 0.2574\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2977 - acc: 0.2570\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2969 - acc: 0.2574\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2934 - acc: 0.2582\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2902 - acc: 0.2597\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2871 - acc: 0.2607\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2892 - acc: 0.2601\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2858 - acc: 0.2613\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.2828 - acc: 0.2621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.03      1159\n",
      "           1       0.63      0.66      0.65     21548\n",
      "           2       0.26      0.30      0.28      3723\n",
      "           3       0.70      0.80      0.74     33461\n",
      "           4       0.43      0.24      0.31       719\n",
      "           5       0.44      0.52      0.48     20361\n",
      "           6       0.46      0.07      0.12      8758\n",
      "           7       0.41      0.36      0.38     11461\n",
      "\n",
      "    accuracy                           0.57    101190\n",
      "   macro avg       0.54      0.37      0.37    101190\n",
      "weighted avg       0.56      0.57      0.55    101190\n",
      "\n",
      "Acur√°cia\n",
      "0.37063193247336323\n",
      "Precisao\n",
      "0.563163871772488\n",
      "Recall\n",
      "0.5694436209111572\n",
      "F1\n",
      "0.5456447036823472\n",
      "[[   18   248    36   196     1   537    11   112]\n",
      " [    0 14293   390  2896    44  3185    64   676]\n",
      " [    0   384  1105   928     3   804    20   479]\n",
      " [    0  2079   746 26745    93  2418    64  1316]\n",
      " [    0    60    29   370   176    51     1    32]\n",
      " [    0  3354   881  3281    32 10589   302  1922]\n",
      " [    0  1191   411  1432    22  3614   577  1511]\n",
      " [    0  1068   690  2611    36  2729   208  4119]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_40 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_41 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_42 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_43 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_44 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_45 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_46 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_47 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_48 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_49 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 2,273,608\n",
      "Trainable params: 2,273,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.40017190541695397, 0.13150230690085424, 0.38899528619781787, 0.13117469479562668, 0.37063193247336323]\n",
      "0.2844952251569232\n",
      "Precision total\n",
      "[0.5784424192514481, 0.3057907368040687, 0.5535203867730304, 0.32080741850927075, 0.563163871772488]\n",
      "0.46434496662206115\n",
      "Recalls total\n",
      "[0.5897071537169716, 0.32891466445733225, 0.5559197600084342, 0.34054257869904864, 0.5694436209111572]\n",
      "0.4769055555585887\n",
      "F1 total\n",
      "[0.5728260266120115, 0.18071996959574058, 0.5462962995575783, 0.19132318609369187, 0.5456447036823472]\n",
      "0.4073620371082739\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accuq8)\n",
    "a = np.array(accuq8)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "p = np.array(precisionsq8)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "r = np.array(recallsq8)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1q8)\n",
    "f = np.array(f1q8)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
