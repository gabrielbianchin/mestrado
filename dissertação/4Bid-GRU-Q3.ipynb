{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNGRU, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0822 11:39:34.055645  5300 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 11:39:34.060603  5300 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 11:39:34.061599  5300 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 11:39:34.062597  5300 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 11:39:35.078909  5300 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   1    3    4 ... 1996 1997 1998] TEST: [   0    2    6   14   23   24   27   28   30   32   33   36   40   41\n",
      "   42   43   65   66   71   73   82   84  106  112  113  114  116  117\n",
      "  118  124  125  127  130  131  132  133  134  139  147  154  168  173\n",
      "  185  186  191  193  196  202  205  210  213  222  232  233  239  240\n",
      "  251  256  264  275  279  284  288  290  291  293  295  300  305  308\n",
      "  312  314  319  328  329  333  338  372  376  382  385  388  397  401\n",
      "  402  403  406  408  409  415  422  423  427  429  431  437  443  450\n",
      "  453  456  463  467  468  474  477  487  490  492  497  502  506  509\n",
      "  511  512  513  519  524  525  526  529  531  533  535  546  549  551\n",
      "  555  557  560  561  573  576  584  588  590  592  594  603  605  606\n",
      "  622  623  625  628  631  636  639  641  642  643  651  668  671  672\n",
      "  673  686  688  690  692  697  698  700  711  729  731  734  738  740\n",
      "  754  766  769  773  783  787  794  801  806  811  813  817  828  831\n",
      "  834  840  843  852  873  893  895  904  910  912  913  921  926  928\n",
      "  930  933  941  942  947  948  956  958  964  965  966  990  996  998\n",
      " 1003 1008 1010 1012 1015 1016 1022 1023 1029 1031 1032 1035 1042 1052\n",
      " 1070 1072 1079 1082 1086 1094 1097 1108 1110 1129 1132 1133 1136 1137\n",
      " 1140 1141 1147 1150 1151 1158 1159 1162 1167 1184 1186 1189 1191 1192\n",
      " 1213 1215 1221 1222 1227 1229 1232 1234 1244 1247 1251 1253 1255 1263\n",
      " 1264 1272 1273 1279 1288 1291 1300 1302 1308 1316 1322 1327 1329 1333\n",
      " 1335 1351 1363 1366 1369 1371 1376 1387 1389 1400 1413 1427 1437 1438\n",
      " 1442 1445 1454 1458 1464 1465 1474 1479 1485 1486 1488 1489 1495 1497\n",
      " 1500 1508 1509 1521 1525 1532 1537 1548 1550 1557 1558 1559 1560 1564\n",
      " 1566 1576 1577 1580 1602 1604 1611 1621 1630 1631 1633 1634 1635 1642\n",
      " 1646 1653 1671 1680 1682 1686 1692 1693 1696 1701 1704 1705 1713 1730\n",
      " 1732 1735 1738 1748 1749 1756 1760 1762 1768 1777 1789 1791 1806 1823\n",
      " 1825 1834 1835 1841 1849 1851 1853 1857 1859 1860 1862 1863 1866 1871\n",
      " 1873 1877 1887 1891 1892 1905 1909 1930 1935 1942 1944 1951 1956 1959\n",
      " 1968 1977 1983 1987 1992 1993 1994 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3291 - acc: 0.8437\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3082 - acc: 0.8612\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3013 - acc: 0.8649\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2991 - acc: 0.8651\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2952 - acc: 0.8672\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2921 - acc: 0.8693\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2900 - acc: 0.8703\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2877 - acc: 0.8709\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2845 - acc: 0.8719\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2844 - acc: 0.8723\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2805 - acc: 0.8742\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2801 - acc: 0.8748\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2773 - acc: 0.8764\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2775 - acc: 0.8769\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2761 - acc: 0.8771\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2708 - acc: 0.8798\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2684 - acc: 0.8818\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2655 - acc: 0.8837\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2607 - acc: 0.8860\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2541 - acc: 0.8897\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2516 - acc: 0.8907\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2493 - acc: 0.8920\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2463 - acc: 0.8936\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2429 - acc: 0.8953\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2443 - acc: 0.8946\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2426 - acc: 0.8950\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2379 - acc: 0.8974\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2348 - acc: 0.8988\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2317 - acc: 0.9003\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2310 - acc: 0.9006\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2300 - acc: 0.9010\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2293 - acc: 0.9015\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2277 - acc: 0.9022\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2222 - acc: 0.9044\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2191 - acc: 0.9062\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2169 - acc: 0.9069\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2142 - acc: 0.9081\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2120 - acc: 0.9095\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2095 - acc: 0.9104\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2073 - acc: 0.9115\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2054 - acc: 0.9126\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2032 - acc: 0.9133\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2013 - acc: 0.9142\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1994 - acc: 0.9151\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1974 - acc: 0.9158\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1947 - acc: 0.9172\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1923 - acc: 0.9181\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1910 - acc: 0.9188\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1915 - acc: 0.9187\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1878 - acc: 0.9202\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1862 - acc: 0.9207\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1843 - acc: 0.9216\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1820 - acc: 0.9228\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1805 - acc: 0.9233\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1787 - acc: 0.9246\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1771 - acc: 0.9251\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1762 - acc: 0.9255\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1749 - acc: 0.9260\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1737 - acc: 0.9266\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1712 - acc: 0.9279\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1709 - acc: 0.9275\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1684 - acc: 0.9288\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1674 - acc: 0.9294\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1661 - acc: 0.9299\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1650 - acc: 0.9304\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1633 - acc: 0.9311\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1623 - acc: 0.9316\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1625 - acc: 0.9316\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1596 - acc: 0.9327\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1585 - acc: 0.9332\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1570 - acc: 0.9337\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1561 - acc: 0.9342\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1568 - acc: 0.9339\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1542 - acc: 0.9352\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1533 - acc: 0.9354\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1519 - acc: 0.9363\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1518 - acc: 0.9362\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1505 - acc: 0.9365\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1493 - acc: 0.9371\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1492 - acc: 0.9373\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1482 - acc: 0.9379\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1462 - acc: 0.9387\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1457 - acc: 0.9390\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1461 - acc: 0.9386\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1443 - acc: 0.9396\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1438 - acc: 0.9400\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1437 - acc: 0.9399\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1422 - acc: 0.9406\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1412 - acc: 0.9408\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1397 - acc: 0.9417\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1389 - acc: 0.9420\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1389 - acc: 0.9418\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1377 - acc: 0.9427\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1370 - acc: 0.9429\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1368 - acc: 0.9430\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1349 - acc: 0.9436\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1350 - acc: 0.9439\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1337 - acc: 0.9443\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1338 - acc: 0.9443\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1331 - acc: 0.9447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.71     44021\n",
      "           1       0.70      0.60      0.65     24276\n",
      "           2       0.77      0.77      0.77     40706\n",
      "\n",
      "    accuracy                           0.72    109003\n",
      "   macro avg       0.72      0.70      0.71    109003\n",
      "weighted avg       0.72      0.72      0.72    109003\n",
      "\n",
      "Acurácia\n",
      "0.7034039533945324\n",
      "Precisao\n",
      "0.7197155139788294\n",
      "Recall\n",
      "0.7194756107630065\n",
      "F1\n",
      "0.7185074785763638\n",
      "[[32417  4611  6993]\n",
      " [ 7217 14649  2410]\n",
      " [ 7568  1779 31359]]\n",
      "TRAIN: [   0    2    3 ... 1997 1998 1999] TEST: [   1   11   22   34   35   39   44   45   46   51   52   54   59   60\n",
      "   67   70   72   76   78   80   81   83   88   98   99  100  110  111\n",
      "  115  136  141  145  149  153  157  158  162  163  166  169  170  171\n",
      "  182  183  198  209  211  225  226  229  231  236  238  244  246  249\n",
      "  255  259  261  267  271  277  282  289  292  294  297  303  313  320\n",
      "  321  325  330  331  341  342  344  348  360  362  364  368  374  375\n",
      "  389  392  393  394  405  407  425  430  433  434  448  460  465  478\n",
      "  483  493  504  515  517  518  522  528  538  543  547  563  566  572\n",
      "  577  579  585  597  601  604  611  614  615  619  620  621  649  654\n",
      "  657  661  663  674  675  676  677  678  679  681  684  693  696  701\n",
      "  703  704  714  718  720  725  730  735  736  743  751  755  759  760\n",
      "  764  776  782  788  790  792  799  815  819  827  830  835  855  856\n",
      "  867  871  876  879  883  888  892  897  899  901  906  911  920  924\n",
      "  927  936  946  951  961  972  973  977  979  981  987  991  997 1002\n",
      " 1007 1011 1021 1036 1039 1047 1049 1051 1053 1062 1064 1066 1067 1068\n",
      " 1074 1085 1087 1095 1102 1106 1117 1120 1121 1123 1126 1127 1131 1142\n",
      " 1144 1146 1160 1169 1177 1182 1183 1193 1201 1208 1211 1216 1217 1223\n",
      " 1226 1237 1239 1243 1245 1254 1258 1262 1265 1267 1268 1275 1280 1281\n",
      " 1283 1294 1298 1299 1305 1306 1313 1315 1317 1318 1323 1325 1326 1328\n",
      " 1332 1334 1336 1339 1342 1350 1360 1365 1367 1373 1377 1380 1381 1382\n",
      " 1391 1395 1397 1404 1406 1411 1418 1419 1423 1424 1426 1428 1435 1447\n",
      " 1448 1449 1455 1457 1467 1469 1478 1491 1492 1498 1506 1515 1519 1523\n",
      " 1531 1533 1534 1538 1539 1540 1542 1553 1554 1567 1568 1585 1586 1600\n",
      " 1605 1612 1624 1626 1637 1650 1656 1664 1666 1668 1670 1684 1698 1702\n",
      " 1703 1710 1715 1718 1724 1739 1742 1764 1772 1774 1776 1778 1779 1782\n",
      " 1783 1784 1795 1801 1802 1805 1807 1810 1812 1815 1818 1819 1820 1828\n",
      " 1829 1832 1833 1836 1837 1850 1861 1872 1881 1882 1894 1898 1899 1903\n",
      " 1906 1907 1912 1915 1919 1924 1925 1931 1939 1940 1946 1947 1952 1954\n",
      " 1958 1963 1967 1979 1982 1984 1986 1991]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3386 - acc: 0.7861\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3141 - acc: 0.8580\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3071 - acc: 0.8623\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3047 - acc: 0.8634\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3020 - acc: 0.8644\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2983 - acc: 0.8663\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2986 - acc: 0.8665\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2946 - acc: 0.8687\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2932 - acc: 0.8688\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2953 - acc: 0.8678\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2902 - acc: 0.8701\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2910 - acc: 0.8694\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2884 - acc: 0.8718\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2835 - acc: 0.8736\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2808 - acc: 0.8756\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2795 - acc: 0.8761\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2778 - acc: 0.8773\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2750 - acc: 0.8786\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2725 - acc: 0.8800\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2697 - acc: 0.8815\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2691 - acc: 0.8819\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2639 - acc: 0.8847\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2571 - acc: 0.8879\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2526 - acc: 0.8906\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2505 - acc: 0.8913\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2459 - acc: 0.8939\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2439 - acc: 0.8946\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2418 - acc: 0.8955\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2389 - acc: 0.8968\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2359 - acc: 0.8984\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2331 - acc: 0.8993\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2306 - acc: 0.9006\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2275 - acc: 0.9022\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2266 - acc: 0.9024\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2236 - acc: 0.9038\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2207 - acc: 0.9052\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2176 - acc: 0.9069\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2154 - acc: 0.9076\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2129 - acc: 0.9091\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2106 - acc: 0.9097\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2081 - acc: 0.9110\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2066 - acc: 0.9116\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2037 - acc: 0.9131\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2016 - acc: 0.9140\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1990 - acc: 0.9151\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1987 - acc: 0.9154\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1963 - acc: 0.9161\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1935 - acc: 0.9175\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1913 - acc: 0.9185\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1899 - acc: 0.9194\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1888 - acc: 0.9199\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1863 - acc: 0.9207\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1844 - acc: 0.9218\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1831 - acc: 0.9220\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1811 - acc: 0.9230\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1794 - acc: 0.9239\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1779 - acc: 0.9245\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1757 - acc: 0.9256\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1751 - acc: 0.9259\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1737 - acc: 0.9265\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1728 - acc: 0.9268\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1714 - acc: 0.9276\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1695 - acc: 0.9284\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1677 - acc: 0.9291\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1669 - acc: 0.9291\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1667 - acc: 0.9294\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1641 - acc: 0.9307\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1634 - acc: 0.9309\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1624 - acc: 0.9315\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1620 - acc: 0.9317\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1608 - acc: 0.9323\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1598 - acc: 0.9326\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1573 - acc: 0.9339\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1569 - acc: 0.9339\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1558 - acc: 0.9344\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1551 - acc: 0.9347\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1540 - acc: 0.9347\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1526 - acc: 0.9359\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1515 - acc: 0.9362\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1509 - acc: 0.93655s\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1498 - acc: 0.9369\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1489 - acc: 0.9373\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1482 - acc: 0.9374\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1474 - acc: 0.9381\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1471 - acc: 0.9381\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1457 - acc: 0.9383\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1455 - acc: 0.9388\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1445 - acc: 0.9390\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1441 - acc: 0.9394\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1420 - acc: 0.9403\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1422 - acc: 0.9400\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1411 - acc: 0.9406\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1412 - acc: 0.9405\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1407 - acc: 0.9408\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1402 - acc: 0.9410\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1379 - acc: 0.9420\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1377 - acc: 0.9420\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1375 - acc: 0.9422\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1364 - acc: 0.9426\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1364 - acc: 0.9423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71     42228\n",
      "           1       0.69      0.63      0.66     22358\n",
      "           2       0.74      0.80      0.77     38418\n",
      "\n",
      "    accuracy                           0.72    103004\n",
      "   macro avg       0.72      0.71      0.71    103004\n",
      "weighted avg       0.72      0.72      0.72    103004\n",
      "\n",
      "Acurácia\n",
      "0.7096051051888246\n",
      "Precisao\n",
      "0.7209530564007298\n",
      "Recall\n",
      "0.7220981709448177\n",
      "F1\n",
      "0.7206524189505312\n",
      "[[29592  4570  8066]\n",
      " [ 5540 14027  2791]\n",
      " [ 6056  1602 30760]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   4    9   13   17   18   25   37   47   56   57   68   85   97  103\n",
      "  105  107  108  126  129  138  146  150  155  159  167  172  177  187\n",
      "  200  214  215  217  219  227  248  260  265  273  281  285  296  301\n",
      "  302  307  310  311  315  316  326  337  340  343  352  353  355  358\n",
      "  361  363  370  373  380  381  383  386  404  417  428  432  440  445\n",
      "  446  447  451  454  455  457  459  462  476  479  481  484  489  495\n",
      "  498  499  505  520  530  532  534  536  542  544  552  554  562  569\n",
      "  570  571  583  589  607  609  624  629  632  633  634  638  646  647\n",
      "  652  659  662  665  666  682  687  705  706  712  713  717  721  722\n",
      "  732  737  744  748  752  753  758  767  774  777  779  780  784  789\n",
      "  793  796  797  800  802  803  807  808  810  818  821  823  825  826\n",
      "  829  832  837  842  846  853  854  858  864  865  866  877  885  886\n",
      "  887  890  891  905  907  908  923  925  929  935  940  943  944  952\n",
      "  959  962  963  967  968  969  978  983  984  985  993 1000 1001 1018\n",
      " 1026 1033 1034 1037 1038 1041 1043 1050 1054 1057 1058 1060 1063 1065\n",
      " 1069 1075 1081 1089 1096 1101 1111 1118 1130 1134 1139 1145 1149 1152\n",
      " 1153 1156 1161 1163 1170 1172 1173 1185 1187 1188 1194 1195 1196 1209\n",
      " 1214 1224 1228 1249 1252 1256 1257 1284 1295 1296 1301 1310 1312 1319\n",
      " 1320 1321 1330 1331 1343 1346 1354 1357 1361 1364 1383 1385 1386 1388\n",
      " 1394 1398 1399 1405 1416 1417 1433 1443 1450 1459 1460 1466 1470 1473\n",
      " 1475 1481 1484 1487 1490 1499 1501 1503 1507 1510 1511 1512 1514 1517\n",
      " 1520 1522 1524 1526 1530 1536 1545 1546 1551 1565 1573 1579 1582 1587\n",
      " 1590 1591 1596 1601 1607 1609 1610 1617 1619 1620 1623 1628 1629 1639\n",
      " 1647 1648 1654 1660 1674 1675 1676 1679 1681 1687 1695 1697 1699 1709\n",
      " 1712 1721 1723 1725 1726 1729 1733 1734 1737 1741 1744 1746 1747 1755\n",
      " 1757 1758 1759 1761 1765 1773 1786 1787 1790 1800 1809 1811 1824 1826\n",
      " 1830 1839 1840 1843 1845 1847 1852 1856 1858 1865 1874 1875 1876 1886\n",
      " 1889 1901 1902 1908 1910 1917 1920 1932 1937 1938 1941 1943 1950 1957\n",
      " 1960 1962 1965 1969 1971 1974 1989 1990]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3349 - acc: 0.8340\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3126 - acc: 0.8588\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3067 - acc: 0.8623\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3022 - acc: 0.8648\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2999 - acc: 0.8657\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2972 - acc: 0.8673\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2963 - acc: 0.8679\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2962 - acc: 0.8679\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2921 - acc: 0.8698\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2885 - acc: 0.8716\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2855 - acc: 0.8727\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2851 - acc: 0.8725\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2841 - acc: 0.8736\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2831 - acc: 0.8741\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2795 - acc: 0.8760\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2794 - acc: 0.8764\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2771 - acc: 0.8768\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2740 - acc: 0.8790\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2758 - acc: 0.8777\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2712 - acc: 0.8810\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2638 - acc: 0.8847\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2582 - acc: 0.8873\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2530 - acc: 0.8894\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2508 - acc: 0.8909\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2498 - acc: 0.8913\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2463 - acc: 0.8929\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2433 - acc: 0.8944\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2404 - acc: 0.8958\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2391 - acc: 0.8963\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2375 - acc: 0.8972\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2362 - acc: 0.8977\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2324 - acc: 0.8992\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2299 - acc: 0.9008\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2259 - acc: 0.9023\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2237 - acc: 0.9034\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2228 - acc: 0.9039\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2206 - acc: 0.9050\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2170 - acc: 0.9066\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2155 - acc: 0.9073\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2137 - acc: 0.9080\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2127 - acc: 0.9084\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2089 - acc: 0.9101\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2065 - acc: 0.9117\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2062 - acc: 0.9113\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2029 - acc: 0.9134\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2006 - acc: 0.9141\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1982 - acc: 0.9153\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1960 - acc: 0.9161\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1952 - acc: 0.9168\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1937 - acc: 0.9174\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1911 - acc: 0.9184\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1905 - acc: 0.9187\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1879 - acc: 0.9200\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1862 - acc: 0.9205\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1835 - acc: 0.9220\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1822 - acc: 0.9224\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1826 - acc: 0.9222\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1797 - acc: 0.9237\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1786 - acc: 0.9237\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1760 - acc: 0.9254\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1756 - acc: 0.9255\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1735 - acc: 0.9263\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1723 - acc: 0.9270\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1731 - acc: 0.9262\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1714 - acc: 0.9274\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1693 - acc: 0.9283\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1677 - acc: 0.9292\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1667 - acc: 0.9293\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1651 - acc: 0.9300\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1652 - acc: 0.9303\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1624 - acc: 0.9312\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1615 - acc: 0.9317\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1607 - acc: 0.9321\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1598 - acc: 0.9324\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1582 - acc: 0.9332\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1568 - acc: 0.9337\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1572 - acc: 0.9338\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1566 - acc: 0.9339\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1543 - acc: 0.9349\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1537 - acc: 0.9349\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1516 - acc: 0.9359\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1512 - acc: 0.9361\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1500 - acc: 0.9369\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1496 - acc: 0.9371\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1492 - acc: 0.9369\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1482 - acc: 0.9376\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1472 - acc: 0.9383\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1469 - acc: 0.9380\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1464 - acc: 0.9382\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1449 - acc: 0.9388\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1442 - acc: 0.9395\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1452 - acc: 0.9386\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1427 - acc: 0.9402\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1419 - acc: 0.9397\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1405 - acc: 0.9409\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1412 - acc: 0.9406\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1403 - acc: 0.9411\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1393 - acc: 0.9417\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1388 - acc: 0.9419\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1378 - acc: 0.9420\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72     42270\n",
      "           1       0.68      0.62      0.65     22783\n",
      "           2       0.76      0.78      0.77     37730\n",
      "\n",
      "    accuracy                           0.72    102783\n",
      "   macro avg       0.72      0.71      0.71    102783\n",
      "weighted avg       0.72      0.72      0.72    102783\n",
      "\n",
      "Acurácia\n",
      "0.7109929760742442\n",
      "Precisao\n",
      "0.7236323439927882\n",
      "Recall\n",
      "0.7245264294679081\n",
      "F1\n",
      "0.7236837474882478\n",
      "[[30852  4800  6618]\n",
      " [ 6121 14209  2453]\n",
      " [ 6548  1774 29408]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [   7   10   12   15   38   53   58   62   63   74   75   77   86   89\n",
      "   90   92   93   96  102  104  120  128  135  137  143  148  160  165\n",
      "  176  178  179  180  189  190  194  197  201  203  204  207  208  216\n",
      "  220  223  224  228  237  242  247  250  254  257  258  262  268  274\n",
      "  276  280  283  306  322  323  324  332  335  336  345  346  350  351\n",
      "  354  357  359  365  367  378  379  384  387  391  395  396  399  400\n",
      "  411  412  414  416  420  424  439  441  442  458  461  466  469  471\n",
      "  472  475  482  486  496  501  503  510  516  539  548  550  556  558\n",
      "  564  565  568  574  580  582  586  593  596  598  602  610  612  616\n",
      "  618  635  637  653  656  667  670  680  683  685  689  691  708  709\n",
      "  710  715  716  723  724  726  727  739  741  745  746  756  757  761\n",
      "  763  768  772  785  786  795  798  804  805  809  820  822  824  836\n",
      "  839  844  845  851  859  861  863  869  874  875  878  889  894  900\n",
      "  903  909  916  919  932  934  937  939  950  953  970  974  982  986\n",
      "  988  989  995  999 1004 1006 1013 1014 1020 1024 1028 1040 1044 1045\n",
      " 1046 1048 1055 1056 1073 1077 1080 1083 1084 1090 1091 1092 1093 1100\n",
      " 1103 1105 1109 1112 1115 1119 1124 1125 1135 1138 1148 1154 1165 1168\n",
      " 1171 1174 1179 1180 1181 1190 1199 1202 1205 1206 1207 1210 1212 1218\n",
      " 1220 1240 1241 1246 1248 1260 1266 1269 1270 1277 1282 1285 1287 1290\n",
      " 1292 1307 1314 1324 1337 1353 1356 1359 1368 1372 1374 1375 1378 1379\n",
      " 1393 1401 1412 1414 1415 1420 1421 1429 1431 1432 1434 1439 1440 1441\n",
      " 1446 1456 1461 1462 1463 1471 1477 1496 1502 1527 1543 1544 1547 1552\n",
      " 1561 1570 1571 1575 1584 1588 1592 1595 1597 1598 1599 1603 1606 1608\n",
      " 1613 1614 1616 1618 1632 1638 1640 1643 1651 1655 1659 1661 1662 1677\n",
      " 1678 1685 1689 1690 1694 1708 1711 1717 1720 1727 1731 1736 1743 1750\n",
      " 1763 1766 1767 1780 1788 1792 1796 1797 1798 1803 1808 1813 1814 1816\n",
      " 1821 1838 1844 1848 1854 1855 1869 1870 1879 1880 1883 1885 1888 1893\n",
      " 1897 1900 1904 1911 1913 1914 1921 1923 1933 1936 1948 1949 1953 1955\n",
      " 1961 1970 1973 1976 1980 1988 1995 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3360 - acc: 0.8267\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3151 - acc: 0.8577\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3077 - acc: 0.8620\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3055 - acc: 0.8627\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3026 - acc: 0.8641\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2999 - acc: 0.8655\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2974 - acc: 0.8667\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2952 - acc: 0.8682\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2923 - acc: 0.8691\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2900 - acc: 0.8705\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2878 - acc: 0.8713\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2883 - acc: 0.8704\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2861 - acc: 0.8722\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2842 - acc: 0.8727\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2834 - acc: 0.8732\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2795 - acc: 0.8753\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2795 - acc: 0.8753\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2776 - acc: 0.8758\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2751 - acc: 0.8779\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2723 - acc: 0.8790\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2688 - acc: 0.8815\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2659 - acc: 0.8827\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2605 - acc: 0.8855\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2559 - acc: 0.8880\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2509 - acc: 0.8905\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2498 - acc: 0.8909\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2452 - acc: 0.8930\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2438 - acc: 0.8936\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2396 - acc: 0.8960\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2371 - acc: 0.8969\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2338 - acc: 0.8989\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2313 - acc: 0.8995\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2287 - acc: 0.9008\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2263 - acc: 0.9016\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2238 - acc: 0.9027\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2227 - acc: 0.9034\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2199 - acc: 0.9048\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2171 - acc: 0.9059\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2150 - acc: 0.9071\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2126 - acc: 0.9077\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2103 - acc: 0.9089\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2077 - acc: 0.9100\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2048 - acc: 0.9113\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2029 - acc: 0.9122\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2007 - acc: 0.9128\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1990 - acc: 0.9140\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1984 - acc: 0.9141\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1974 - acc: 0.9145\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 12s 7ms/sample - loss: 0.1941 - acc: 0.9161\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1917 - acc: 0.9171\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1902 - acc: 0.9174\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1895 - acc: 0.9182\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1867 - acc: 0.9188\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1849 - acc: 0.9197\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1831 - acc: 0.9211\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1829 - acc: 0.9212\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1813 - acc: 0.9219\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1795 - acc: 0.9225\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1771 - acc: 0.9233\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1758 - acc: 0.9235\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1744 - acc: 0.9242\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1724 - acc: 0.9245\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1713 - acc: 0.9260\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1707 - acc: 0.9259\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1714 - acc: 0.9255\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1688 - acc: 0.9269\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1672 - acc: 0.9274\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1664 - acc: 0.9280\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1643 - acc: 0.9293\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1634 - acc: 0.9297\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1621 - acc: 0.9303\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1609 - acc: 0.9306\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1595 - acc: 0.9305\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1591 - acc: 0.9314\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1588 - acc: 0.9320\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1576 - acc: 0.9323\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1569 - acc: 0.9325\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1550 - acc: 0.9332\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1539 - acc: 0.9339\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1545 - acc: 0.9334\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1532 - acc: 0.9336\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1511 - acc: 0.9346\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1510 - acc: 0.9350\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1498 - acc: 0.9358\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1491 - acc: 0.9360\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1480 - acc: 0.9364\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1478 - acc: 0.9371\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1465 - acc: 0.9375\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1458 - acc: 0.9374\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1456 - acc: 0.9373\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1441 - acc: 0.9384\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1432 - acc: 0.9386\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1430 - acc: 0.9385\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1419 - acc: 0.9391\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1409 - acc: 0.9399\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1411 - acc: 0.9397\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1394 - acc: 0.9402\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1390 - acc: 0.9401\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1388 - acc: 0.9402\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1377 - acc: 0.9412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73     42124\n",
      "           1       0.70      0.62      0.66     22942\n",
      "           2       0.77      0.77      0.77     35996\n",
      "\n",
      "    accuracy                           0.73    101062\n",
      "   macro avg       0.73      0.71      0.72    101062\n",
      "weighted avg       0.73      0.73      0.73    101062\n",
      "\n",
      "Acurácia\n",
      "0.7145767650989309\n",
      "Precisao\n",
      "0.7293643417221605\n",
      "Recall\n",
      "0.72939383744632\n",
      "F1\n",
      "0.7284291972852662\n",
      "[[31794  4486  5844]\n",
      " [ 6401 14195  2346]\n",
      " [ 6785  1486 27725]]\n",
      "TRAIN: [   0    1    2 ... 1995 1998 1999] TEST: [   3    5    8   16   19   20   21   26   29   31   48   49   50   55\n",
      "   61   64   69   79   87   91   94   95  101  109  119  121  122  123\n",
      "  140  142  144  151  152  156  161  164  174  175  181  184  188  192\n",
      "  195  199  206  212  218  221  230  234  235  241  243  245  252  253\n",
      "  263  266  269  270  272  278  286  287  298  299  304  309  317  318\n",
      "  327  334  339  347  349  356  366  369  371  377  390  398  410  413\n",
      "  418  419  421  426  435  436  438  444  449  452  464  470  473  480\n",
      "  485  488  491  494  500  507  508  514  521  523  527  537  540  541\n",
      "  545  553  559  567  575  578  581  587  591  595  599  600  608  613\n",
      "  617  626  627  630  640  644  645  648  650  655  658  660  664  669\n",
      "  694  695  699  702  707  719  728  733  742  747  749  750  762  765\n",
      "  770  771  775  778  781  791  812  814  816  833  838  841  847  848\n",
      "  849  850  857  860  862  868  870  872  880  881  882  884  896  898\n",
      "  902  914  915  917  918  922  931  938  945  949  954  955  957  960\n",
      "  971  975  976  980  992  994 1005 1009 1017 1019 1025 1027 1030 1059\n",
      " 1061 1071 1076 1078 1088 1098 1099 1104 1107 1113 1114 1116 1122 1128\n",
      " 1143 1155 1157 1164 1166 1175 1176 1178 1197 1198 1200 1203 1204 1219\n",
      " 1225 1230 1231 1233 1235 1236 1238 1242 1250 1259 1261 1271 1274 1276\n",
      " 1278 1286 1289 1293 1297 1303 1304 1309 1311 1338 1340 1341 1344 1345\n",
      " 1347 1348 1349 1352 1355 1358 1362 1370 1384 1390 1392 1396 1402 1403\n",
      " 1407 1408 1409 1410 1422 1425 1430 1436 1444 1451 1452 1453 1468 1472\n",
      " 1476 1480 1482 1483 1493 1494 1504 1505 1513 1516 1518 1528 1529 1535\n",
      " 1541 1549 1555 1556 1562 1563 1569 1572 1574 1578 1581 1583 1589 1593\n",
      " 1594 1615 1622 1625 1627 1636 1641 1644 1645 1649 1652 1657 1658 1663\n",
      " 1665 1667 1669 1672 1673 1683 1688 1691 1700 1706 1707 1714 1716 1719\n",
      " 1722 1728 1740 1745 1751 1752 1753 1754 1769 1770 1771 1775 1781 1785\n",
      " 1793 1794 1799 1804 1817 1822 1827 1831 1842 1846 1864 1867 1868 1878\n",
      " 1884 1890 1895 1896 1916 1918 1922 1926 1927 1928 1929 1934 1945 1964\n",
      " 1966 1972 1975 1978 1981 1985 1996 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3388 - acc: 0.7876\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3157 - acc: 0.8575\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3081 - acc: 0.8613\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3062 - acc: 0.8622\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3021 - acc: 0.8635\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3031 - acc: 0.8626\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2987 - acc: 0.8654\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2959 - acc: 0.8668\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2925 - acc: 0.8681\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2903 - acc: 0.8692\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2897 - acc: 0.8701\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2893 - acc: 0.8697\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2862 - acc: 0.8709\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2865 - acc: 0.8711\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2850 - acc: 0.8718\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2806 - acc: 0.8743\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2777 - acc: 0.8756\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2775 - acc: 0.8759\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2739 - acc: 0.8781\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2717 - acc: 0.8789\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2647 - acc: 0.8836\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2577 - acc: 0.8870\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2549 - acc: 0.8885\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2519 - acc: 0.8904\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2493 - acc: 0.8918\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2465 - acc: 0.8932\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2447 - acc: 0.8940\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2417 - acc: 0.8955\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2385 - acc: 0.8973\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2365 - acc: 0.8978\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2345 - acc: 0.8988\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2314 - acc: 0.9002\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2292 - acc: 0.9016\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2260 - acc: 0.9029\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2244 - acc: 0.9040\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2202 - acc: 0.9055\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2187 - acc: 0.9062\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2172 - acc: 0.9068\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2151 - acc: 0.9077\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2117 - acc: 0.9091\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2094 - acc: 0.9104\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2076 - acc: 0.9112\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2054 - acc: 0.9120\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2035 - acc: 0.9130\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2021 - acc: 0.9136\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1995 - acc: 0.9150\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1971 - acc: 0.9157\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1955 - acc: 0.9167\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1929 - acc: 0.9179\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1918 - acc: 0.9181\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1898 - acc: 0.9192\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1887 - acc: 0.9198\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1878 - acc: 0.9200\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1845 - acc: 0.9215\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1832 - acc: 0.9221\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1814 - acc: 0.9230\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1799 - acc: 0.9237\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1784 - acc: 0.9246\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1777 - acc: 0.9245\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1758 - acc: 0.9254\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1742 - acc: 0.9261\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1730 - acc: 0.9268\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1739 - acc: 0.9261\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1732 - acc: 0.9265\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1692 - acc: 0.9283\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1678 - acc: 0.9289\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1672 - acc: 0.9290\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1657 - acc: 0.9298\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1651 - acc: 0.9301\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1632 - acc: 0.9309\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1634 - acc: 0.9309\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1607 - acc: 0.9323\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1601 - acc: 0.9327\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1594 - acc: 0.9325\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1582 - acc: 0.9333\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1571 - acc: 0.9338\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1563 - acc: 0.9341\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1579 - acc: 0.9333\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1557 - acc: 0.9347\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1540 - acc: 0.9353\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1525 - acc: 0.9359\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1513 - acc: 0.9366\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1509 - acc: 0.9366\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1512 - acc: 0.9363\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1499 - acc: 0.9371\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1488 - acc: 0.9376\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1481 - acc: 0.9380\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1472 - acc: 0.9381\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1462 - acc: 0.9387\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1456 - acc: 0.93894s - loss:\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1451 - acc: 0.9392\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1442 - acc: 0.9398\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1429 - acc: 0.9399\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1426 - acc: 0.9404\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1417 - acc: 0.9406\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1413 - acc: 0.9408\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1408 - acc: 0.9412\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.1391 - acc: 0.9418\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.1397 - acc: 0.9417\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.1394 - acc: 0.9418\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.72     40955\n",
      "           1       0.68      0.64      0.66     22837\n",
      "           2       0.78      0.75      0.76     36164\n",
      "\n",
      "    accuracy                           0.72     99956\n",
      "   macro avg       0.72      0.71      0.72     99956\n",
      "weighted avg       0.72      0.72      0.72     99956\n",
      "\n",
      "Acurácia\n",
      "0.7115171083927231\n",
      "Precisao\n",
      "0.723667854300988\n",
      "Recall\n",
      "0.7222577934291088\n",
      "F1\n",
      "0.7222725033833455\n",
      "[[30496  4779  5680]\n",
      " [ 6367 14681  1789]\n",
      " [ 7162  1985 27017]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_16 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 617,403\n",
      "Trainable params: 617,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.7034039533945324, 0.7096051051888246, 0.7109929760742442, 0.7145767650989309, 0.7115171083927231]\n",
      "0.710019181629851\n",
      "Precision total\n",
      "[0.7197155139788294, 0.7209530564007298, 0.7236323439927882, 0.7293643417221605, 0.723667854300988]\n",
      "0.7234666220790992\n",
      "Recalls total\n",
      "[0.7194756107630065, 0.7220981709448177, 0.7245264294679081, 0.72939383744632, 0.7222577934291088]\n",
      "0.7235503684102322\n",
      "F1 total\n",
      "[0.7185074785763638, 0.7206524189505312, 0.7236837474882478, 0.7284291972852662, 0.7222725033833455]\n",
      "0.7227090691367509\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
