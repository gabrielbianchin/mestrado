{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 15:28:24.324437 12668 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 15:28:24.328397 12668 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 15:28:24.329396 12668 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 15:28:24.330393 12668 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   1    2    3 ... 1997 1998 1999] TEST: [   0    6    7   17   19   28   30   33   36   38   40   42   43   45\n",
      "   48   51   53   71   76   88   90   96   99  103  114  119  120  134\n",
      "  140  141  142  147  149  152  153  163  164  165  181  185  190  196\n",
      "  198  199  203  205  214  216  219  227  230  231  234  237  239  254\n",
      "  260  265  268  269  270  276  289  292  294  296  305  315  327  332\n",
      "  339  346  353  355  381  383  390  393  412  419  421  422  423  426\n",
      "  427  432  433  435  440  441  443  455  465  466  474  477  488  489\n",
      "  502  515  517  519  521  524  527  536  537  542  545  552  556  557\n",
      "  567  569  579  581  590  591  596  615  624  625  626  629  634  639\n",
      "  644  647  658  665  670  671  691  698  703  708  711  722  727  730\n",
      "  732  736  740  749  750  754  757  765  768  770  773  787  807  808\n",
      "  819  822  831  840  842  844  848  852  857  865  866  872  874  877\n",
      "  879  880  887  889  896  898  900  906  917  919  924  926  929  936\n",
      "  940  948  951  953  957  968  971  995  996 1003 1005 1006 1007 1008\n",
      " 1019 1027 1028 1034 1037 1047 1048 1052 1056 1063 1067 1068 1069 1071\n",
      " 1072 1073 1077 1091 1098 1102 1112 1118 1131 1135 1136 1138 1144 1146\n",
      " 1147 1153 1165 1171 1176 1182 1183 1189 1190 1194 1195 1196 1199 1215\n",
      " 1219 1226 1227 1228 1230 1231 1235 1236 1238 1243 1248 1250 1252 1259\n",
      " 1262 1264 1265 1270 1271 1276 1281 1286 1290 1296 1297 1301 1302 1308\n",
      " 1312 1319 1323 1326 1336 1358 1359 1368 1370 1384 1390 1391 1417 1419\n",
      " 1422 1424 1427 1436 1445 1446 1447 1450 1453 1454 1459 1460 1461 1469\n",
      " 1496 1498 1502 1511 1514 1515 1519 1521 1551 1553 1557 1564 1574 1578\n",
      " 1579 1582 1585 1586 1590 1591 1592 1607 1610 1612 1615 1622 1639 1651\n",
      " 1654 1666 1672 1674 1676 1680 1681 1692 1697 1699 1702 1703 1704 1705\n",
      " 1706 1713 1715 1719 1729 1731 1735 1739 1741 1747 1752 1753 1760 1767\n",
      " 1771 1773 1778 1779 1785 1788 1794 1795 1799 1802 1811 1815 1816 1819\n",
      " 1826 1827 1840 1841 1844 1845 1847 1849 1852 1856 1861 1871 1875 1876\n",
      " 1883 1885 1889 1892 1896 1904 1908 1921 1925 1932 1945 1952 1955 1961\n",
      " 1962 1967 1971 1981 1982 1985 1992 1993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 15:28:25.818441 12668 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.6166 - acc: 0.1439\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5678 - acc: 0.1596\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5356 - acc: 0.1733\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.5220 - acc: 0.1790\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.5134 - acc: 0.1829\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.5078 - acc: 0.1854\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.5053 - acc: 0.1865\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4982 - acc: 0.1893\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4942 - acc: 0.1906\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4929 - acc: 0.1915\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4849 - acc: 0.1944\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4853 - acc: 0.1946\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4805 - acc: 0.1963\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4746 - acc: 0.1988\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4694 - acc: 0.2005\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4654 - acc: 0.2024\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4620 - acc: 0.2037\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4565 - acc: 0.2059\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4512 - acc: 0.2073\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4448 - acc: 0.2096\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4399 - acc: 0.2113\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4362 - acc: 0.2127\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4272 - acc: 0.2160\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4217 - acc: 0.2177\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.4172 - acc: 0.2189\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.4084 - acc: 0.2220\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4021 - acc: 0.2241\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3983 - acc: 0.2254\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.3928 - acc: 0.2270\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3872 - acc: 0.2289\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3811 - acc: 0.2307\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.3755 - acc: 0.2326\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.3698 - acc: 0.2347\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3668 - acc: 0.2354\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3633 - acc: 0.2367\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3554 - acc: 0.2391\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3512 - acc: 0.2404\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3449 - acc: 0.2428\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.3421 - acc: 0.2433\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3387 - acc: 0.2446\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3352 - acc: 0.2458\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3365 - acc: 0.2453\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3306 - acc: 0.2470\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3244 - acc: 0.2494\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3194 - acc: 0.2508\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3155 - acc: 0.2521\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3126 - acc: 0.2530\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3119 - acc: 0.2532\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3083 - acc: 0.2547\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3052 - acc: 0.2554\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3034 - acc: 0.2563\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3002 - acc: 0.2571\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2971 - acc: 0.2584\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2944 - acc: 0.2592\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2920 - acc: 0.2599\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2906 - acc: 0.2604\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2883 - acc: 0.2611\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2843 - acc: 0.2624\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2827 - acc: 0.2630\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2801 - acc: 0.2639\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2783 - acc: 0.2648\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2773 - acc: 0.2649\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2764 - acc: 0.2654\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2746 - acc: 0.2660\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2712 - acc: 0.2673\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2687 - acc: 0.2678\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2675 - acc: 0.2685\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2649 - acc: 0.2689\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2629 - acc: 0.2698\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2624 - acc: 0.2701\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2605 - acc: 0.2708\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2582 - acc: 0.2715\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2585 - acc: 0.2715\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2567 - acc: 0.2721\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2539 - acc: 0.2732\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2527 - acc: 0.2737\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2515 - acc: 0.2740\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2493 - acc: 0.2748\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2494 - acc: 0.2745\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2473 - acc: 0.2756\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2474 - acc: 0.2754\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2454 - acc: 0.2761\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2433 - acc: 0.2769\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2430 - acc: 0.2769\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2422 - acc: 0.2773\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2412 - acc: 0.2776\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2398 - acc: 0.2781\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2387 - acc: 0.2784\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2376 - acc: 0.2790\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2362 - acc: 0.2796\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2362 - acc: 0.2793\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2355 - acc: 0.2799\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2344 - acc: 0.2800\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2321 - acc: 0.2809\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2298 - acc: 0.2818\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2297 - acc: 0.2818\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2300 - acc: 0.2818\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2290 - acc: 0.2822\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2264 - acc: 0.2832\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2254 - acc: 0.2834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.02      0.04      1307\n",
      "           1       0.59      0.64      0.62     21207\n",
      "           2       0.31      0.24      0.27      3757\n",
      "           3       0.71      0.73      0.72     33435\n",
      "           4       0.47      0.21      0.29       734\n",
      "           5       0.44      0.52      0.48     20359\n",
      "           6       0.36      0.15      0.21      9021\n",
      "           7       0.38      0.42      0.40     11415\n",
      "\n",
      "    accuracy                           0.55    101235\n",
      "   macro avg       0.48      0.37      0.38    101235\n",
      "weighted avg       0.54      0.55      0.54    101235\n",
      "\n",
      "Acurácia\n",
      "0.36649565007323864\n",
      "Precisao\n",
      "0.5437842848219864\n",
      "Recall\n",
      "0.552299106040401\n",
      "F1\n",
      "0.5393091260663508\n",
      "[[   28   278    29   181     0   565    51   175]\n",
      " [    0 13646   282  2661     6  3281   321  1010]\n",
      " [    0   459   883   849     1   797   126   642]\n",
      " [    0  2885   479 24532   122  2984   335  2098]\n",
      " [    0    70    16   346   155    76    13    58]\n",
      " [   15  3381   459  2647    15 10507  1003  2332]\n",
      " [    3  1301   292  1176     4  3270  1348  1627]\n",
      " [    2  1089   429  2140    25  2338   579  4813]]\n",
      "TRAIN: [   0    2    3 ... 1997 1998 1999] TEST: [   1    5   12   22   32   34   46   47   60   62   67   68   72   77\n",
      "   79   80   91   92  107  108  109  112  118  126  130  133  136  144\n",
      "  158  159  161  166  167  169  175  179  180  184  188  194  200  206\n",
      "  210  211  218  221  229  232  238  245  250  253  255  256  257  267\n",
      "  272  277  278  279  284  288  306  311  314  319  333  335  336  344\n",
      "  345  347  351  352  354  361  364  367  368  372  375  387  394  395\n",
      "  396  399  400  404  406  408  409  416  429  431  438  444  445  448\n",
      "  452  461  462  468  469  473  475  481  483  490  491  496  505  510\n",
      "  511  512  520  525  530  531  538  540  548  549  551  554  555  560\n",
      "  562  564  577  585  587  589  594  595  599  600  601  605  616  619\n",
      "  620  621  628  630  632  635  636  637  638  649  651  659  662  668\n",
      "  675  677  678  682  684  687  689  694  695  705  710  714  719  721\n",
      "  737  751  776  777  782  784  786  789  794  796  809  810  812  814\n",
      "  816  829  830  834  838  843  850  854  856  860  875  886  888  892\n",
      "  893  894  899  909  912  918  922  928  932  934  946  954  956  964\n",
      "  965  969  970  973  975  977  982  984  987  990 1016 1020 1021 1023\n",
      " 1026 1031 1035 1044 1051 1055 1065 1074 1089 1093 1101 1113 1115 1116\n",
      " 1117 1121 1126 1130 1139 1141 1143 1145 1157 1158 1177 1179 1180 1191\n",
      " 1192 1206 1208 1213 1214 1218 1234 1237 1241 1242 1244 1246 1249 1251\n",
      " 1266 1278 1285 1288 1292 1293 1305 1307 1310 1313 1314 1322 1324 1325\n",
      " 1329 1339 1340 1344 1347 1360 1363 1374 1379 1380 1387 1401 1404 1406\n",
      " 1413 1416 1425 1428 1448 1451 1457 1473 1485 1486 1489 1493 1494 1495\n",
      " 1499 1501 1507 1517 1522 1531 1535 1541 1544 1546 1550 1554 1556 1562\n",
      " 1566 1570 1571 1572 1581 1587 1589 1594 1606 1617 1618 1620 1621 1624\n",
      " 1641 1646 1649 1657 1658 1665 1668 1673 1682 1684 1686 1687 1689 1691\n",
      " 1693 1696 1701 1709 1712 1718 1733 1744 1750 1751 1755 1762 1770 1787\n",
      " 1789 1792 1793 1820 1829 1839 1842 1848 1854 1866 1884 1891 1893 1894\n",
      " 1899 1900 1902 1903 1905 1906 1907 1911 1928 1931 1942 1946 1947 1951\n",
      " 1953 1954 1963 1968 1972 1978 1980 1984]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.6113 - acc: 0.1447\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5664 - acc: 0.1565\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5394 - acc: 0.1677\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5194 - acc: 0.1759\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5107 - acc: 0.1801\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5089 - acc: 0.1811\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5028 - acc: 0.1839\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4979 - acc: 0.1857\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4950 - acc: 0.1870\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4884 - acc: 0.1900\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4845 - acc: 0.1914\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4822 - acc: 0.1924\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4791 - acc: 0.1937\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4785 - acc: 0.1937\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4722 - acc: 0.1965\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4677 - acc: 0.1980\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4630 - acc: 0.1997\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4577 - acc: 0.2014\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4523 - acc: 0.2036\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4473 - acc: 0.2055\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4409 - acc: 0.2077\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4392 - acc: 0.2082\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4326 - acc: 0.2106\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4243 - acc: 0.2131\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4171 - acc: 0.2162\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4107 - acc: 0.2182\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4081 - acc: 0.2189\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4031 - acc: 0.2207\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3940 - acc: 0.2234\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3888 - acc: 0.2258\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3851 - acc: 0.2264\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3785 - acc: 0.2288\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3726 - acc: 0.2312\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3677 - acc: 0.2322\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3630 - acc: 0.2337\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3595 - acc: 0.2346\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3560 - acc: 0.2357\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3519 - acc: 0.2369\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3485 - acc: 0.2382\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3435 - acc: 0.2397\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3394 - acc: 0.2411\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3333 - acc: 0.2431\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3296 - acc: 0.2443\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3261 - acc: 0.2454\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3217 - acc: 0.2467\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3191 - acc: 0.2473\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3173 - acc: 0.2479\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3138 - acc: 0.2494\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3125 - acc: 0.2496\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3097 - acc: 0.2506\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3053 - acc: 0.2521\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3025 - acc: 0.2532\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3040 - acc: 0.2523\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3020 - acc: 0.2531\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2975 - acc: 0.2546\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2924 - acc: 0.2566\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2900 - acc: 0.2571\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2880 - acc: 0.2581\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2852 - acc: 0.2587\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2828 - acc: 0.2596\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2811 - acc: 0.2601\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2793 - acc: 0.2608\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2792 - acc: 0.2608\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2750 - acc: 0.2621\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2720 - acc: 0.2633\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2727 - acc: 0.2632\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2702 - acc: 0.2640\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2683 - acc: 0.2647\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2674 - acc: 0.2648\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2641 - acc: 0.2658\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2631 - acc: 0.2666\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2625 - acc: 0.2666\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2617 - acc: 0.2669\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2602 - acc: 0.2672\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2568 - acc: 0.2685\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2550 - acc: 0.2693\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.2521 - acc: 0.2705\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2522 - acc: 0.2702\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2507 - acc: 0.2708\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2501 - acc: 0.2709\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2468 - acc: 0.2722\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2460 - acc: 0.2725\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2458 - acc: 0.2726\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2447 - acc: 0.2730\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2440 - acc: 0.2734\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2438 - acc: 0.2735\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2413 - acc: 0.2741\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2394 - acc: 0.2747\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2376 - acc: 0.2758\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2379 - acc: 0.2753\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2371 - acc: 0.2757\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2382 - acc: 0.2752\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2349 - acc: 0.2766\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2340 - acc: 0.2769\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2328 - acc: 0.2774\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2318 - acc: 0.2776\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2302 - acc: 0.2784\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2293 - acc: 0.2786\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2300 - acc: 0.2785\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2293 - acc: 0.2787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.03      0.05      1317\n",
      "           1       0.62      0.68      0.65     22204\n",
      "           2       0.39      0.30      0.34      4162\n",
      "           3       0.70      0.76      0.73     34171\n",
      "           4       0.38      0.31      0.34       617\n",
      "           5       0.45      0.51      0.48     21236\n",
      "           6       0.35      0.17      0.23      9324\n",
      "           7       0.40      0.39      0.40     12045\n",
      "\n",
      "    accuracy                           0.57    105076\n",
      "   macro avg       0.50      0.39      0.40    105076\n",
      "weighted avg       0.55      0.57      0.55    105076\n",
      "\n",
      "Acurácia\n",
      "0.39171004413797483\n",
      "Precisao\n",
      "0.5531843644575419\n",
      "Recall\n",
      "0.5653241463321786\n",
      "F1\n",
      "0.551304812550308\n",
      "[[   33   238    30   219     0   575    81   141]\n",
      " [    3 14996   218  2705    47  2978   356   901]\n",
      " [    3   428  1257   905    11   841   159   558]\n",
      " [    0  2725   534 25826   174  2711   392  1809]\n",
      " [    0    55    10   234   189    59    13    57]\n",
      " [    4  3350   504  3235    23 10827  1235  2058]\n",
      " [    2  1286   242  1339    17  3403  1585  1450]\n",
      " [    1  1204   445  2454    38  2473   741  4689]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   8    9   10   14   18   23   24   26   31   44   49   52   54   55\n",
      "   56   58   61   64   70   73   83   85   86   87   89   97   98  100\n",
      "  105  110  115  116  117  121  122  123  128  131  138  139  145  146\n",
      "  150  151  156  162  174  183  186  189  193  197  202  204  208  224\n",
      "  233  236  240  242  243  247  249  261  263  266  273  280  281  286\n",
      "  297  307  309  310  312  316  320  323  326  334  337  342  348  350\n",
      "  358  359  401  403  413  414  415  417  418  425  446  447  450  454\n",
      "  459  460  463  470  478  479  482  497  501  503  506  507  509  523\n",
      "  526  533  544  546  547  561  568  580  592  598  602  604  609  612\n",
      "  617  618  643  646  650  654  655  661  669  674  680  681  685  686\n",
      "  696  699  704  709  713  725  731  743  744  746  753  755  759  760\n",
      "  767  772  780  785  791  792  795  798  800  801  802  804  811  815\n",
      "  821  832  841  846  847  849  853  858  863  868  871  876  881  883\n",
      "  884  885  890  897  902  903  904  914  915  916  923  927  930  937\n",
      "  955  958  960  966  967  974  976  980  981  983  994  999 1017 1018\n",
      " 1025 1040 1042 1050 1054 1057 1058 1060 1061 1066 1075 1083 1085 1087\n",
      " 1092 1106 1111 1124 1129 1134 1140 1149 1154 1159 1160 1164 1170 1175\n",
      " 1188 1193 1200 1201 1205 1209 1210 1212 1217 1220 1221 1224 1232 1239\n",
      " 1240 1247 1257 1258 1272 1275 1279 1282 1304 1306 1309 1321 1328 1334\n",
      " 1338 1345 1352 1357 1364 1375 1378 1385 1397 1399 1400 1405 1409 1412\n",
      " 1414 1420 1421 1423 1429 1430 1431 1432 1438 1441 1449 1452 1458 1463\n",
      " 1466 1468 1476 1477 1479 1492 1500 1503 1513 1520 1526 1528 1533 1534\n",
      " 1537 1543 1545 1558 1573 1575 1584 1597 1600 1601 1605 1609 1616 1623\n",
      " 1632 1633 1638 1652 1655 1661 1667 1677 1683 1690 1694 1700 1708 1710\n",
      " 1711 1717 1721 1723 1724 1725 1726 1730 1736 1737 1742 1749 1758 1764\n",
      " 1768 1774 1775 1776 1777 1782 1784 1790 1804 1805 1812 1814 1825 1828\n",
      " 1830 1832 1833 1835 1836 1855 1858 1864 1868 1870 1873 1881 1887 1888\n",
      " 1895 1898 1901 1915 1917 1920 1924 1926 1927 1933 1936 1937 1939 1941\n",
      " 1944 1950 1965 1975 1977 1979 1983 1988]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.6150 - acc: 0.1427\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5629 - acc: 0.1609\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5367 - acc: 0.1717\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5189 - acc: 0.1795\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5136 - acc: 0.1817\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5102 - acc: 0.1837\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5033 - acc: 0.1864\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4975 - acc: 0.1887\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4932 - acc: 0.1908\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4927 - acc: 0.1910\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4870 - acc: 0.1931\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4823 - acc: 0.1951\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4787 - acc: 0.1964\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4753 - acc: 0.1976\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4700 - acc: 0.1998\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4648 - acc: 0.2017\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4626 - acc: 0.2025\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4549 - acc: 0.2055\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4475 - acc: 0.2086\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.4458 - acc: 0.2084\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4367 - acc: 0.2118\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4303 - acc: 0.2143\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.4247 - acc: 0.2162\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4159 - acc: 0.2187\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4125 - acc: 0.2202\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4079 - acc: 0.2215\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4016 - acc: 0.2237\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3971 - acc: 0.2255\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3886 - acc: 0.2281\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3808 - acc: 0.2303\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3763 - acc: 0.2319\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3733 - acc: 0.2329\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3668 - acc: 0.2348\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3629 - acc: 0.2360\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3584 - acc: 0.2375\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3543 - acc: 0.2389\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3486 - acc: 0.2409\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3466 - acc: 0.2414\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3407 - acc: 0.2432\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3365 - acc: 0.2443\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3318 - acc: 0.2462\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3303 - acc: 0.2465\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3287 - acc: 0.2469\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3238 - acc: 0.2487\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3184 - acc: 0.2505\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3159 - acc: 0.2510\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3135 - acc: 0.2516\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3094 - acc: 0.2533\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3088 - acc: 0.2535\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3049 - acc: 0.2548\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3032 - acc: 0.2552\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2994 - acc: 0.2566\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2969 - acc: 0.2577\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2946 - acc: 0.2581\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2927 - acc: 0.2586\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2884 - acc: 0.2602\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2864 - acc: 0.2609\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2897 - acc: 0.2598\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2844 - acc: 0.2618\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2851 - acc: 0.2613\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2800 - acc: 0.2631\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2773 - acc: 0.2640\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2751 - acc: 0.2647\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2740 - acc: 0.2653\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2709 - acc: 0.2664\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2688 - acc: 0.2670\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2676 - acc: 0.2672\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2665 - acc: 0.2677\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2642 - acc: 0.2689\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2633 - acc: 0.2688\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2616 - acc: 0.2694\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2595 - acc: 0.2704\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2593 - acc: 0.2704\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2570 - acc: 0.2710\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2562 - acc: 0.2715\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2532 - acc: 0.2725\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2518 - acc: 0.2730\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2517 - acc: 0.2728\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2501 - acc: 0.2736\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2482 - acc: 0.2745\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2481 - acc: 0.2744\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2470 - acc: 0.2746\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2453 - acc: 0.2752\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2429 - acc: 0.2760\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2412 - acc: 0.2764\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2415 - acc: 0.2766\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2405 - acc: 0.2772\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2382 - acc: 0.2776\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2396 - acc: 0.2775\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2373 - acc: 0.2783\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2352 - acc: 0.2788\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2353 - acc: 0.2791\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2344 - acc: 0.2793\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2328 - acc: 0.2799\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.2328 - acc: 0.2797\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2321 - acc: 0.2799\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2315 - acc: 0.2804\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2283 - acc: 0.2815\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2277 - acc: 0.2819\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2276 - acc: 0.2818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.03      0.06      1229\n",
      "           1       0.63      0.64      0.63     21240\n",
      "           2       0.33      0.31      0.32      4047\n",
      "           3       0.70      0.78      0.74     33457\n",
      "           4       0.56      0.23      0.33       732\n",
      "           5       0.46      0.52      0.49     20612\n",
      "           6       0.38      0.17      0.24      9021\n",
      "           7       0.42      0.40      0.41     11732\n",
      "\n",
      "    accuracy                           0.57    102070\n",
      "   macro avg       0.51      0.39      0.40    102070\n",
      "weighted avg       0.56      0.57      0.55    102070\n",
      "\n",
      "Acurácia\n",
      "0.3855978907456443\n",
      "Precisao\n",
      "0.5572278961823396\n",
      "Recall\n",
      "0.5696580777897521\n",
      "F1\n",
      "0.5549867792320409\n",
      "[[   41   241    47   168     2   504    83   143]\n",
      " [    4 13571   303  2999     8  3219   315   821]\n",
      " [    1   363  1255   896     4   895   125   508]\n",
      " [    0  2250   616 26200    83  2439   290  1579]\n",
      " [    0    51    10   364   170    88     8    41]\n",
      " [   10  2929   646  3171    11 10721  1090  2034]\n",
      " [    5  1153   306  1383     6  3284  1544  1340]\n",
      " [    5  1011   592  2487    18  2351   625  4643]]\n",
      "TRAIN: [   0    1    2 ... 1996 1998 1999] TEST: [   3    4   13   15   20   35   39   41   50   59   63   66   74   75\n",
      "   94   95  101  104  106  124  125  129  135  148  155  157  160  168\n",
      "  171  177  182  191  192  195  207  212  220  241  246  252  264  271\n",
      "  274  282  283  287  290  293  295  298  300  302  303  304  308  313\n",
      "  321  324  325  329  331  340  343  349  360  365  369  371  374  376\n",
      "  377  379  380  384  386  389  391  392  402  410  424  430  434  439\n",
      "  451  453  456  467  471  472  476  487  495  499  500  508  513  516\n",
      "  518  522  528  532  535  550  553  558  565  566  570  573  574  578\n",
      "  582  583  586  593  607  608  610  611  627  631  642  653  663  664\n",
      "  672  673  676  693  697  700  701  702  706  707  716  720  726  734\n",
      "  738  742  745  756  758  761  762  766  769  771  775  788  817  824\n",
      "  826  833  835  837  845  855  861  864  870  873  878  891  895  901\n",
      "  905  907  910  913  921  925  933  939  942  944  949  950  961  963\n",
      "  979  988  989  998 1000 1002 1011 1013 1014 1015 1024 1030 1032 1036\n",
      " 1039 1043 1045 1049 1062 1064 1070 1076 1078 1080 1081 1084 1086 1088\n",
      " 1090 1099 1100 1103 1108 1119 1122 1123 1127 1133 1137 1150 1151 1152\n",
      " 1155 1161 1162 1163 1166 1167 1168 1169 1173 1174 1185 1197 1198 1203\n",
      " 1207 1211 1222 1223 1225 1229 1245 1254 1255 1261 1273 1274 1280 1287\n",
      " 1289 1291 1294 1299 1300 1303 1315 1317 1320 1330 1333 1335 1342 1350\n",
      " 1351 1354 1365 1366 1367 1369 1371 1376 1386 1392 1403 1407 1408 1410\n",
      " 1411 1415 1418 1426 1433 1435 1442 1443 1444 1455 1456 1465 1467 1471\n",
      " 1474 1475 1478 1480 1481 1482 1483 1484 1487 1488 1491 1497 1504 1508\n",
      " 1525 1529 1532 1540 1549 1552 1561 1565 1567 1568 1576 1577 1593 1602\n",
      " 1603 1614 1626 1627 1631 1634 1640 1642 1650 1653 1656 1660 1662 1663\n",
      " 1670 1671 1679 1688 1695 1716 1722 1727 1728 1743 1748 1761 1765 1769\n",
      " 1772 1780 1781 1796 1801 1803 1817 1821 1824 1843 1850 1851 1857 1859\n",
      " 1863 1865 1867 1874 1877 1878 1879 1880 1882 1897 1913 1914 1919 1922\n",
      " 1929 1930 1934 1940 1948 1949 1956 1957 1958 1960 1964 1966 1969 1970\n",
      " 1973 1974 1976 1986 1990 1994 1995 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.6134 - acc: 0.1466\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5649 - acc: 0.1606\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5363 - acc: 0.1728\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5227 - acc: 0.1779\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5154 - acc: 0.1809\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5105 - acc: 0.1834\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5029 - acc: 0.1868\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5010 - acc: 0.1875\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4952 - acc: 0.1902\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4943 - acc: 0.1902\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4869 - acc: 0.1933\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4830 - acc: 0.1951\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4818 - acc: 0.1952\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4782 - acc: 0.1968\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4719 - acc: 0.1991\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4669 - acc: 0.2012\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4623 - acc: 0.2029\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4585 - acc: 0.2043\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4524 - acc: 0.2063\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4490 - acc: 0.2075\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4441 - acc: 0.2096\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4364 - acc: 0.2124\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4312 - acc: 0.2135\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4263 - acc: 0.2155\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4174 - acc: 0.2186\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4144 - acc: 0.2196\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4082 - acc: 0.2221\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4030 - acc: 0.2235\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4016 - acc: 0.2234\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3900 - acc: 0.2277\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3845 - acc: 0.2293\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3789 - acc: 0.2318\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3731 - acc: 0.2332\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3707 - acc: 0.2339\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3678 - acc: 0.2346\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3620 - acc: 0.2362\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3560 - acc: 0.2379\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3506 - acc: 0.2401\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3480 - acc: 0.2405\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3449 - acc: 0.2417\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3412 - acc: 0.2425\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3351 - acc: 0.2450\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3319 - acc: 0.2459\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3283 - acc: 0.2471\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3233 - acc: 0.2485\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3202 - acc: 0.2496\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3222 - acc: 0.2489\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3152 - acc: 0.2510\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3125 - acc: 0.2522\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3101 - acc: 0.2529\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3060 - acc: 0.2542\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3033 - acc: 0.2553\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3006 - acc: 0.2562\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2982 - acc: 0.2569\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2946 - acc: 0.2581\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2940 - acc: 0.2583\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2908 - acc: 0.2595\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2922 - acc: 0.2589\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2889 - acc: 0.2600\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2871 - acc: 0.2606\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2834 - acc: 0.2618\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2804 - acc: 0.2629\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2779 - acc: 0.2635\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2755 - acc: 0.2645\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2746 - acc: 0.2645\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2724 - acc: 0.2655\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2704 - acc: 0.2665\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2692 - acc: 0.2669\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2680 - acc: 0.2673\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2666 - acc: 0.2673\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2640 - acc: 0.2683\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2632 - acc: 0.2690\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2616 - acc: 0.2696\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.2594 - acc: 0.2700\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2577 - acc: 0.2707\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2574 - acc: 0.2710\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2540 - acc: 0.2723\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2531 - acc: 0.2724\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2524 - acc: 0.2728\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2498 - acc: 0.2732\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2500 - acc: 0.2734\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2484 - acc: 0.2741\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2478 - acc: 0.2745\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2452 - acc: 0.2752\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2446 - acc: 0.2757\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2422 - acc: 0.2763\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2433 - acc: 0.2757\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2420 - acc: 0.2761\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2405 - acc: 0.2770\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2402 - acc: 0.2771\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2381 - acc: 0.2777\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2363 - acc: 0.2785\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2356 - acc: 0.2787\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2336 - acc: 0.2794\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2328 - acc: 0.2800\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2327 - acc: 0.2797\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2323 - acc: 0.2798\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2305 - acc: 0.2806\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2297 - acc: 0.2808\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2289 - acc: 0.2810\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.04      0.08      1235\n",
      "           1       0.62      0.65      0.63     23020\n",
      "           2       0.38      0.28      0.32      3858\n",
      "           3       0.69      0.75      0.72     31796\n",
      "           4       0.49      0.26      0.34       722\n",
      "           5       0.46      0.51      0.48     20847\n",
      "           6       0.36      0.18      0.24      9093\n",
      "           7       0.40      0.42      0.41     11417\n",
      "\n",
      "    accuracy                           0.56    101988\n",
      "   macro avg       0.50      0.39      0.40    101988\n",
      "weighted avg       0.55      0.56      0.55    101988\n",
      "\n",
      "Acurácia\n",
      "0.38642805676794\n",
      "Precisao\n",
      "0.5496243295858757\n",
      "Recall\n",
      "0.5612523041926502\n",
      "F1\n",
      "0.5486184144175278\n",
      "[[   55   249    32   177     2   497    79   144]\n",
      " [    3 14879   226  3015    38  3359   413  1087]\n",
      " [    1   413  1067   924     4   753   136   560]\n",
      " [    0  2758   459 23963   119  2430   353  1714]\n",
      " [    0    88    10   315   187    66     7    49]\n",
      " [   16  3418   443  2831    11 10673  1200  2255]\n",
      " [   14  1258   213  1299    11  3170  1666  1462]\n",
      " [    5  1040   383  2255    13  2235   735  4751]]\n",
      "TRAIN: [   0    1    3 ... 1994 1995 1997] TEST: [   2   11   16   21   25   27   29   37   57   65   69   78   81   82\n",
      "   84   93  102  111  113  127  132  137  143  154  170  172  173  176\n",
      "  178  187  201  209  213  215  217  222  223  225  226  228  235  244\n",
      "  248  251  258  259  262  275  285  291  299  301  317  318  322  328\n",
      "  330  338  341  356  357  362  363  366  370  373  378  382  385  388\n",
      "  397  398  405  407  411  420  428  436  437  442  449  457  458  464\n",
      "  480  484  485  486  492  493  494  498  504  514  529  534  539  541\n",
      "  543  559  563  571  572  575  576  584  588  597  603  606  613  614\n",
      "  622  623  633  640  641  645  648  652  656  657  660  666  667  679\n",
      "  683  688  690  692  712  715  717  718  723  724  728  729  733  735\n",
      "  739  741  747  748  752  763  764  774  778  779  781  783  790  793\n",
      "  797  799  803  805  806  813  818  820  823  825  827  828  836  839\n",
      "  851  859  862  867  869  882  908  911  920  931  935  938  941  943\n",
      "  945  947  952  959  962  972  978  985  986  991  992  993  997 1001\n",
      " 1004 1009 1010 1012 1022 1029 1033 1038 1041 1046 1053 1059 1079 1082\n",
      " 1094 1095 1096 1097 1104 1105 1107 1109 1110 1114 1120 1125 1128 1132\n",
      " 1142 1148 1156 1172 1178 1181 1184 1186 1187 1202 1204 1216 1233 1253\n",
      " 1256 1260 1263 1267 1268 1269 1277 1283 1284 1295 1298 1311 1316 1318\n",
      " 1327 1331 1332 1337 1341 1343 1346 1348 1349 1353 1355 1356 1361 1362\n",
      " 1372 1373 1377 1381 1382 1383 1388 1389 1393 1394 1395 1396 1398 1402\n",
      " 1434 1437 1439 1440 1462 1464 1470 1472 1490 1505 1506 1509 1510 1512\n",
      " 1516 1518 1523 1524 1527 1530 1536 1538 1539 1542 1547 1548 1555 1559\n",
      " 1560 1563 1569 1580 1583 1588 1595 1596 1598 1599 1604 1608 1611 1613\n",
      " 1619 1625 1628 1629 1630 1635 1636 1637 1643 1644 1645 1647 1648 1659\n",
      " 1664 1669 1675 1678 1685 1698 1707 1714 1720 1732 1734 1738 1740 1745\n",
      " 1746 1754 1756 1757 1759 1763 1766 1783 1786 1791 1797 1798 1800 1806\n",
      " 1807 1808 1809 1810 1813 1818 1822 1823 1831 1834 1837 1838 1846 1853\n",
      " 1860 1862 1869 1872 1886 1890 1909 1910 1912 1916 1918 1923 1935 1938\n",
      " 1943 1959 1987 1989 1991 1996 1998 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.6145 - acc: 0.1395\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5661 - acc: 0.1569\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5392 - acc: 0.1680\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5214 - acc: 0.1748\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5132 - acc: 0.1783\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5073 - acc: 0.1810\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.5014 - acc: 0.1839\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4981 - acc: 0.1851\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4935 - acc: 0.1873\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4884 - acc: 0.1893\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4849 - acc: 0.1910\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4829 - acc: 0.1919\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4772 - acc: 0.1938\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4733 - acc: 0.1955\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4682 - acc: 0.1974\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4639 - acc: 0.1989\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4604 - acc: 0.2003\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4561 - acc: 0.2018\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4493 - acc: 0.2043\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4445 - acc: 0.2061\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4388 - acc: 0.2080\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4307 - acc: 0.2110\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4249 - acc: 0.2129\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4185 - acc: 0.2150\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4134 - acc: 0.2169\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.4072 - acc: 0.2188\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3993 - acc: 0.2215\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3943 - acc: 0.2231\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3947 - acc: 0.2228\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3822 - acc: 0.2272\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3774 - acc: 0.2283\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3717 - acc: 0.2303\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3662 - acc: 0.2324\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3625 - acc: 0.2334\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3567 - acc: 0.2354\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3551 - acc: 0.2358\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3500 - acc: 0.2374\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3471 - acc: 0.2385\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3390 - acc: 0.2411\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3366 - acc: 0.2417\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3332 - acc: 0.2426\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3285 - acc: 0.2443\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3262 - acc: 0.2451\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3210 - acc: 0.2465\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3192 - acc: 0.2474\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3161 - acc: 0.2482\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3128 - acc: 0.2493\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3077 - acc: 0.2509\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3087 - acc: 0.2504\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3049 - acc: 0.2520\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3018 - acc: 0.2526\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2982 - acc: 0.2541\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2969 - acc: 0.2543\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2937 - acc: 0.2555\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2915 - acc: 0.2563\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2891 - acc: 0.2570\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2863 - acc: 0.2579\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2846 - acc: 0.2584\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2818 - acc: 0.2593\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2817 - acc: 0.2596\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2790 - acc: 0.2605\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2754 - acc: 0.2617\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2739 - acc: 0.2618\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2716 - acc: 0.2628\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2702 - acc: 0.2632\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2708 - acc: 0.2630\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2686 - acc: 0.2643\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2651 - acc: 0.2656\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2621 - acc: 0.2662\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2613 - acc: 0.2666\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2597 - acc: 0.2671\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2594 - acc: 0.2673\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2575 - acc: 0.2680\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2555 - acc: 0.2685\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2527 - acc: 0.2695\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2514 - acc: 0.2697\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2520 - acc: 0.2700\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2504 - acc: 0.2702\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2490 - acc: 0.2708\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2475 - acc: 0.2715\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2464 - acc: 0.2717\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2454 - acc: 0.2720\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2447 - acc: 0.2725\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2428 - acc: 0.2730\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2408 - acc: 0.2739\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2398 - acc: 0.2741\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2387 - acc: 0.2746\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2374 - acc: 0.2750\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2366 - acc: 0.2756\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2338 - acc: 0.2760\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2343 - acc: 0.2761\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2335 - acc: 0.2763\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2336 - acc: 0.2764\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2319 - acc: 0.2771\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2322 - acc: 0.2769\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2309 - acc: 0.2772\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2294 - acc: 0.2781\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2280 - acc: 0.2784\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2267 - acc: 0.2789\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2254 - acc: 0.2792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.03      0.05      1176\n",
      "           1       0.64      0.64      0.64     21261\n",
      "           2       0.35      0.30      0.32      4047\n",
      "           3       0.71      0.78      0.75     36284\n",
      "           4       0.49      0.27      0.35       712\n",
      "           5       0.47      0.52      0.49     21031\n",
      "           6       0.40      0.15      0.22      9129\n",
      "           7       0.38      0.44      0.41     11799\n",
      "\n",
      "    accuracy                           0.58    105439\n",
      "   macro avg       0.49      0.39      0.41    105439\n",
      "weighted avg       0.57      0.58      0.56    105439\n",
      "\n",
      "Acurácia\n",
      "0.3923423276001009\n",
      "Precisao\n",
      "0.5678375875364139\n",
      "Recall\n",
      "0.57778431130796\n",
      "F1\n",
      "0.5641977475170515\n",
      "[[   34   204    38   201     1   470    39   189]\n",
      " [    3 13558   254  2880    24  3182   274  1086]\n",
      " [    0   402  1216   975     4   721    94   635]\n",
      " [    1  2142   538 28332   127  2747   295  2102]\n",
      " [    0    69    13   297   194    68     7    64]\n",
      " [   22  2879   605  3140     9 10939   884  2553]\n",
      " [    2  1004   297  1360     9  3241  1403  1813]\n",
      " [    5   968   511  2531    24  2043   472  5245]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_20 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 1,065,608\n",
      "Trainable params: 1,065,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.36649565007323864, 0.39171004413797483, 0.3855978907456443, 0.38642805676794, 0.3923423276001009]\n",
      "0.3845147938649798\n",
      "Precision total\n",
      "[0.5437842848219864, 0.5531843644575419, 0.5572278961823396, 0.5496243295858757, 0.5678375875364139]\n",
      "0.5543316925168315\n",
      "Recalls total\n",
      "[0.552299106040401, 0.5653241463321786, 0.5696580777897521, 0.5612523041926502, 0.57778431130796]\n",
      "0.5652635891325885\n",
      "F1 total\n",
      "[0.5393091260663508, 0.551304812550308, 0.5549867792320409, 0.5486184144175278, 0.5641977475170515]\n",
      "0.5516833759566558\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accuq8)\n",
    "a = np.array(accuq8)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "p = np.array(precisionsq8)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "r = np.array(recallsq8)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1q8)\n",
    "f = np.array(f1q8)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
