{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classesQ8 = base.iloc[:1400000, 20:28].values\n",
    "classesQ8 = np.reshape(classesQ8, (2000, 700, 8))\n",
    "print(classesQ8.shape)\n",
    "\n",
    "classesQ3 = base.iloc[:1400000, 28:31].values\n",
    "classesQ3 = np.reshape(classesQ3, (2000, 700, 3))\n",
    "print(classesQ3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNGRU, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede(saida):\n",
    "    model = Sequential()\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(saida, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test, saida):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], saida))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], saida))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "    \n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0822 00:47:47.091457 16316 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 00:47:47.096421 16316 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 00:47:47.097443 16316 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 00:47:47.098410 16316 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    2    5 ... 1994 1996 1999] TEST: [   1    3    4   10   48   49   53   54   60   67   72   77   78   82\n",
      "   83   87   95   98   99  108  113  117  125  131  132  138  144  146\n",
      "  156  164  165  179  186  202  203  205  207  211  213  216  224  231\n",
      "  238  244  245  248  256  265  266  269  279  280  283  296  305  307\n",
      "  314  319  325  328  333  334  338  342  344  353  362  368  379  384\n",
      "  386  389  396  398  400  402  409  425  429  438  457  461  469  470\n",
      "  472  478  480  486  494  497  507  509  513  522  535  538  539  546\n",
      "  547  551  553  564  569  570  577  578  580  582  584  606  612  613\n",
      "  616  641  642  650  665  669  673  678  680  682  688  702  705  710\n",
      "  714  716  723  724  735  743  751  752  753  760  764  766  768  770\n",
      "  772  773  775  778  780  783  784  786  787  795  798  820  822  824\n",
      "  825  831  832  835  846  853  855  867  868  873  877  888  890  898\n",
      "  906  908  909  915  916  918  928  929  933  939  955  961  963  968\n",
      "  969  970  972  978  986  989  992 1001 1004 1006 1013 1015 1019 1022\n",
      " 1027 1039 1046 1051 1054 1059 1064 1065 1066 1069 1070 1072 1077 1085\n",
      " 1089 1092 1102 1108 1111 1120 1125 1128 1138 1146 1148 1159 1160 1166\n",
      " 1168 1171 1175 1177 1188 1197 1201 1202 1203 1209 1217 1229 1230 1233\n",
      " 1248 1253 1268 1270 1279 1284 1288 1289 1294 1297 1298 1319 1322 1332\n",
      " 1335 1336 1339 1340 1343 1346 1347 1348 1352 1358 1370 1377 1381 1384\n",
      " 1388 1394 1395 1397 1409 1416 1424 1433 1436 1453 1454 1459 1470 1472\n",
      " 1480 1485 1486 1489 1491 1493 1496 1497 1509 1511 1522 1525 1527 1529\n",
      " 1532 1537 1539 1542 1545 1547 1549 1550 1564 1568 1570 1576 1577 1580\n",
      " 1581 1584 1589 1591 1592 1597 1599 1603 1606 1619 1620 1621 1630 1638\n",
      " 1641 1643 1648 1650 1653 1654 1659 1670 1682 1686 1692 1705 1713 1719\n",
      " 1722 1725 1727 1731 1732 1735 1736 1738 1742 1748 1751 1753 1758 1759\n",
      " 1773 1780 1785 1789 1791 1795 1796 1800 1804 1807 1809 1813 1815 1818\n",
      " 1825 1829 1830 1833 1835 1838 1843 1845 1863 1869 1877 1878 1879 1906\n",
      " 1911 1914 1917 1918 1925 1928 1933 1934 1938 1941 1944 1948 1951 1956\n",
      " 1958 1966 1973 1981 1989 1995 1997 1998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 00:47:49.245667 16316 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 29s 18ms/sample - loss: 0.5999 - acc: 0.1579\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5462 - acc: 0.1721\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5312 - acc: 0.1790\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5270 - acc: 0.1808\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5206 - acc: 0.1838\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5150 - acc: 0.1864\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5126 - acc: 0.1873\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5080 - acc: 0.1896\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5042 - acc: 0.1910\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5007 - acc: 0.1928\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5005 - acc: 0.1927\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4976 - acc: 0.1938\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4910 - acc: 0.1965\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4871 - acc: 0.1983\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4861 - acc: 0.1989\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4842 - acc: 0.1989\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4800 - acc: 0.2013\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4757 - acc: 0.2025\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4708 - acc: 0.2047\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4666 - acc: 0.2061\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4618 - acc: 0.2079\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4562 - acc: 0.2100\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4482 - acc: 0.2126\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4453 - acc: 0.2139\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4400 - acc: 0.2157\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4351 - acc: 0.2175\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4300 - acc: 0.2193\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4244 - acc: 0.2210\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4196 - acc: 0.2225\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4172 - acc: 0.2235\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4137 - acc: 0.2244\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4092 - acc: 0.2262\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4062 - acc: 0.2272\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4022 - acc: 0.2283\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3986 - acc: 0.2293\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3971 - acc: 0.2301\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3926 - acc: 0.2313\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3893 - acc: 0.2321\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3835 - acc: 0.2343\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3825 - acc: 0.2346\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3768 - acc: 0.2366\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3744 - acc: 0.2370\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3745 - acc: 0.2370\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3696 - acc: 0.2387\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3679 - acc: 0.2390\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3648 - acc: 0.2402\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3624 - acc: 0.2410\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3595 - acc: 0.2419\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3564 - acc: 0.2429\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3560 - acc: 0.2431\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3526 - acc: 0.2445\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3503 - acc: 0.2447\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3481 - acc: 0.2458\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3458 - acc: 0.2464\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3449 - acc: 0.2468\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3419 - acc: 0.2477\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3403 - acc: 0.2482\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3388 - acc: 0.2486\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3364 - acc: 0.2494\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3353 - acc: 0.2498\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3336 - acc: 0.2505\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3309 - acc: 0.2514\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3312 - acc: 0.2513\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3289 - acc: 0.2516\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3278 - acc: 0.2523\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3250 - acc: 0.2532\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3226 - acc: 0.2537\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3205 - acc: 0.2547\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3194 - acc: 0.2547\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3191 - acc: 0.2549\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3183 - acc: 0.2556\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3163 - acc: 0.2562\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3146 - acc: 0.2566\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3137 - acc: 0.2565\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3118 - acc: 0.2574\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3110 - acc: 0.2575\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3100 - acc: 0.2577\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3090 - acc: 0.2583\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3075 - acc: 0.2588\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3058 - acc: 0.2594\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3046 - acc: 0.2597\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3042 - acc: 0.2597\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3010 - acc: 0.2613\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3007 - acc: 0.2610\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2998 - acc: 0.2614\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2985 - acc: 0.2618\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2984 - acc: 0.2620\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2963 - acc: 0.2626\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2964 - acc: 0.2624\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2956 - acc: 0.2627\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2931 - acc: 0.2636\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2920 - acc: 0.2645\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2918 - acc: 0.2642\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2906 - acc: 0.2647\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2893 - acc: 0.2651\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2890 - acc: 0.2647\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2882 - acc: 0.2654\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2868 - acc: 0.2661\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2859 - acc: 0.2661\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2846 - acc: 0.2666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.00      0.01      1242\n",
      "           1       0.64      0.70      0.67     20825\n",
      "           2       0.36      0.28      0.32      3645\n",
      "           3       0.75      0.81      0.78     31549\n",
      "           4       0.51      0.32      0.40       647\n",
      "           5       0.48      0.54      0.51     19763\n",
      "           6       0.38      0.14      0.20      8811\n",
      "           7       0.41      0.44      0.42     11029\n",
      "\n",
      "    accuracy                           0.60     97511\n",
      "   macro avg       0.51      0.40      0.41     97511\n",
      "weighted avg       0.58      0.60      0.58     97511\n",
      "\n",
      "Acurácia\n",
      "0.40464018899985854\n",
      "Precisao\n",
      "0.5811668159892723\n",
      "Recall\n",
      "0.5964045082093302\n",
      "F1\n",
      "0.5787956721682876\n",
      "[[    5   270    26   147     1   600    58   135]\n",
      " [    1 14484   194  1861    14  2999   254  1018]\n",
      " [    0   403  1037   814     8   686    87   610]\n",
      " [    0  1778   519 25633   124  1916   196  1383]\n",
      " [    0    51    14   253   208    60     6    55]\n",
      " [    2  3259   439  2237    20 10746   872  2188]\n",
      " [    1  1226   215  1120    11  3366  1203  1669]\n",
      " [    0   995   406  2136    19  2117   516  4840]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   7   11   22   25   47   50   51   55   63   75   76   81   86   88\n",
      "   93  100  105  122  124  129  130  133  136  139  140  142  151  163\n",
      "  172  180  183  184  187  188  194  195  201  204  208  209  227  230\n",
      "  233  234  257  258  260  261  268  288  291  292  298  300  304  309\n",
      "  313  317  321  340  351  352  359  360  364  367  369  371  374  378\n",
      "  380  382  383  385  387  394  397  401  404  408  410  415  426  435\n",
      "  441  445  451  452  454  455  458  460  471  474  476  477  479  481\n",
      "  482  484  491  493  498  503  511  512  515  517  518  533  541  542\n",
      "  548  549  561  562  563  567  568  571  573  592  597  601  607  608\n",
      "  611  618  628  631  632  634  635  660  662  664  672  681  684  686\n",
      "  693  694  695  720  728  730  737  739  741  744  750  762  769  771\n",
      "  774  777  781  785  789  791  793  805  807  809  811  821  830  837\n",
      "  839  842  851  852  864  880  895  905  910  919  920  922  923  925\n",
      "  932  936  940  943  947  949  951  954  958  959  960  966  967  971\n",
      "  976  985  988  998 1003 1005 1007 1010 1012 1026 1029 1032 1038 1042\n",
      " 1043 1050 1058 1074 1080 1088 1093 1098 1101 1105 1110 1112 1113 1114\n",
      " 1119 1126 1134 1142 1149 1155 1162 1169 1179 1181 1183 1186 1187 1190\n",
      " 1200 1204 1220 1221 1222 1226 1228 1238 1240 1243 1252 1254 1258 1263\n",
      " 1265 1271 1273 1281 1283 1287 1292 1304 1307 1309 1311 1314 1316 1317\n",
      " 1331 1337 1341 1342 1345 1350 1351 1353 1354 1355 1361 1362 1366 1367\n",
      " 1373 1374 1385 1386 1399 1402 1404 1406 1413 1420 1432 1434 1439 1444\n",
      " 1447 1449 1451 1452 1456 1458 1462 1465 1487 1488 1490 1498 1504 1508\n",
      " 1510 1540 1546 1548 1555 1556 1557 1560 1561 1565 1566 1567 1569 1572\n",
      " 1578 1579 1583 1585 1593 1595 1598 1601 1602 1607 1612 1616 1625 1627\n",
      " 1628 1629 1634 1642 1645 1646 1647 1660 1664 1671 1674 1694 1701 1711\n",
      " 1733 1739 1750 1763 1765 1766 1771 1772 1774 1775 1802 1805 1806 1810\n",
      " 1816 1828 1837 1840 1842 1846 1857 1870 1873 1874 1875 1876 1891 1895\n",
      " 1902 1904 1905 1907 1912 1916 1920 1929 1935 1946 1952 1953 1955 1963\n",
      " 1968 1969 1970 1974 1979 1986 1987 1993]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5838 - acc: 0.1596\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5313 - acc: 0.1707\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5227 - acc: 0.1749\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5169 - acc: 0.1777\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5133 - acc: 0.1790\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5090 - acc: 0.1810\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5046 - acc: 0.1832\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5010 - acc: 0.1841\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4971 - acc: 0.1862\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4940 - acc: 0.1874\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4912 - acc: 0.1890\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4862 - acc: 0.1908\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4898 - acc: 0.1892\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4829 - acc: 0.1922\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4777 - acc: 0.1946\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4751 - acc: 0.1956\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4723 - acc: 0.1965\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4642 - acc: 0.1998\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4557 - acc: 0.2028\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4500 - acc: 0.2051\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4481 - acc: 0.2057\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4426 - acc: 0.2076\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4382 - acc: 0.2095\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4357 - acc: 0.2101\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4297 - acc: 0.2125\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4249 - acc: 0.2137\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4202 - acc: 0.2152\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4169 - acc: 0.2166\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4151 - acc: 0.2170\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4103 - acc: 0.2188\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4084 - acc: 0.2189\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4042 - acc: 0.2208\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3969 - acc: 0.2234\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3944 - acc: 0.2241\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3912 - acc: 0.2252\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3875 - acc: 0.2260\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3844 - acc: 0.2273\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3824 - acc: 0.2280\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3764 - acc: 0.2302\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3752 - acc: 0.2301\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3730 - acc: 0.2308\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3693 - acc: 0.2319\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3661 - acc: 0.2332\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3636 - acc: 0.2342\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3606 - acc: 0.2348\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3597 - acc: 0.2353\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3555 - acc: 0.2362\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3534 - acc: 0.2372\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3507 - acc: 0.2382\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3492 - acc: 0.2385\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3497 - acc: 0.2384\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3455 - acc: 0.2399\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3419 - acc: 0.2410\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3391 - acc: 0.2417\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3381 - acc: 0.2422\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3373 - acc: 0.2426\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3344 - acc: 0.2434\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3323 - acc: 0.2442\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3305 - acc: 0.2447\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3287 - acc: 0.2452\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3287 - acc: 0.2450\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3247 - acc: 0.2467\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3241 - acc: 0.2466\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3226 - acc: 0.2471\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3203 - acc: 0.2479\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3194 - acc: 0.2483\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3180 - acc: 0.2486\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3170 - acc: 0.2490\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3152 - acc: 0.2495\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3121 - acc: 0.2506\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3114 - acc: 0.2508\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3104 - acc: 0.2511\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3086 - acc: 0.2518\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3078 - acc: 0.2519\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3073 - acc: 0.2523\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3057 - acc: 0.2524\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3025 - acc: 0.2536\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3028 - acc: 0.2536\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3024 - acc: 0.2537\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2998 - acc: 0.2547\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2996 - acc: 0.2549\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2976 - acc: 0.2552\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2968 - acc: 0.2559\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2963 - acc: 0.2557\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2947 - acc: 0.2565\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2931 - acc: 0.2569\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2925 - acc: 0.2571\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2908 - acc: 0.2575\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2891 - acc: 0.2580\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2887 - acc: 0.2581\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2873 - acc: 0.2588\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2867 - acc: 0.2591\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2868 - acc: 0.2589\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2852 - acc: 0.2595\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2839 - acc: 0.2601\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2840 - acc: 0.2601\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2832 - acc: 0.2601\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2815 - acc: 0.2606\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2810 - acc: 0.2612\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2810 - acc: 0.2610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.00      0.01      1316\n",
      "           1       0.67      0.68      0.67     21624\n",
      "           2       0.38      0.34      0.36      4117\n",
      "           3       0.74      0.84      0.79     35326\n",
      "           4       0.50      0.32      0.39       651\n",
      "           5       0.48      0.58      0.52     21268\n",
      "           6       0.37      0.13      0.19      9176\n",
      "           7       0.45      0.40      0.42     11794\n",
      "\n",
      "    accuracy                           0.61    105272\n",
      "   macro avg       0.52      0.41      0.42    105272\n",
      "weighted avg       0.59      0.61      0.59    105272\n",
      "\n",
      "Acurácia\n",
      "0.41081514006043773\n",
      "Precisao\n",
      "0.5901949568061552\n",
      "Recall\n",
      "0.6084998860095752\n",
      "F1\n",
      "0.5886369096585271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    6   240    34   162     0   696    57   121]\n",
      " [    0 14609   296  2249    22  3391   233   824]\n",
      " [    0   338  1402   998    13   788    94   484]\n",
      " [    0  1501   496 29711   136  2207   223  1052]\n",
      " [    0    68     8   281   210    45     6    33]\n",
      " [    4  3011   589  2710    10 12253   855  1836]\n",
      " [    1  1063   284  1390    13  3814  1183  1428]\n",
      " [    0   855   582  2697    15  2405   556  4684]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   5    6    9   13   19   23   27   36   42   43   45   56   57   65\n",
      "   70   71   74   79   85   94  102  110  112  114  119  149  155  157\n",
      "  166  168  169  176  178  189  190  200  210  215  219  220  229  232\n",
      "  240  243  246  255  272  277  278  284  285  289  297  306  310  315\n",
      "  320  322  324  327  330  331  339  347  348  350  358  363  366  373\n",
      "  381  392  403  407  412  418  419  428  432  433  434  437  440  447\n",
      "  453  456  463  464  466  468  487  490  496  499  500  505  516  519\n",
      "  520  526  528  540  554  557  558  559  560  566  572  575  587  588\n",
      "  589  591  594  596  598  600  603  604  605  609  610  614  620  623\n",
      "  636  637  640  643  644  651  652  654  655  657  658  661  663  667\n",
      "  668  683  685  689  691  698  712  713  725  731  742  747  748  759\n",
      "  761  763  776  779  782  790  792  796  802  804  808  814  816  819\n",
      "  826  833  834  836  844  849  850  854  856  862  863  866  870  871\n",
      "  872  874  876  887  894  899  913  924  926  935  962  965  974  977\n",
      "  982  990  991  994  999 1009 1023 1035 1036 1037 1044 1053 1056 1060\n",
      " 1067 1076 1078 1079 1082 1083 1084 1090 1099 1100 1103 1109 1129 1131\n",
      " 1132 1136 1137 1140 1145 1150 1152 1157 1174 1176 1184 1185 1191 1194\n",
      " 1199 1207 1210 1211 1214 1215 1234 1244 1245 1246 1255 1256 1257 1272\n",
      " 1274 1276 1277 1293 1295 1296 1299 1306 1308 1313 1320 1325 1328 1334\n",
      " 1360 1371 1379 1387 1398 1401 1408 1414 1417 1418 1421 1423 1427 1431\n",
      " 1441 1442 1445 1460 1469 1474 1478 1479 1482 1483 1492 1499 1500 1502\n",
      " 1505 1515 1516 1518 1519 1520 1524 1526 1528 1534 1536 1538 1541 1554\n",
      " 1573 1582 1587 1588 1600 1608 1611 1617 1631 1632 1636 1657 1661 1663\n",
      " 1667 1669 1672 1675 1683 1688 1693 1695 1698 1702 1703 1704 1706 1707\n",
      " 1708 1709 1710 1712 1715 1716 1717 1720 1721 1724 1734 1743 1745 1754\n",
      " 1756 1767 1776 1777 1778 1782 1787 1790 1794 1803 1808 1819 1820 1832\n",
      " 1834 1847 1849 1850 1855 1856 1860 1862 1867 1882 1887 1889 1893 1894\n",
      " 1897 1901 1908 1915 1923 1926 1930 1939 1943 1954 1957 1961 1962 1964\n",
      " 1971 1975 1983 1985 1988 1991 1992 1994]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5758 - acc: 0.1578\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5291 - acc: 0.1673\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5173 - acc: 0.1724\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5123 - acc: 0.1747\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5090 - acc: 0.1766\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5073 - acc: 0.1773\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4990 - acc: 0.1814\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4931 - acc: 0.1837\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4920 - acc: 0.1841\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4887 - acc: 0.1860\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4870 - acc: 0.1861\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4825 - acc: 0.1882\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4819 - acc: 0.1884\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4762 - acc: 0.1907\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4708 - acc: 0.1931\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4699 - acc: 0.1934\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4663 - acc: 0.1948\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4646 - acc: 0.1956\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4615 - acc: 0.1966\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4550 - acc: 0.1997\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4495 - acc: 0.2012\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4436 - acc: 0.2039\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4394 - acc: 0.2049\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4341 - acc: 0.2069\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4279 - acc: 0.2091\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4272 - acc: 0.2094\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4181 - acc: 0.2127\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4144 - acc: 0.2139\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4093 - acc: 0.2155\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4059 - acc: 0.2170\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4022 - acc: 0.2177\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3986 - acc: 0.2190\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3942 - acc: 0.2206\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3910 - acc: 0.2218\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3867 - acc: 0.2234\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3863 - acc: 0.2232\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3801 - acc: 0.2253\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3766 - acc: 0.2263\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3737 - acc: 0.2272\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3695 - acc: 0.2285\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3684 - acc: 0.2289\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3640 - acc: 0.2302\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3612 - acc: 0.2312\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3591 - acc: 0.2320\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3571 - acc: 0.2323\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3524 - acc: 0.2339\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3507 - acc: 0.2343\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3465 - acc: 0.2358\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3447 - acc: 0.2369\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3423 - acc: 0.2371\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3401 - acc: 0.2381\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3373 - acc: 0.2391\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3376 - acc: 0.2385\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3350 - acc: 0.2398\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3329 - acc: 0.2402\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3323 - acc: 0.2408\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3269 - acc: 0.2423\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3266 - acc: 0.2424\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3244 - acc: 0.2429\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3232 - acc: 0.2431\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3222 - acc: 0.2436\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3183 - acc: 0.2452\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3175 - acc: 0.2451\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3158 - acc: 0.2458\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3129 - acc: 0.2467\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3127 - acc: 0.2470\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3106 - acc: 0.2471\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3107 - acc: 0.2475\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3077 - acc: 0.2488\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3066 - acc: 0.2489\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3071 - acc: 0.2489\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3030 - acc: 0.2503\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3025 - acc: 0.2500\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3015 - acc: 0.2505\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3002 - acc: 0.2510\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2979 - acc: 0.2518\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2971 - acc: 0.2521\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2955 - acc: 0.2525\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2943 - acc: 0.2530\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2936 - acc: 0.2533\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2916 - acc: 0.2538\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2912 - acc: 0.2540\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2903 - acc: 0.2543\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2900 - acc: 0.2545\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2883 - acc: 0.2551\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2870 - acc: 0.2554\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2862 - acc: 0.2555\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2847 - acc: 0.2563\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2829 - acc: 0.2567\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2836 - acc: 0.2563\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2815 - acc: 0.2573\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2816 - acc: 0.2572\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2796 - acc: 0.2580\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2805 - acc: 0.2578\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2773 - acc: 0.2588\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2755 - acc: 0.2595\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2762 - acc: 0.2590\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2766 - acc: 0.2589\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2745 - acc: 0.2596\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2727 - acc: 0.2602\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.02      0.03      1335\n",
      "           1       0.64      0.68      0.66     22058\n",
      "           2       0.32      0.30      0.31      4216\n",
      "           3       0.73      0.83      0.78     37155\n",
      "           4       0.39      0.25      0.31       872\n",
      "           5       0.47      0.52      0.50     22064\n",
      "           6       0.35      0.17      0.23      9559\n",
      "           7       0.43      0.36      0.39     12238\n",
      "\n",
      "    accuracy                           0.59    109497\n",
      "   macro avg       0.49      0.39      0.40    109497\n",
      "weighted avg       0.57      0.59      0.58    109497\n",
      "\n",
      "Acurácia\n",
      "0.3927072759886489\n",
      "Precisao\n",
      "0.5749557536537037\n",
      "Recall\n",
      "0.5940528050996831\n",
      "F1\n",
      "0.5764503499736907\n",
      "[[   22   282    42   178     4   613    80   114]\n",
      " [    1 15075   331  2409    44  3165   366   667]\n",
      " [    0   451  1276   959    10   876   152   492]\n",
      " [    0  1599   596 30850   196  2356   296  1262]\n",
      " [    0    68    28   430   222    66    15    43]\n",
      " [   12  3593   676  3040    32 11569  1297  1845]\n",
      " [    2  1295   390  1395    24  3473  1611  1369]\n",
      " [    0  1099   694  2791    36  2431   765  4422]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [  12   18   24   26   28   35   40   41   58   59   61   62   89   90\n",
      "   92   96   97  101  103  104  106  107  111  115  116  118  121  126\n",
      "  127  128  134  135  145  148  153  154  173  174  175  182  185  191\n",
      "  193  196  197  206  214  217  221  222  225  226  228  235  239  247\n",
      "  249  250  253  254  259  263  264  271  273  274  275  282  293  294\n",
      "  295  299  302  308  311  312  316  323  329  332  336  341  343  345\n",
      "  346  349  355  356  357  361  365  372  376  388  393  406  417  422\n",
      "  427  431  436  439  442  446  449  462  465  467  488  495  504  514\n",
      "  527  529  530  531  534  537  545  555  579  585  586  590  595  599\n",
      "  615  617  619  621  624  625  626  629  639  645  647  648  656  666\n",
      "  671  674  676  679  687  703  704  708  709  711  717  718  719  721\n",
      "  732  734  736  740  745  754  756  757  758  765  767  788  794  801\n",
      "  812  813  815  817  823  829  840  841  843  847  859  860  865  875\n",
      "  882  886  889  893  896  900  902  904  907  911  912  917  921  927\n",
      "  930  931  938  941  946  975  981  984  995  996 1000 1008 1011 1016\n",
      " 1020 1021 1028 1033 1040 1041 1045 1048 1055 1057 1061 1068 1071 1075\n",
      " 1081 1091 1097 1106 1107 1116 1118 1121 1124 1130 1133 1139 1141 1143\n",
      " 1144 1153 1156 1161 1163 1165 1167 1172 1173 1178 1180 1193 1195 1196\n",
      " 1206 1213 1218 1219 1223 1227 1232 1236 1237 1247 1251 1260 1261 1262\n",
      " 1266 1269 1280 1282 1285 1305 1310 1312 1318 1324 1327 1330 1333 1344\n",
      " 1349 1356 1357 1359 1363 1364 1368 1369 1382 1393 1400 1403 1410 1415\n",
      " 1419 1425 1430 1437 1438 1440 1443 1446 1455 1457 1463 1464 1466 1468\n",
      " 1473 1477 1484 1495 1503 1507 1513 1517 1521 1531 1533 1535 1543 1544\n",
      " 1551 1562 1571 1574 1590 1609 1618 1622 1626 1635 1649 1655 1662 1668\n",
      " 1673 1676 1677 1680 1681 1687 1689 1690 1696 1699 1700 1723 1730 1741\n",
      " 1744 1746 1752 1757 1760 1761 1764 1768 1779 1788 1797 1798 1801 1812\n",
      " 1814 1817 1821 1824 1831 1841 1853 1858 1866 1872 1880 1883 1884 1885\n",
      " 1886 1890 1896 1898 1899 1900 1910 1913 1921 1922 1924 1927 1932 1936\n",
      " 1940 1942 1945 1949 1965 1972 1976 1996]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5903 - acc: 0.1616\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5415 - acc: 0.1719\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5285 - acc: 0.1780\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5235 - acc: 0.1804\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5210 - acc: 0.1812\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5140 - acc: 0.1848\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5089 - acc: 0.1872\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5078 - acc: 0.1872\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5015 - acc: 0.1904\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4994 - acc: 0.1906\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5012 - acc: 0.1902\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4911 - acc: 0.1945\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4915 - acc: 0.1941\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4889 - acc: 0.1956\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4850 - acc: 0.1970\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4838 - acc: 0.1971\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4768 - acc: 0.2005\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4721 - acc: 0.2020\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4676 - acc: 0.2041\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4660 - acc: 0.2045\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4563 - acc: 0.2083\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4490 - acc: 0.2108\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4442 - acc: 0.2121\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4416 - acc: 0.2132\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4368 - acc: 0.2149\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4296 - acc: 0.2174\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4282 - acc: 0.2176\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4238 - acc: 0.2193\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4170 - acc: 0.2218\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4143 - acc: 0.2224\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4118 - acc: 0.2234\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4091 - acc: 0.2241\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4013 - acc: 0.2266\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3983 - acc: 0.2278\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3945 - acc: 0.2288\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3930 - acc: 0.2293\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3881 - acc: 0.2311\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3876 - acc: 0.2311\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3821 - acc: 0.2328\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3781 - acc: 0.2341\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3764 - acc: 0.2347\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3720 - acc: 0.2363\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3701 - acc: 0.2370\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3669 - acc: 0.2375\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3642 - acc: 0.2387\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3620 - acc: 0.2392\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3606 - acc: 0.2398\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3569 - acc: 0.2411\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3550 - acc: 0.2415\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3520 - acc: 0.2425\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3494 - acc: 0.2434\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3468 - acc: 0.2442\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3445 - acc: 0.2451\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3436 - acc: 0.2453\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3448 - acc: 0.2451\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3398 - acc: 0.2466\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3365 - acc: 0.2474\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3343 - acc: 0.2481\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3343 - acc: 0.2483\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3316 - acc: 0.2491\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3305 - acc: 0.2495\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3283 - acc: 0.2502\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3275 - acc: 0.2503\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3246 - acc: 0.2513\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3234 - acc: 0.2519\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3222 - acc: 0.2522\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3210 - acc: 0.2531\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3176 - acc: 0.2540\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3185 - acc: 0.2533\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3150 - acc: 0.2546\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3145 - acc: 0.2548\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3143 - acc: 0.2550\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3122 - acc: 0.2555\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3115 - acc: 0.2554\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3093 - acc: 0.2564\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3070 - acc: 0.2573\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3070 - acc: 0.2568\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3055 - acc: 0.2578\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3047 - acc: 0.2582\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3027 - acc: 0.2587\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3025 - acc: 0.2587\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3006 - acc: 0.2597\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3001 - acc: 0.2594\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2988 - acc: 0.2601\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2972 - acc: 0.2606\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2962 - acc: 0.2610\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2947 - acc: 0.2614\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2932 - acc: 0.2617\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2925 - acc: 0.2624\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2934 - acc: 0.2618\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2916 - acc: 0.2624\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2887 - acc: 0.2634\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2876 - acc: 0.2637\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2867 - acc: 0.2643\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2865 - acc: 0.2639\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2868 - acc: 0.2643\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2840 - acc: 0.2650\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2850 - acc: 0.2649\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2834 - acc: 0.2652\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2834 - acc: 0.2654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.00      0.01      1171\n",
      "           1       0.61      0.76      0.67     22043\n",
      "           2       0.36      0.32      0.34      3918\n",
      "           3       0.77      0.79      0.78     31618\n",
      "           4       0.42      0.27      0.33       650\n",
      "           5       0.49      0.52      0.50     20021\n",
      "           6       0.38      0.14      0.20      8875\n",
      "           7       0.44      0.43      0.44     11398\n",
      "\n",
      "    accuracy                           0.60     99694\n",
      "   macro avg       0.47      0.40      0.41     99694\n",
      "weighted avg       0.58      0.60      0.58     99694\n",
      "\n",
      "Acurácia\n",
      "0.40412615808427155\n",
      "Precisao\n",
      "0.5811667664982225\n",
      "Recall\n",
      "0.5989828876361667\n",
      "F1\n",
      "0.5811705507329178\n",
      "[[    4   361    40   104     1   503    49   109]\n",
      " [    3 16648   228  1557    11  2574   266   756]\n",
      " [    0   586  1263   714     6   687    98   564]\n",
      " [    0  2369   533 25078   165  1764   190  1519]\n",
      " [    0    51    20   304   174    52     8    41]\n",
      " [    0  4403   519  1950    19 10362   877  1891]\n",
      " [    1  1620   307   994    16  3233  1237  1467]\n",
      " [    4  1412   605  1872    20  1995   541  4949]]\n",
      "TRAIN: [   1    3    4 ... 1996 1997 1998] TEST: [   0    2    8   14   15   16   17   20   21   29   30   31   32   33\n",
      "   34   37   38   39   44   46   52   64   66   68   69   73   80   84\n",
      "   91  109  120  123  137  141  143  147  150  152  158  159  160  161\n",
      "  162  167  170  171  177  181  192  198  199  212  218  223  236  237\n",
      "  241  242  251  252  262  267  270  276  281  286  287  290  301  303\n",
      "  318  326  335  337  354  370  375  377  390  391  395  399  405  411\n",
      "  413  414  416  420  421  423  424  430  443  444  448  450  459  473\n",
      "  475  483  485  489  492  501  502  506  508  510  521  523  524  525\n",
      "  532  536  543  544  550  552  556  565  574  576  581  583  593  602\n",
      "  622  627  630  633  638  646  649  653  659  670  675  677  690  692\n",
      "  696  697  699  700  701  706  707  715  722  726  727  729  733  738\n",
      "  746  749  755  797  799  800  803  806  810  818  827  828  838  845\n",
      "  848  857  858  861  869  878  879  881  883  884  885  891  892  897\n",
      "  901  903  914  934  937  942  944  945  948  950  952  953  956  957\n",
      "  964  973  979  980  983  987  993  997 1002 1014 1017 1018 1024 1025\n",
      " 1030 1031 1034 1047 1049 1052 1062 1063 1073 1086 1087 1094 1095 1096\n",
      " 1104 1115 1117 1122 1123 1127 1135 1147 1151 1154 1158 1164 1170 1182\n",
      " 1189 1192 1198 1205 1208 1212 1216 1224 1225 1231 1235 1239 1241 1242\n",
      " 1249 1250 1259 1264 1267 1275 1278 1286 1290 1291 1300 1301 1302 1303\n",
      " 1315 1321 1323 1326 1329 1338 1365 1372 1375 1376 1378 1380 1383 1389\n",
      " 1390 1391 1392 1396 1405 1407 1411 1412 1422 1426 1428 1429 1435 1448\n",
      " 1450 1461 1467 1471 1475 1476 1481 1494 1501 1506 1512 1514 1523 1530\n",
      " 1552 1553 1558 1559 1563 1575 1586 1594 1596 1604 1605 1610 1613 1614\n",
      " 1615 1623 1624 1633 1637 1639 1640 1644 1651 1652 1656 1658 1665 1666\n",
      " 1678 1679 1684 1685 1691 1697 1714 1718 1726 1728 1729 1737 1740 1747\n",
      " 1749 1755 1762 1769 1770 1781 1783 1784 1786 1792 1793 1799 1811 1822\n",
      " 1823 1826 1827 1836 1839 1844 1848 1851 1852 1854 1859 1861 1864 1865\n",
      " 1868 1871 1881 1888 1892 1903 1909 1919 1931 1937 1947 1950 1959 1960\n",
      " 1967 1977 1978 1980 1982 1984 1990 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.5830 - acc: 0.1623\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5333 - acc: 0.1716\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.5242 - acc: 0.1757\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5234 - acc: 0.1759\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5148 - acc: 0.1801\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5107 - acc: 0.1821\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5055 - acc: 0.1842\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.5014 - acc: 0.1860\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4983 - acc: 0.1875\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4952 - acc: 0.1891\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4926 - acc: 0.1897\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4880 - acc: 0.1918\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4839 - acc: 0.1935\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4814 - acc: 0.1949\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4808 - acc: 0.1950\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4800 - acc: 0.1950\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4697 - acc: 0.1992\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4651 - acc: 0.2010\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4595 - acc: 0.2028\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.4514 - acc: 0.2058\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4461 - acc: 0.2082\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4401 - acc: 0.2102\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4379 - acc: 0.2108\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4341 - acc: 0.2121\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4281 - acc: 0.2145\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4226 - acc: 0.2163\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4191 - acc: 0.2173\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4168 - acc: 0.2181\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4113 - acc: 0.2201\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4055 - acc: 0.2218\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.4065 - acc: 0.2216\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3978 - acc: 0.2243\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3958 - acc: 0.2250\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3910 - acc: 0.2266\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3875 - acc: 0.2278\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3857 - acc: 0.2284\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3798 - acc: 0.2301\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3783 - acc: 0.2308\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3738 - acc: 0.2323\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3702 - acc: 0.2332\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3689 - acc: 0.2338\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3641 - acc: 0.2356\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3605 - acc: 0.2365\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3600 - acc: 0.2365\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3570 - acc: 0.2378\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3546 - acc: 0.2384\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3511 - acc: 0.2396\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3469 - acc: 0.2408\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3469 - acc: 0.2408\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3461 - acc: 0.2412\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3434 - acc: 0.2420\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3393 - acc: 0.2430\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3386 - acc: 0.2434\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3352 - acc: 0.2443\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3338 - acc: 0.2450\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3309 - acc: 0.2457\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3291 - acc: 0.2467\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3279 - acc: 0.2469\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3251 - acc: 0.2480\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3238 - acc: 0.2484\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3214 - acc: 0.2490\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3201 - acc: 0.2496\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3182 - acc: 0.2499\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3175 - acc: 0.2502\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3163 - acc: 0.2505\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3142 - acc: 0.2513\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3133 - acc: 0.2517\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3120 - acc: 0.2525\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3106 - acc: 0.2524\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3084 - acc: 0.2534\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3062 - acc: 0.2541\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3058 - acc: 0.2540\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3043 - acc: 0.2547\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3022 - acc: 0.2554\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3007 - acc: 0.2559\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3025 - acc: 0.2554\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2989 - acc: 0.2563\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2975 - acc: 0.2569\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2961 - acc: 0.2575\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2960 - acc: 0.2574\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2937 - acc: 0.2582\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2926 - acc: 0.2586\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2927 - acc: 0.2588\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2904 - acc: 0.2594\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2895 - acc: 0.2597\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2890 - acc: 0.2599\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2866 - acc: 0.2607\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2867 - acc: 0.2606\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2857 - acc: 0.2611\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2845 - acc: 0.2614\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2836 - acc: 0.2617\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2818 - acc: 0.2622\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2808 - acc: 0.2625\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2816 - acc: 0.2620\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2799 - acc: 0.2630\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2781 - acc: 0.2632\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2785 - acc: 0.2633\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2766 - acc: 0.2640\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2765 - acc: 0.2639\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2763 - acc: 0.2642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.02      0.03      1200\n",
      "           1       0.66      0.67      0.67     22382\n",
      "           2       0.37      0.28      0.32      3975\n",
      "           3       0.72      0.84      0.77     33495\n",
      "           4       0.47      0.23      0.31       697\n",
      "           5       0.48      0.54      0.51     20969\n",
      "           6       0.36      0.13      0.20      9167\n",
      "           7       0.44      0.40      0.42     11949\n",
      "\n",
      "    accuracy                           0.60    103834\n",
      "   macro avg       0.50      0.39      0.40    103834\n",
      "weighted avg       0.57      0.60      0.58    103834\n",
      "\n",
      "Acurácia\n",
      "0.3898944481444624\n",
      "Precisao\n",
      "0.5746838123502073\n",
      "Recall\n",
      "0.5965772290386578\n",
      "F1\n",
      "0.5757623822454044\n",
      "[[   19   233    27   210     1   529    62   119]\n",
      " [    5 15056   225  2593     8  3262   324   909]\n",
      " [    0   400  1095  1044     4   843    85   504]\n",
      " [    0  1425   458 28235   101  1857   186  1233]\n",
      " [    0    46    23   390   163    46     6    23]\n",
      " [    7  3343   448  2888    20 11315   994  1954]\n",
      " [    1  1221   229  1380    19  3595  1237  1485]\n",
      " [    5  1063   427  2691    31  2346   561  4825]]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_40 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_41 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_42 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_43 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_44 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_45 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_46 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_47 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_48 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_49 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 1,705,608\n",
      "Trainable params: 1,705,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Acurácias total\n",
      "[0.40464018899985854, 0.41081514006043773, 0.3927072759886489, 0.40412615808427155, 0.3898944481444624]\n",
      "0.40043664225553577\n",
      "Precision total\n",
      "[0.5811668159892723, 0.5901949568061552, 0.5749557536537037, 0.5811667664982225, 0.5746838123502073]\n",
      "0.5804336210595122\n",
      "Recalls total\n",
      "[0.5964045082093302, 0.6084998860095752, 0.5940528050996831, 0.5989828876361667, 0.5965772290386578]\n",
      "0.5989034631986826\n",
      "F1 total\n",
      "[0.5787956721682876, 0.5886369096585271, 0.5764503499736907, 0.5811705507329178, 0.5757623822454044]\n",
      "0.5801631729557655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   6    9   10   11   15   18   29   33   48   51   52   56   59   65\n",
      "   88   89  108  110  111  113  116  117  118  126  127  130  132  134\n",
      "  136  139  142  147  150  155  157  175  177  178  183  184  185  193\n",
      "  200  201  203  204  209  211  213  222  224  236  237  241  242  243\n",
      "  245  263  264  282  285  286  287  299  302  313  322  341  349  351\n",
      "  359  361  362  363  364  368  373  376  385  389  390  405  408  412\n",
      "  416  418  420  434  435  447  448  458  463  473  477  505  510  516\n",
      "  517  527  528  533  537  538  542  544  546  551  560  567  575  576\n",
      "  579  580  587  604  606  610  613  616  635  640  642  644  649  665\n",
      "  675  682  684  688  690  694  696  710  714  716  721  722  725  727\n",
      "  730  735  737  742  752  776  777  787  792  794  799  802  804  809\n",
      "  820  823  824  825  828  829  834  835  844  847  850  851  855  856\n",
      "  858  859  864  865  873  874  876  882  899  907  918  921  922  924\n",
      "  930  935  941  947  951  954  969  974  975  979  982  986  995  997\n",
      "  999 1001 1003 1006 1009 1015 1017 1019 1024 1025 1026 1029 1044 1062\n",
      " 1063 1067 1084 1090 1092 1094 1103 1104 1115 1130 1136 1137 1139 1146\n",
      " 1148 1153 1156 1157 1160 1162 1166 1172 1187 1194 1200 1201 1202 1204\n",
      " 1205 1206 1211 1213 1214 1215 1218 1219 1226 1230 1231 1233 1238 1240\n",
      " 1243 1244 1247 1264 1270 1280 1281 1312 1316 1324 1332 1333 1338 1340\n",
      " 1342 1348 1349 1351 1354 1356 1360 1363 1368 1372 1377 1383 1384 1388\n",
      " 1392 1400 1404 1410 1411 1412 1414 1426 1429 1430 1435 1436 1443 1446\n",
      " 1448 1454 1455 1463 1478 1481 1482 1488 1492 1503 1505 1508 1517 1519\n",
      " 1523 1527 1528 1530 1533 1544 1545 1546 1548 1549 1563 1573 1576 1583\n",
      " 1585 1586 1590 1591 1597 1598 1603 1604 1615 1617 1638 1639 1640 1649\n",
      " 1651 1655 1662 1665 1667 1676 1679 1688 1699 1701 1702 1708 1713 1720\n",
      " 1730 1738 1744 1749 1752 1764 1765 1766 1777 1786 1791 1797 1800 1812\n",
      " 1816 1831 1838 1842 1846 1852 1853 1861 1867 1873 1876 1881 1882 1890\n",
      " 1891 1892 1894 1895 1900 1902 1904 1907 1914 1918 1928 1931 1935 1936\n",
      " 1937 1940 1941 1944 1971 1976 1992 1994]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3495 - acc: 0.7857\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3148 - acc: 0.8565\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3103 - acc: 0.8595\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3058 - acc: 0.8620\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3059 - acc: 0.8625\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3020 - acc: 0.8646\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2994 - acc: 0.8655\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2994 - acc: 0.8656\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2949 - acc: 0.8681\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2964 - acc: 0.8665\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2951 - acc: 0.8672\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2898 - acc: 0.8704\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2894 - acc: 0.8706\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2870 - acc: 0.8724\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2873 - acc: 0.8714\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2840 - acc: 0.8734\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2821 - acc: 0.8730\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2795 - acc: 0.8751\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2757 - acc: 0.8772\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2755 - acc: 0.8773\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2716 - acc: 0.8797\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2692 - acc: 0.8813\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2673 - acc: 0.8820\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2627 - acc: 0.8844\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2591 - acc: 0.8868\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2562 - acc: 0.8885\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2490 - acc: 0.8922\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2448 - acc: 0.8940\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2407 - acc: 0.8959\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2380 - acc: 0.8970\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2347 - acc: 0.8988\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2305 - acc: 0.9006\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2265 - acc: 0.9023\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2246 - acc: 0.9036\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2215 - acc: 0.9048\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2176 - acc: 0.9064\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2153 - acc: 0.9076\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2157 - acc: 0.9074\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2107 - acc: 0.9096\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2077 - acc: 0.9108\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2042 - acc: 0.9124\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2018 - acc: 0.9135\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1996 - acc: 0.9146\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1989 - acc: 0.9150\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1949 - acc: 0.9166\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1927 - acc: 0.9177\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1911 - acc: 0.9183\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1907 - acc: 0.9184\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1906 - acc: 0.9189\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1902 - acc: 0.9189\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1838 - acc: 0.9217\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1819 - acc: 0.9223\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1796 - acc: 0.9235\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1782 - acc: 0.9238\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1763 - acc: 0.9249\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1744 - acc: 0.9257\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1741 - acc: 0.9257\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1748 - acc: 0.9255\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1728 - acc: 0.9265\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1706 - acc: 0.9274\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1716 - acc: 0.9273\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1671 - acc: 0.9287\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1646 - acc: 0.9301\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1649 - acc: 0.9298\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1633 - acc: 0.9303\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1616 - acc: 0.9311\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1607 - acc: 0.9314\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1577 - acc: 0.9331\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1563 - acc: 0.9334\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1561 - acc: 0.9335\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1546 - acc: 0.9344\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1532 - acc: 0.9348\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1524 - acc: 0.9350\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1512 - acc: 0.9356\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1509 - acc: 0.9356\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1506 - acc: 0.9360\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1486 - acc: 0.9371\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1476 - acc: 0.9375\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1481 - acc: 0.9372\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1460 - acc: 0.9385\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1456 - acc: 0.9386\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1439 - acc: 0.9391\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1437 - acc: 0.9394\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1437 - acc: 0.9395\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1432 - acc: 0.9396\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1421 - acc: 0.9399\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1408 - acc: 0.9405\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1407 - acc: 0.9405\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1414 - acc: 0.9404\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1399 - acc: 0.9412\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1365 - acc: 0.9424\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1372 - acc: 0.9421\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1360 - acc: 0.9426\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1348 - acc: 0.9432\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1350 - acc: 0.9431\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1341 - acc: 0.9439\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1327 - acc: 0.9444\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1330 - acc: 0.9441\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1327 - acc: 0.9443\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1321 - acc: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72     42343\n",
      "           1       0.67      0.61      0.64     22553\n",
      "           2       0.78      0.73      0.75     38707\n",
      "\n",
      "    accuracy                           0.71    103603\n",
      "   macro avg       0.71      0.70      0.70    103603\n",
      "weighted avg       0.72      0.71      0.71    103603\n",
      "\n",
      "Acurácia\n",
      "0.6973964650793204\n",
      "Precisao\n",
      "0.7151082785502414\n",
      "Recall\n",
      "0.7132611990000289\n",
      "F1\n",
      "0.7128495522798034\n",
      "[[31985  4347  6011]\n",
      " [ 6690 13728  2135]\n",
      " [ 8114  2410 28183]]\n",
      "TRAIN: [   0    1    2 ... 1996 1998 1999] TEST: [  12   23   27   28   37   43   45   46   74   78   81   84   85   87\n",
      "   93   97   99  101  112  114  115  119  122  140  154  159  172  188\n",
      "  189  190  194  207  210  221  226  228  235  251  253  261  262  266\n",
      "  267  269  270  272  274  276  283  289  290  292  301  305  307  314\n",
      "  316  320  327  329  342  348  354  355  356  360  367  372  374  377\n",
      "  396  404  407  409  421  422  442  444  451  457  460  461  462  466\n",
      "  468  469  471  481  485  486  489  490  491  494  496  500  507  508\n",
      "  511  526  531  532  539  541  543  553  556  557  559  562  564  570\n",
      "  589  599  600  605  609  615  617  619  628  633  647  648  653  657\n",
      "  658  663  671  680  689  691  692  702  705  709  712  719  729  743\n",
      "  746  750  753  755  764  770  775  783  786  801  803  805  806  807\n",
      "  811  813  818  826  827  831  836  840  841  842  843  854  871  875\n",
      "  885  887  888  889  894  896  897  898  901  904  910  911  912  915\n",
      "  919  926  927  928  940  942  945  946  948  949  952  961  962  991\n",
      " 1000 1004 1005 1007 1012 1014 1016 1022 1030 1032 1036 1039 1043 1048\n",
      " 1054 1056 1064 1071 1072 1076 1077 1086 1091 1096 1098 1113 1116 1119\n",
      " 1120 1121 1122 1129 1132 1135 1154 1155 1159 1163 1186 1189 1192 1195\n",
      " 1221 1222 1225 1227 1228 1229 1232 1236 1241 1246 1251 1254 1267 1271\n",
      " 1276 1285 1297 1302 1305 1307 1321 1323 1330 1335 1336 1343 1347 1352\n",
      " 1359 1369 1374 1376 1379 1381 1393 1394 1398 1399 1401 1406 1413 1415\n",
      " 1419 1423 1425 1431 1433 1444 1449 1461 1462 1465 1476 1484 1486 1489\n",
      " 1491 1493 1497 1499 1509 1510 1513 1518 1521 1537 1539 1551 1555 1557\n",
      " 1565 1570 1577 1582 1587 1594 1596 1601 1612 1614 1618 1621 1622 1627\n",
      " 1636 1637 1641 1642 1645 1646 1660 1663 1671 1675 1680 1681 1685 1704\n",
      " 1706 1711 1716 1717 1718 1719 1721 1722 1728 1731 1733 1734 1735 1739\n",
      " 1742 1743 1746 1747 1750 1759 1760 1771 1773 1774 1780 1782 1783 1785\n",
      " 1787 1789 1798 1801 1802 1809 1810 1813 1814 1815 1823 1824 1826 1830\n",
      " 1841 1854 1855 1856 1868 1871 1886 1888 1905 1911 1930 1933 1945 1946\n",
      " 1952 1968 1973 1975 1985 1986 1993 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3467 - acc: 0.7388\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3163 - acc: 0.8569\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3143 - acc: 0.8571\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3094 - acc: 0.8594\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3075 - acc: 0.8616\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3058 - acc: 0.8621\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3028 - acc: 0.8641\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3003 - acc: 0.8652\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2995 - acc: 0.8658\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2974 - acc: 0.8667\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2954 - acc: 0.8679\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2941 - acc: 0.8678\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2913 - acc: 0.8696\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2901 - acc: 0.8704\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2881 - acc: 0.8715\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2859 - acc: 0.8720\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2828 - acc: 0.8740\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2821 - acc: 0.8746\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2770 - acc: 0.8778\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2749 - acc: 0.8790\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2745 - acc: 0.8796\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2656 - acc: 0.8846\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2608 - acc: 0.8869\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2559 - acc: 0.8888\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2511 - acc: 0.8914\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2471 - acc: 0.8930\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2437 - acc: 0.8947\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2402 - acc: 0.8960\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2379 - acc: 0.8973\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2350 - acc: 0.8986\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2326 - acc: 0.8998\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2317 - acc: 0.9004\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2267 - acc: 0.9026\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2224 - acc: 0.9045\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2192 - acc: 0.9060\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2173 - acc: 0.9067\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2152 - acc: 0.9075\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2130 - acc: 0.9087\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2104 - acc: 0.9097\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2062 - acc: 0.9116\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2051 - acc: 0.9120\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2045 - acc: 0.9125\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2000 - acc: 0.9144\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1988 - acc: 0.9149\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1946 - acc: 0.9169\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1936 - acc: 0.9175\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1907 - acc: 0.9188\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1890 - acc: 0.9197\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1882 - acc: 0.9199\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1848 - acc: 0.9215\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1827 - acc: 0.9221\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1805 - acc: 0.9233\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1808 - acc: 0.9229\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1786 - acc: 0.9244\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1783 - acc: 0.9246\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1764 - acc: 0.9251\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1734 - acc: 0.9265\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1714 - acc: 0.9273\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1697 - acc: 0.9278\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1690 - acc: 0.9284\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1690 - acc: 0.9283\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1656 - acc: 0.9297\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1659 - acc: 0.9297\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1649 - acc: 0.9302\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1624 - acc: 0.9316\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1606 - acc: 0.9321\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1597 - acc: 0.9324\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1582 - acc: 0.9329\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1579 - acc: 0.9333\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1576 - acc: 0.9334\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1549 - acc: 0.9347\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1542 - acc: 0.9349\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1530 - acc: 0.9354\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1523 - acc: 0.9359\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1515 - acc: 0.9361\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1494 - acc: 0.9367\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1497 - acc: 0.9367\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1484 - acc: 0.9371\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1479 - acc: 0.9375\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1466 - acc: 0.9383\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1448 - acc: 0.9390\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1447 - acc: 0.9390\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1446 - acc: 0.9391\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1430 - acc: 0.9396\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1437 - acc: 0.9392\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1422 - acc: 0.9403\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1406 - acc: 0.9408\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1398 - acc: 0.9413\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1396 - acc: 0.9411\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1396 - acc: 0.9412\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1384 - acc: 0.9417\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1376 - acc: 0.9419\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1357 - acc: 0.9432\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1346 - acc: 0.9435\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1368 - acc: 0.9425\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1344 - acc: 0.9435\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1336 - acc: 0.9438\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1334 - acc: 0.9441\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1324 - acc: 0.9445\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1316 - acc: 0.9448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.72     40152\n",
      "           1       0.68      0.66      0.67     22313\n",
      "           2       0.77      0.77      0.77     36097\n",
      "\n",
      "    accuracy                           0.73     98562\n",
      "   macro avg       0.72      0.72      0.72     98562\n",
      "weighted avg       0.73      0.73      0.73     98562\n",
      "\n",
      "Acurácia\n",
      "0.7183100108057104\n",
      "Precisao\n",
      "0.726379664290623\n",
      "Recall\n",
      "0.7267506747022179\n",
      "F1\n",
      "0.7265067072063129\n",
      "[[29007  4762  6383]\n",
      " [ 5567 14708  2038]\n",
      " [ 6085  2097 27915]]\n",
      "TRAIN: [   0    2    3 ... 1996 1997 1999] TEST: [   1    5   14   16   21   26   30   32   35   47   50   62   63   66\n",
      "   67   68   69   71   72   73   75   79   86   91  100  105  106  121\n",
      "  128  131  133  141  152  153  161  165  166  169  171  174  176  179\n",
      "  186  195  197  202  214  234  238  239  244  247  248  252  260  268\n",
      "  278  280  281  288  291  293  295  297  310  311  317  323  324  331\n",
      "  332  337  340  344  346  347  352  369  370  375  381  386  388  393\n",
      "  398  399  411  413  414  417  424  426  433  436  443  446  453  455\n",
      "  465  472  474  480  484  495  497  499  502  503  515  518  521  522\n",
      "  523  525  529  530  535  536  550  554  561  569  572  577  578  584\n",
      "  585  588  590  594  596  597  602  614  618  622  625  627  634  637\n",
      "  655  656  668  669  674  676  677  685  693  697  699  700  703  704\n",
      "  713  720  724  734  740  741  749  751  757  758  759  760  768  774\n",
      "  784  793  795  798  800  808  815  817  819  833  837  849  852  853\n",
      "  857  862  863  866  877  880  893  900  903  908  913  916  925  931\n",
      "  932  934  937  944  953  958  959  963  964  967  973  985  989  992\n",
      "  996 1011 1021 1035 1042 1047 1049 1051 1052 1058 1059 1060 1065 1069\n",
      " 1074 1081 1082 1087 1088 1099 1101 1106 1110 1125 1127 1134 1138 1144\n",
      " 1149 1152 1158 1164 1165 1167 1171 1193 1207 1210 1216 1224 1234 1245\n",
      " 1249 1252 1253 1255 1256 1259 1261 1262 1265 1266 1268 1274 1275 1277\n",
      " 1283 1286 1294 1298 1301 1303 1306 1308 1315 1326 1329 1331 1334 1337\n",
      " 1339 1345 1346 1357 1358 1364 1366 1375 1391 1402 1416 1422 1442 1450\n",
      " 1452 1464 1466 1470 1474 1479 1485 1496 1500 1504 1515 1516 1522 1524\n",
      " 1529 1531 1532 1540 1542 1543 1552 1554 1556 1568 1569 1572 1593 1608\n",
      " 1609 1610 1611 1635 1647 1648 1650 1652 1658 1659 1661 1666 1673 1684\n",
      " 1686 1690 1693 1694 1695 1696 1698 1703 1715 1727 1741 1745 1751 1755\n",
      " 1758 1761 1762 1767 1769 1770 1778 1779 1784 1790 1793 1794 1795 1799\n",
      " 1803 1817 1820 1822 1834 1835 1848 1851 1857 1863 1872 1883 1884 1896\n",
      " 1903 1908 1919 1920 1923 1924 1929 1938 1943 1953 1961 1967 1974 1978\n",
      " 1979 1980 1982 1983 1987 1991 1995 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3458 - acc: 0.8115\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3121 - acc: 0.8586\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3059 - acc: 0.8625\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3047 - acc: 0.8624\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3018 - acc: 0.8639\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2985 - acc: 0.8653\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2968 - acc: 0.8667\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2973 - acc: 0.8667\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2933 - acc: 0.8681\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2902 - acc: 0.8702\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2908 - acc: 0.8691\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2871 - acc: 0.8710\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2854 - acc: 0.8723\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2815 - acc: 0.8737\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2814 - acc: 0.8741\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2781 - acc: 0.8757\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2781 - acc: 0.8756\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2741 - acc: 0.8787\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2694 - acc: 0.8813\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2619 - acc: 0.8853\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2581 - acc: 0.8873\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2531 - acc: 0.8896\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2536 - acc: 0.8896\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2462 - acc: 0.8931\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2458 - acc: 0.8935\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2406 - acc: 0.8960\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2385 - acc: 0.8969\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2346 - acc: 0.8989\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2313 - acc: 0.9005\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2292 - acc: 0.9013\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2261 - acc: 0.9026\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2238 - acc: 0.9036\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2212 - acc: 0.9049\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2190 - acc: 0.9061\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2146 - acc: 0.9079\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2129 - acc: 0.9086\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2101 - acc: 0.9100\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2103 - acc: 0.9101\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2051 - acc: 0.9123\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2026 - acc: 0.9135\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2012 - acc: 0.9145\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1987 - acc: 0.9151\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1962 - acc: 0.9163\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1939 - acc: 0.9176\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1916 - acc: 0.9185\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1899 - acc: 0.9189\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1871 - acc: 0.9206\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1872 - acc: 0.9205\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1844 - acc: 0.9217\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1818 - acc: 0.9226\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1805 - acc: 0.9234\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1794 - acc: 0.9241\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1786 - acc: 0.9242\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1768 - acc: 0.9250\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1781 - acc: 0.9245\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1765 - acc: 0.9255\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1713 - acc: 0.9278\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1694 - acc: 0.9283\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1680 - acc: 0.9293\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1675 - acc: 0.9294\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1667 - acc: 0.9297\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1654 - acc: 0.9303\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1635 - acc: 0.9311\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1634 - acc: 0.9313\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1615 - acc: 0.9322\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1602 - acc: 0.9327\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1576 - acc: 0.9336\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1569 - acc: 0.9338\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1575 - acc: 0.9337\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1569 - acc: 0.9338\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1549 - acc: 0.9351\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1531 - acc: 0.9357\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1515 - acc: 0.9365\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1507 - acc: 0.9367\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1509 - acc: 0.9366\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1487 - acc: 0.9375\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1479 - acc: 0.9378\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1464 - acc: 0.9387\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1456 - acc: 0.9389\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1444 - acc: 0.9396\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1450 - acc: 0.9392\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1451 - acc: 0.9394\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1436 - acc: 0.9398\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1428 - acc: 0.9401\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1414 - acc: 0.9408\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1398 - acc: 0.9416\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1392 - acc: 0.9416\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1394 - acc: 0.9415\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1377 - acc: 0.9423\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1373 - acc: 0.9425\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1371 - acc: 0.9426\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1357 - acc: 0.9432\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1361 - acc: 0.9431\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1348 - acc: 0.9438\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1360 - acc: 0.9435\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1339 - acc: 0.9441\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1317 - acc: 0.9448\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1319 - acc: 0.9451\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1311 - acc: 0.9452\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1312 - acc: 0.9452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72     44496\n",
      "           1       0.68      0.65      0.66     23925\n",
      "           2       0.78      0.78      0.78     40416\n",
      "\n",
      "    accuracy                           0.73    108837\n",
      "   macro avg       0.72      0.72      0.72    108837\n",
      "weighted avg       0.73      0.73      0.73    108837\n",
      "\n",
      "Acurácia\n",
      "0.7189161693252065\n",
      "Precisao\n",
      "0.7303509142131751\n",
      "Recall\n",
      "0.7304776868160644\n",
      "F1\n",
      "0.7302580930628763\n",
      "[[32609  5303  6584]\n",
      " [ 6382 15457  2086]\n",
      " [ 6913  2066 31437]]\n",
      "TRAIN: [   0    1    5 ... 1997 1998 1999] TEST: [   2    3    4    7    8   19   22   24   25   34   38   39   42   49\n",
      "   53   58   60   61   64   70   90   95   98  102  103  104  107  109\n",
      "  123  124  129  138  143  144  146  149  156  158  160  167  170  182\n",
      "  187  192  198  199  215  217  219  220  225  227  229  231  232  233\n",
      "  246  249  250  254  256  258  265  271  273  275  277  279  294  296\n",
      "  304  306  308  315  318  319  321  325  330  336  338  339  345  350\n",
      "  358  366  371  378  379  380  382  392  394  395  401  403  406  410\n",
      "  415  419  423  425  427  428  429  431  438  445  449  450  452  454\n",
      "  456  459  467  470  478  482  483  487  488  498  506  513  514  534\n",
      "  540  545  549  565  566  571  581  582  586  592  593  595  611  612\n",
      "  620  626  631  632  638  643  645  646  659  661  666  667  670  672\n",
      "  679  681  707  708  711  715  717  723  726  731  732  738  739  744\n",
      "  745  747  756  761  763  765  771  778  779  780  781  782  788  789\n",
      "  791  797  812  822  830  832  839  845  848  867  868  870  881  883\n",
      "  890  891  895  905  906  909  914  920  929  933  943  950  955  956\n",
      "  970  972  977  981  983  990  993  994  998 1002 1020 1023 1027 1033\n",
      " 1037 1038 1040 1041 1045 1046 1053 1061 1066 1068 1093 1097 1109 1112\n",
      " 1124 1131 1133 1145 1150 1151 1161 1169 1170 1175 1176 1178 1179 1181\n",
      " 1182 1188 1190 1191 1197 1198 1199 1208 1235 1242 1257 1263 1269 1273\n",
      " 1278 1284 1289 1290 1295 1296 1299 1304 1309 1310 1319 1328 1350 1353\n",
      " 1355 1361 1367 1378 1380 1382 1407 1408 1424 1428 1432 1434 1440 1445\n",
      " 1447 1451 1456 1458 1459 1460 1469 1471 1477 1490 1494 1506 1511 1525\n",
      " 1526 1534 1547 1550 1553 1560 1562 1567 1574 1578 1579 1580 1584 1592\n",
      " 1599 1600 1605 1607 1624 1629 1631 1643 1644 1653 1656 1664 1669 1670\n",
      " 1677 1678 1687 1692 1707 1710 1712 1714 1729 1737 1757 1763 1768 1775\n",
      " 1776 1806 1807 1808 1811 1818 1819 1825 1828 1832 1833 1839 1840 1844\n",
      " 1845 1847 1860 1862 1866 1870 1879 1880 1893 1898 1899 1901 1906 1909\n",
      " 1910 1921 1922 1926 1927 1932 1939 1947 1949 1951 1954 1955 1958 1959\n",
      " 1962 1966 1969 1972 1977 1981 1990 1996]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.3458 - acc: 0.8038\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.3156 - acc: 0.8571\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3094 - acc: 0.8608\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3093 - acc: 0.8603\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3050 - acc: 0.8615\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3019 - acc: 0.8639\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3030 - acc: 0.8624\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2974 - acc: 0.8662\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2969 - acc: 0.8663\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2945 - acc: 0.8672\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2934 - acc: 0.8680\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2894 - acc: 0.8696\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2896 - acc: 0.8694\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2874 - acc: 0.8716\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2879 - acc: 0.8704\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2825 - acc: 0.8727\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2827 - acc: 0.8740\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2792 - acc: 0.8756\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2772 - acc: 0.8769\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2725 - acc: 0.8788\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2704 - acc: 0.8805\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2665 - acc: 0.8823\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2631 - acc: 0.8844\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2570 - acc: 0.8876\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2529 - acc: 0.8892\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2489 - acc: 0.8913\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2432 - acc: 0.8942\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2393 - acc: 0.8960\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2390 - acc: 0.8966\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2332 - acc: 0.8987\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2303 - acc: 0.9002\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2298 - acc: 0.9006\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2241 - acc: 0.9033\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2219 - acc: 0.9043\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2182 - acc: 0.9052\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2183 - acc: 0.9060\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2135 - acc: 0.9078\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2110 - acc: 0.9092\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2078 - acc: 0.9103\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2063 - acc: 0.9110\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2031 - acc: 0.9122\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2013 - acc: 0.9132\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1995 - acc: 0.9142\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1984 - acc: 0.9148\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1949 - acc: 0.9165\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1927 - acc: 0.9174\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1904 - acc: 0.9183\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1882 - acc: 0.9190\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1888 - acc: 0.9186\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1854 - acc: 0.9202\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1823 - acc: 0.9219\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1817 - acc: 0.9218\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1794 - acc: 0.9229\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1773 - acc: 0.9236\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1755 - acc: 0.9246\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1754 - acc: 0.9249\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1730 - acc: 0.9255\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1722 - acc: 0.9264\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1712 - acc: 0.9270\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1698 - acc: 0.9274\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1679 - acc: 0.9284\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1657 - acc: 0.9291\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1657 - acc: 0.9292\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1632 - acc: 0.9299\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1614 - acc: 0.9312\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1604 - acc: 0.9320\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1595 - acc: 0.9323\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1596 - acc: 0.9318\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1569 - acc: 0.9329\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1570 - acc: 0.9331\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1560 - acc: 0.9335\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1545 - acc: 0.9343\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1532 - acc: 0.9344\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1515 - acc: 0.9352\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1511 - acc: 0.9356\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1503 - acc: 0.9357\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1500 - acc: 0.9361\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1476 - acc: 0.9370\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1466 - acc: 0.9378\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1464 - acc: 0.9378\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1453 - acc: 0.9380\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1443 - acc: 0.9386\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1433 - acc: 0.9388\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1427 - acc: 0.9389\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1427 - acc: 0.9390\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1411 - acc: 0.9398\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1409 - acc: 0.9400\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1398 - acc: 0.9402\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1390 - acc: 0.9406\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1375 - acc: 0.9414\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1383 - acc: 0.9413\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1374 - acc: 0.9418\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1366 - acc: 0.9419\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1354 - acc: 0.9419\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1349 - acc: 0.9421\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1354 - acc: 0.9421\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1330 - acc: 0.9430\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1333 - acc: 0.9429\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1321 - acc: 0.9435\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1305 - acc: 0.9442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.72     42907\n",
      "           1       0.68      0.64      0.66     23591\n",
      "           2       0.76      0.79      0.77     38120\n",
      "\n",
      "    accuracy                           0.72    104618\n",
      "   macro avg       0.72      0.71      0.71    104618\n",
      "weighted avg       0.72      0.72      0.72    104618\n",
      "\n",
      "Acurácia\n",
      "0.7129050595186438\n",
      "Precisao\n",
      "0.7224667893635419\n",
      "Recall\n",
      "0.7235466172169225\n",
      "F1\n",
      "0.7227576591775072\n",
      "[[30702  5121  7084]\n",
      " [ 6063 15031  2497]\n",
      " [ 6192  1965 29963]]\n",
      "TRAIN: [   1    2    3 ... 1996 1997 1998] TEST: [   0   13   17   20   31   36   40   41   44   54   55   57   76   77\n",
      "   80   82   83   92   94   96  120  125  135  137  145  148  151  162\n",
      "  163  164  168  173  180  181  191  196  205  206  208  212  216  218\n",
      "  223  230  240  255  257  259  284  298  300  303  309  312  326  328\n",
      "  333  334  335  343  353  357  365  383  384  387  391  397  400  402\n",
      "  430  432  437  439  440  441  464  475  476  479  492  493  501  504\n",
      "  509  512  519  520  524  547  548  552  555  558  563  568  573  574\n",
      "  583  591  598  601  603  607  608  621  623  624  629  630  636  639\n",
      "  641  650  651  652  654  660  662  664  673  678  683  686  687  695\n",
      "  698  701  706  718  728  733  736  748  754  762  766  767  769  772\n",
      "  773  785  790  796  810  814  816  821  838  846  860  861  869  872\n",
      "  878  879  884  886  892  902  917  923  936  938  939  957  960  965\n",
      "  966  968  971  976  978  980  984  987  988 1008 1010 1013 1018 1028\n",
      " 1031 1034 1050 1055 1057 1070 1073 1075 1078 1079 1080 1083 1085 1089\n",
      " 1095 1100 1102 1105 1107 1108 1111 1114 1117 1118 1123 1126 1128 1140\n",
      " 1141 1142 1143 1147 1168 1173 1174 1177 1180 1183 1184 1185 1196 1203\n",
      " 1209 1212 1217 1220 1223 1237 1239 1248 1250 1258 1260 1272 1279 1282\n",
      " 1287 1288 1291 1292 1293 1300 1311 1313 1314 1317 1318 1320 1322 1325\n",
      " 1327 1341 1344 1362 1365 1370 1371 1373 1385 1386 1387 1389 1390 1395\n",
      " 1396 1397 1403 1405 1409 1417 1418 1420 1421 1427 1437 1438 1439 1441\n",
      " 1453 1457 1467 1468 1472 1473 1475 1480 1483 1487 1495 1498 1501 1502\n",
      " 1507 1512 1514 1520 1535 1536 1538 1541 1558 1559 1561 1564 1566 1571\n",
      " 1575 1581 1588 1589 1595 1602 1606 1613 1616 1619 1620 1623 1625 1626\n",
      " 1628 1630 1632 1633 1634 1654 1657 1668 1672 1674 1682 1683 1689 1691\n",
      " 1697 1700 1705 1709 1723 1724 1725 1726 1732 1736 1740 1748 1753 1754\n",
      " 1756 1772 1781 1788 1792 1796 1804 1805 1821 1827 1829 1836 1837 1843\n",
      " 1849 1850 1858 1859 1864 1865 1869 1874 1875 1877 1878 1885 1887 1889\n",
      " 1897 1912 1913 1915 1916 1917 1925 1934 1942 1948 1950 1956 1957 1960\n",
      " 1963 1964 1965 1970 1984 1988 1989 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 28s 17ms/sample - loss: 0.3476 - acc: 0.7932\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3168 - acc: 0.8567\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3128 - acc: 0.8579\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3095 - acc: 0.8605\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3072 - acc: 0.8616\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3041 - acc: 0.8627\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.3017 - acc: 0.8636\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2992 - acc: 0.8652\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2971 - acc: 0.8663\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2977 - acc: 0.8671\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2934 - acc: 0.8689\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2936 - acc: 0.8685\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2948 - acc: 0.8677\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2882 - acc: 0.8710\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2857 - acc: 0.8727\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2857 - acc: 0.8723\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2820 - acc: 0.8742\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2853 - acc: 0.8728\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2779 - acc: 0.8766\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2738 - acc: 0.8787\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2716 - acc: 0.8801\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2655 - acc: 0.8830\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2613 - acc: 0.8860\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2573 - acc: 0.8880\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2526 - acc: 0.8903\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2511 - acc: 0.8912\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2437 - acc: 0.8945\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2413 - acc: 0.8955\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2390 - acc: 0.8965\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2358 - acc: 0.8980\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2327 - acc: 0.8999\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2340 - acc: 0.8992\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.2275 - acc: 0.9022\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2254 - acc: 0.9031\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2237 - acc: 0.9042\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2201 - acc: 0.9058\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2172 - acc: 0.9071\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2148 - acc: 0.9081\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2128 - acc: 0.9087\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2101 - acc: 0.9100\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2079 - acc: 0.9111\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2057 - acc: 0.9122\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2032 - acc: 0.9133\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.2020 - acc: 0.9134\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1998 - acc: 0.9149\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1958 - acc: 0.9166\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1942 - acc: 0.9172\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1932 - acc: 0.9178\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1916 - acc: 0.9184\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1892 - acc: 0.9193\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1896 - acc: 0.9193\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1862 - acc: 0.9207\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1832 - acc: 0.9221\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1809 - acc: 0.9232\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1798 - acc: 0.9235\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1781 - acc: 0.9244\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1771 - acc: 0.9250\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1752 - acc: 0.9259\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1758 - acc: 0.9255\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1736 - acc: 0.9266\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1738 - acc: 0.9262\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1727 - acc: 0.9270\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1704 - acc: 0.9279\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1685 - acc: 0.9287\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1660 - acc: 0.9300\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1657 - acc: 0.9298\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1645 - acc: 0.9305\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1630 - acc: 0.9311\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1619 - acc: 0.9316\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1598 - acc: 0.9326\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1591 - acc: 0.9329\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1581 - acc: 0.9334\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1570 - acc: 0.9336\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1569 - acc: 0.9339\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1552 - acc: 0.9348\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1536 - acc: 0.9353\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1528 - acc: 0.9355\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1531 - acc: 0.9356\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1515 - acc: 0.9361\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1505 - acc: 0.9370\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1502 - acc: 0.9365\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1496 - acc: 0.9371\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1488 - acc: 0.9376\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1477 - acc: 0.9378\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1463 - acc: 0.9386\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1460 - acc: 0.9384\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1443 - acc: 0.9394\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1447 - acc: 0.9392\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1444 - acc: 0.9394\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1428 - acc: 0.9402\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1418 - acc: 0.9404\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1411 - acc: 0.9406\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1408 - acc: 0.9408\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1408 - acc: 0.9411\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1392 - acc: 0.9416\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 25s 15ms/sample - loss: 0.1386 - acc: 0.9417\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1386 - acc: 0.9418\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1368 - acc: 0.9427\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1372 - acc: 0.9427\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1366 - acc: 0.9431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73     41700\n",
      "           1       0.69      0.67      0.68     22814\n",
      "           2       0.76      0.79      0.78     35674\n",
      "\n",
      "    accuracy                           0.73    100188\n",
      "   macro avg       0.73      0.73      0.73    100188\n",
      "weighted avg       0.73      0.73      0.73    100188\n",
      "\n",
      "Acurácia\n",
      "0.7270445193504201\n",
      "Precisao\n",
      "0.7335886998237553\n",
      "Recall\n",
      "0.7343693855551563\n",
      "F1\n",
      "0.7337692708034645\n",
      "[[30049  4966  6685]\n",
      " [ 5359 15216  2239]\n",
      " [ 5645  1719 28310]]\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_90 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_91 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_92 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_93 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_94 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_95 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_96 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_97 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_98 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_99 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 1,704,603\n",
      "Trainable params: 1,704,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Acurácias total\n",
      "[0.6973964650793204, 0.7183100108057104, 0.7189161693252065, 0.7129050595186438, 0.7270445193504201]\n",
      "0.7149144448158603\n",
      "Precision total\n",
      "[0.7151082785502414, 0.726379664290623, 0.7303509142131751, 0.7224667893635419, 0.7335886998237553]\n",
      "0.7255788692482673\n",
      "Recalls total\n",
      "[0.7132611990000289, 0.7267506747022179, 0.7304776868160644, 0.7235466172169225, 0.7343693855551563]\n",
      "0.725681112658078\n",
      "F1 total\n",
      "[0.7128495522798034, 0.7265067072063129, 0.7302580930628763, 0.7227576591775072, 0.7337692708034645]\n",
      "0.7252282565059929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede(8)\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classesQ8[train_index],\n",
    "                           previsores[test_index], classesQ8[test_index], 8)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Acurácias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())\n",
    "\n",
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede(3)\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classesQ3[train_index],\n",
    "                           previsores[test_index], classesQ3[test_index], 3)\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "print('Acurácias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
