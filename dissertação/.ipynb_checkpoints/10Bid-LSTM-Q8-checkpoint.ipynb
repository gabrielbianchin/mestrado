{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 19:59:57.600255  5036 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 19:59:57.606239  5036 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 19:59:57.608232  5036 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 19:59:57.608232  5036 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [   7   11   29   31   33   43   47   48   49   60   67   80   81   98\n",
      "   99  110  112  113  115  117  129  133  134  135  137  138  146  147\n",
      "  168  169  170  182  185  190  202  208  213  217  231  238  239  241\n",
      "  249  255  256  260  263  265  268  269  272  273  277  280  283  285\n",
      "  291  296  297  300  304  306  312  313  316  317  318  321  332  336\n",
      "  338  359  361  362  368  381  387  406  408  419  421  427  433  440\n",
      "  449  459  463  466  471  479  482  485  492  495  500  508  514  516\n",
      "  518  521  524  534  536  538  543  547  556  557  570  571  572  581\n",
      "  583  591  598  608  610  611  617  618  620  622  623  625  629  633\n",
      "  637  650  651  657  658  659  664  665  667  670  678  679  685  687\n",
      "  690  693  709  713  722  727  746  755  756  761  771  773  776  780\n",
      "  784  789  793  795  802  803  804  815  816  819  833  834  838  841\n",
      "  850  851  854  859  866  867  873  875  887  903  907  911  912  913\n",
      "  917  918  921  922  926  932  934  945  956  957  969  970  975  977\n",
      "  978  986  991  997  999 1005 1011 1014 1043 1047 1049 1050 1051 1060\n",
      " 1061 1064 1065 1068 1072 1073 1075 1080 1081 1084 1092 1094 1097 1106\n",
      " 1109 1116 1118 1124 1125 1128 1130 1137 1138 1141 1142 1153 1156 1162\n",
      " 1164 1166 1168 1169 1176 1178 1184 1191 1193 1206 1207 1213 1216 1219\n",
      " 1220 1223 1225 1229 1238 1241 1244 1252 1255 1256 1262 1264 1273 1277\n",
      " 1292 1294 1298 1299 1306 1310 1313 1314 1318 1324 1331 1333 1337 1342\n",
      " 1346 1348 1354 1357 1358 1366 1367 1373 1378 1379 1380 1383 1389 1393\n",
      " 1394 1403 1404 1436 1437 1443 1444 1452 1468 1469 1481 1487 1494 1498\n",
      " 1499 1501 1504 1506 1517 1531 1533 1543 1544 1551 1554 1560 1581 1582\n",
      " 1592 1594 1599 1607 1611 1613 1615 1631 1638 1639 1643 1649 1660 1662\n",
      " 1671 1676 1688 1692 1694 1699 1700 1704 1720 1721 1729 1736 1737 1743\n",
      " 1747 1749 1756 1758 1762 1771 1772 1783 1785 1787 1798 1802 1809 1813\n",
      " 1815 1821 1824 1826 1836 1838 1839 1843 1854 1862 1865 1868 1870 1873\n",
      " 1882 1897 1909 1927 1935 1937 1944 1946 1947 1953 1955 1957 1964 1965\n",
      " 1967 1975 1986 1987 1988 1989 1994 1998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 19:59:59.564031  5036 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.6216 - acc: 0.1292\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5740 - acc: 0.1473\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5497 - acc: 0.1598\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5254 - acc: 0.1711\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5125 - acc: 0.1762\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5054 - acc: 0.1793\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4994 - acc: 0.1819\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4946 - acc: 0.1843\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4914 - acc: 0.1851\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4899 - acc: 0.1861\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4827 - acc: 0.1892\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4805 - acc: 0.1900\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4802 - acc: 0.1898\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4761 - acc: 0.1922\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4710 - acc: 0.1936\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4691 - acc: 0.1942\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4610 - acc: 0.1976\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4555 - acc: 0.1996\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4499 - acc: 0.2017\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4460 - acc: 0.2031\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4458 - acc: 0.2032\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4337 - acc: 0.2072\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4263 - acc: 0.2102\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4192 - acc: 0.2123\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4141 - acc: 0.2143\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4075 - acc: 0.2161\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4019 - acc: 0.2181\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3966 - acc: 0.2195\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3913 - acc: 0.2216\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3905 - acc: 0.2215\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3801 - acc: 0.2252\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3735 - acc: 0.2272\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3690 - acc: 0.2288\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3672 - acc: 0.2294\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3623 - acc: 0.2309\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3522 - acc: 0.2340\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3483 - acc: 0.2353\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3455 - acc: 0.2360\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3426 - acc: 0.2369\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3350 - acc: 0.2397\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3320 - acc: 0.2406\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3285 - acc: 0.2414\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3253 - acc: 0.2426\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3204 - acc: 0.2442\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3179 - acc: 0.2449\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3137 - acc: 0.2463\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3124 - acc: 0.2467\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3078 - acc: 0.2483\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3052 - acc: 0.2492\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3003 - acc: 0.2508\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2976 - acc: 0.2515\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2970 - acc: 0.2518\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2916 - acc: 0.2535\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2908 - acc: 0.2537\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2896 - acc: 0.2541\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2865 - acc: 0.2553\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2833 - acc: 0.2566\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2825 - acc: 0.2566\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2803 - acc: 0.2573\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2755 - acc: 0.2588\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2735 - acc: 0.2594\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2721 - acc: 0.2601\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2707 - acc: 0.2606\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2702 - acc: 0.2607\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2686 - acc: 0.2613\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2648 - acc: 0.2625\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2634 - acc: 0.2632\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2605 - acc: 0.2642\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2592 - acc: 0.2645\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2563 - acc: 0.2656\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2565 - acc: 0.2656\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2547 - acc: 0.2663\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2529 - acc: 0.2669\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2534 - acc: 0.2668\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2504 - acc: 0.2679\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2469 - acc: 0.2691\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2473 - acc: 0.2687\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2450 - acc: 0.2694\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2447 - acc: 0.2695\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2425 - acc: 0.2707\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2408 - acc: 0.2713\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2392 - acc: 0.2715\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2386 - acc: 0.2719\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2372 - acc: 0.2722\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2363 - acc: 0.2727\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2354 - acc: 0.2731\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2336 - acc: 0.2736\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2316 - acc: 0.2740\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2328 - acc: 0.2741\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2319 - acc: 0.2743\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2298 - acc: 0.2751\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2277 - acc: 0.2759\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2268 - acc: 0.2762\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2252 - acc: 0.2767\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2236 - acc: 0.2773\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2248 - acc: 0.2771\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2231 - acc: 0.2776\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2213 - acc: 0.2782\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2197 - acc: 0.2788\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2184 - acc: 0.2792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.04      0.07      1302\n",
      "           1       0.62      0.62      0.62     22633\n",
      "           2       0.29      0.26      0.28      4078\n",
      "           3       0.68      0.76      0.72     35908\n",
      "           4       0.44      0.26      0.32       731\n",
      "           5       0.45      0.49      0.47     21596\n",
      "           6       0.38      0.17      0.24      9840\n",
      "           7       0.37      0.41      0.39     12214\n",
      "\n",
      "    accuracy                           0.55    108302\n",
      "   macro avg       0.50      0.38      0.39    108302\n",
      "weighted avg       0.54      0.55      0.54    108302\n",
      "\n",
      "Acur√°cia\n",
      "0.37620547233751583\n",
      "Precisao\n",
      "0.5444429480042022\n",
      "Recall\n",
      "0.5530461117984894\n",
      "F1\n",
      "0.5404005461392015\n",
      "[[   50   207    47   198     2   539    72   187]\n",
      " [    1 13950   404  3526    44  3235   370  1103]\n",
      " [    0   391  1078  1079    14   743   113   660]\n",
      " [    2  2588   633 27279   108  2784   329  2185]\n",
      " [    0    57    15   345   187    54     8    65]\n",
      " [   10  3164   630  3346    28 10667  1284  2467]\n",
      " [    2  1214   348  1449    20  3327  1710  1770]\n",
      " [    2  1093   542  2650    26  2266   660  4975]]\n",
      "TRAIN: [   0    1    3 ... 1996 1997 1998] TEST: [   2    5    6   12   19   21   23   26   28   35   36   37   42   52\n",
      "   53   54   62   65   66   71   72   76   91  101  107  119  121  124\n",
      "  125  126  141  156  161  167  172  180  186  188  191  192  193  194\n",
      "  204  207  211  214  215  219  224  225  234  242  247  251  252  258\n",
      "  267  274  278  290  301  308  309  311  334  339  342  343  357  365\n",
      "  366  375  376  385  395  397  402  404  407  410  412  414  417  418\n",
      "  420  430  432  445  446  447  448  450  451  452  453  457  460  469\n",
      "  470  484  486  487  489  497  498  509  520  522  529  531  532  540\n",
      "  541  542  545  552  559  562  563  564  565  567  568  575  576  580\n",
      "  593  595  597  605  614  615  616  630  643  649  660  672  676  684\n",
      "  691  695  701  702  703  715  716  724  736  741  742  744  749  758\n",
      "  781  782  799  807  809  811  820  827  828  829  832  835  840  842\n",
      "  845  862  865  869  870  882  883  884  888  891  894  897  901  904\n",
      "  905  906  920  929  931  937  958  960  963  964  966  967  972  980\n",
      "  988  990  998 1012 1013 1019 1020 1024 1028 1031 1033 1034 1040 1042\n",
      " 1045 1048 1053 1057 1059 1067 1074 1076 1078 1079 1085 1090 1091 1098\n",
      " 1103 1104 1110 1117 1129 1132 1144 1146 1148 1154 1163 1171 1186 1192\n",
      " 1210 1222 1224 1228 1234 1235 1247 1250 1268 1269 1270 1281 1283 1285\n",
      " 1288 1289 1290 1296 1302 1303 1305 1312 1316 1317 1319 1322 1323 1350\n",
      " 1352 1356 1361 1374 1376 1381 1387 1405 1406 1408 1417 1418 1419 1421\n",
      " 1424 1434 1435 1440 1445 1446 1447 1451 1455 1459 1467 1470 1475 1476\n",
      " 1488 1490 1493 1503 1505 1508 1510 1515 1516 1519 1522 1528 1535 1536\n",
      " 1552 1559 1566 1574 1575 1576 1596 1600 1601 1603 1605 1620 1621 1624\n",
      " 1633 1641 1642 1646 1648 1650 1655 1656 1661 1663 1670 1672 1678 1685\n",
      " 1687 1696 1698 1701 1705 1711 1719 1725 1727 1735 1748 1755 1757 1761\n",
      " 1776 1778 1779 1780 1781 1784 1786 1795 1796 1797 1806 1810 1814 1816\n",
      " 1819 1825 1828 1830 1845 1848 1851 1853 1859 1861 1864 1871 1874 1879\n",
      " 1884 1888 1889 1900 1902 1910 1911 1915 1916 1917 1919 1923 1925 1938\n",
      " 1939 1966 1968 1970 1972 1990 1993 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.6324 - acc: 0.1338\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5849 - acc: 0.1519\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5616 - acc: 0.1633\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5405 - acc: 0.1725\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5284 - acc: 0.1777\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5200 - acc: 0.1814\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5127 - acc: 0.1849\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5108 - acc: 0.1856\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5059 - acc: 0.1875\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5040 - acc: 0.1885\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4986 - acc: 0.1902\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4938 - acc: 0.1928\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4908 - acc: 0.1945\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4863 - acc: 0.1961\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4835 - acc: 0.1971\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4744 - acc: 0.2001\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4742 - acc: 0.2003\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4626 - acc: 0.2054\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4580 - acc: 0.2064\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4545 - acc: 0.2077\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4467 - acc: 0.2106\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4402 - acc: 0.2136\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4319 - acc: 0.2163\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4268 - acc: 0.2180\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4201 - acc: 0.2200\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4112 - acc: 0.2234\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4036 - acc: 0.2269\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3973 - acc: 0.2278\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3935 - acc: 0.2287\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3847 - acc: 0.2314\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3782 - acc: 0.2331\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3733 - acc: 0.2348\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3694 - acc: 0.2361\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3630 - acc: 0.2378\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3594 - acc: 0.2391\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3568 - acc: 0.2398\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3507 - acc: 0.2418\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3470 - acc: 0.2431\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3441 - acc: 0.2437\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3365 - acc: 0.2462\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3329 - acc: 0.2471\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3289 - acc: 0.2485\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3257 - acc: 0.2497\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3224 - acc: 0.2507\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3182 - acc: 0.2519\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3152 - acc: 0.2532\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3127 - acc: 0.2538\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3107 - acc: 0.2546\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3060 - acc: 0.2560\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3030 - acc: 0.2570\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2998 - acc: 0.2583\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2973 - acc: 0.2588\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2925 - acc: 0.2606\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2905 - acc: 0.2612\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2885 - acc: 0.2621\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2851 - acc: 0.2629\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2849 - acc: 0.2632\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2829 - acc: 0.2637\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2809 - acc: 0.2644\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2797 - acc: 0.2649\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2765 - acc: 0.2658\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2730 - acc: 0.2672\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2707 - acc: 0.2682\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2702 - acc: 0.2681\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2693 - acc: 0.2685\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2652 - acc: 0.2699\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2652 - acc: 0.2698\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2624 - acc: 0.2710\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2599 - acc: 0.2716\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2581 - acc: 0.2724\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2563 - acc: 0.2731\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2560 - acc: 0.2733\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2534 - acc: 0.2739\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2510 - acc: 0.2750\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2511 - acc: 0.2747\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2491 - acc: 0.2758\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2470 - acc: 0.2763\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2476 - acc: 0.2760\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2459 - acc: 0.2767\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2439 - acc: 0.2774\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2431 - acc: 0.2778\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2404 - acc: 0.2787\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2416 - acc: 0.2783\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2403 - acc: 0.2789\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2370 - acc: 0.2797\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2354 - acc: 0.2806\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2350 - acc: 0.2807\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2341 - acc: 0.2812\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2332 - acc: 0.2813\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2317 - acc: 0.2820\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2300 - acc: 0.2825\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2294 - acc: 0.2828\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2283 - acc: 0.2831\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2274 - acc: 0.2835\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2247 - acc: 0.2842\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2237 - acc: 0.2849\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2233 - acc: 0.2850\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2234 - acc: 0.2849\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2211 - acc: 0.2857\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2220 - acc: 0.2852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.03      0.05      1121\n",
      "           1       0.60      0.63      0.61     19947\n",
      "           2       0.28      0.24      0.26      3956\n",
      "           3       0.71      0.76      0.73     34322\n",
      "           4       0.40      0.22      0.28       673\n",
      "           5       0.44      0.51      0.47     20020\n",
      "           6       0.36      0.12      0.18      8597\n",
      "           7       0.37      0.39      0.38     11116\n",
      "\n",
      "    accuracy                           0.56     99752\n",
      "   macro avg       0.48      0.36      0.37     99752\n",
      "weighted avg       0.54      0.56      0.54     99752\n",
      "\n",
      "Acur√°cia\n",
      "0.3616369102715842\n",
      "Precisao\n",
      "0.5448686072738022\n",
      "Recall\n",
      "0.5562294490336034\n",
      "F1\n",
      "0.5403885523459083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   30   224    31   195     1   456    35   149]\n",
      " [    0 12491   319  2692    26  3331   204   884]\n",
      " [    1   448   935   991     8   861   116   596]\n",
      " [    0  2464   634 26234    97  2798   260  1835]\n",
      " [    0    50    15   339   145    56     4    64]\n",
      " [    8  3035   576  2995    30 10258   738  2380]\n",
      " [    1  1044   307  1402    23  3230  1006  1584]\n",
      " [    2   967   473  2359    31  2437   461  4386]]\n",
      "TRAIN: [   1    2    4 ... 1997 1998 1999] TEST: [   0    3   10   13   16   17   20   24   30   40   44   45   46   51\n",
      "   55   59   63   64   78   82   84   85   88   89   97  108  116  131\n",
      "  139  140  144  145  148  149  150  151  154  157  162  173  177  179\n",
      "  184  189  196  200  203  206  212  216  220  222  227  235  246  248\n",
      "  253  261  276  284  288  294  295  298  305  310  315  326  330  331\n",
      "  337  340  348  350  353  356  360  363  370  373  378  382  390  393\n",
      "  400  405  409  413  423  425  429  431  435  436  441  443  454  461\n",
      "  462  467  472  473  474  478  483  490  502  504  515  517  527  530\n",
      "  535  546  548  549  550  551  553  560  561  566  573  574  584  585\n",
      "  586  587  589  594  603  613  627  631  632  635  640  642  646  653\n",
      "  663  668  669  671  673  675  677  681  686  689  692  694  704  707\n",
      "  708  712  714  723  725  730  732  734  739  747  752  753  754  759\n",
      "  762  764  765  766  769  775  777  787  788  798  800  806  817  822\n",
      "  830  836  846  849  855  857  860  861  868  877  881  892  898  908\n",
      "  909  915  924  925  928  930  933  939  952  968  971  981  983  984\n",
      "  994 1000 1004 1009 1010 1015 1016 1021 1022 1023 1026 1029 1030 1041\n",
      " 1044 1052 1056 1062 1063 1089 1096 1100 1101 1115 1122 1131 1135 1143\n",
      " 1149 1150 1152 1170 1173 1177 1180 1182 1185 1189 1190 1198 1200 1204\n",
      " 1209 1211 1212 1221 1226 1230 1232 1233 1242 1243 1246 1249 1251 1254\n",
      " 1260 1263 1265 1267 1272 1276 1286 1291 1297 1301 1307 1309 1329 1332\n",
      " 1336 1341 1355 1359 1362 1363 1368 1375 1382 1386 1390 1392 1395 1396\n",
      " 1398 1399 1400 1401 1411 1413 1416 1423 1427 1429 1430 1441 1453 1461\n",
      " 1463 1466 1478 1482 1489 1496 1507 1512 1514 1523 1526 1527 1534 1539\n",
      " 1540 1545 1549 1555 1561 1563 1573 1577 1585 1586 1587 1590 1593 1608\n",
      " 1610 1617 1619 1622 1628 1629 1630 1636 1647 1652 1654 1658 1665 1668\n",
      " 1679 1682 1686 1690 1693 1703 1706 1708 1714 1715 1716 1724 1728 1739\n",
      " 1744 1753 1754 1763 1769 1770 1773 1790 1794 1805 1818 1823 1827 1829\n",
      " 1837 1855 1856 1866 1877 1881 1885 1899 1904 1905 1907 1914 1926 1931\n",
      " 1932 1945 1948 1951 1952 1954 1959 1995]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.6293 - acc: 0.1304\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5842 - acc: 0.1480\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5603 - acc: 0.1591\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5335 - acc: 0.1715\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5186 - acc: 0.1775\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5135 - acc: 0.1798\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5090 - acc: 0.1816\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5016 - acc: 0.1847\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4996 - acc: 0.1855\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4959 - acc: 0.1874\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4922 - acc: 0.1890\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4878 - acc: 0.1907\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4836 - acc: 0.1923\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4822 - acc: 0.1929\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4751 - acc: 0.1958\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4707 - acc: 0.1971\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4652 - acc: 0.1994\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4603 - acc: 0.2014\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4532 - acc: 0.2035\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4511 - acc: 0.2048\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4461 - acc: 0.2063\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4387 - acc: 0.2091\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4289 - acc: 0.2125\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4242 - acc: 0.2141\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4164 - acc: 0.2168\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4104 - acc: 0.2187\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4062 - acc: 0.2201\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3969 - acc: 0.2229\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3913 - acc: 0.2248\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3861 - acc: 0.2263\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3850 - acc: 0.2268\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3761 - acc: 0.2297\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3705 - acc: 0.2309\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3657 - acc: 0.2331\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3610 - acc: 0.2342\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3542 - acc: 0.2364\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3492 - acc: 0.2383\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3473 - acc: 0.2387\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3424 - acc: 0.2402\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3383 - acc: 0.2417\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3362 - acc: 0.2422\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3283 - acc: 0.2450\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3257 - acc: 0.2454\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3229 - acc: 0.2465\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3179 - acc: 0.2482\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3153 - acc: 0.2488\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3113 - acc: 0.2501\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3086 - acc: 0.2512\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3046 - acc: 0.2522\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3045 - acc: 0.2524\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3002 - acc: 0.2538\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2981 - acc: 0.2543\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2963 - acc: 0.2551\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2926 - acc: 0.2566\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2881 - acc: 0.2577\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2857 - acc: 0.2589\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2865 - acc: 0.2582\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2812 - acc: 0.2601\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2801 - acc: 0.2605\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2779 - acc: 0.2611\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2760 - acc: 0.2619\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2731 - acc: 0.2627\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2706 - acc: 0.2639\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2681 - acc: 0.2648\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2647 - acc: 0.2656\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2669 - acc: 0.2650\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2628 - acc: 0.2664\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2608 - acc: 0.2673\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2613 - acc: 0.2669\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2595 - acc: 0.2677\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2557 - acc: 0.2690\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2545 - acc: 0.2694\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2524 - acc: 0.2700\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2524 - acc: 0.2701\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2488 - acc: 0.2716\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2492 - acc: 0.2713\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2485 - acc: 0.2716\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2456 - acc: 0.2728\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2446 - acc: 0.2730\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2435 - acc: 0.2735\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2408 - acc: 0.2744\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2400 - acc: 0.2745\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2370 - acc: 0.2757\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2361 - acc: 0.2760\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2361 - acc: 0.2760\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2340 - acc: 0.2768\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2339 - acc: 0.2770\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2325 - acc: 0.2775\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2299 - acc: 0.2783\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2304 - acc: 0.2779\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2306 - acc: 0.2781\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2278 - acc: 0.2792\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2268 - acc: 0.2794\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2264 - acc: 0.2797\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2246 - acc: 0.2804\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2254 - acc: 0.2802\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2228 - acc: 0.2810\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2218 - acc: 0.2810\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2201 - acc: 0.2820\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2224 - acc: 0.2813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.04      0.07      1381\n",
      "           1       0.65      0.65      0.65     22666\n",
      "           2       0.38      0.31      0.34      4003\n",
      "           3       0.71      0.76      0.74     33219\n",
      "           4       0.39      0.33      0.36       705\n",
      "           5       0.45      0.54      0.49     21229\n",
      "           6       0.38      0.18      0.25      9314\n",
      "           7       0.42      0.43      0.42     11998\n",
      "\n",
      "    accuracy                           0.57    104515\n",
      "   macro avg       0.51      0.40      0.41    104515\n",
      "weighted avg       0.57      0.57      0.56    104515\n",
      "\n",
      "Acur√°cia\n",
      "0.4045777853983672\n",
      "Precisao\n",
      "0.5665625540634561\n",
      "Recall\n",
      "0.5726450748696359\n",
      "F1\n",
      "0.5612860414992079\n",
      "[[   49   251    39   202     1   613    64   162]\n",
      " [    1 14664   253  2674    28  3727   338   981]\n",
      " [    0   388  1238   816    14   869   131   547]\n",
      " [    1  2124   512 25314   212  2806   334  1916]\n",
      " [    0    74     8   273   233    50     8    59]\n",
      " [   14  3077   486  2836    47 11530  1151  2088]\n",
      " [    2  1028   291  1137    16  3689  1693  1458]\n",
      " [    7   950   435  2171    39  2576   691  5129]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   4    8    9   14   15   18   25   27   32   34   38   39   41   50\n",
      "   56   57   70   73   74   83   86   93   94  102  106  109  111  118\n",
      "  120  123  127  128  130  136  142  143  153  158  165  166  171  174\n",
      "  175  176  178  187  198  199  201  210  221  233  237  244  245  250\n",
      "  254  264  266  270  279  282  286  287  299  302  319  320  322  324\n",
      "  333  341  345  346  347  352  354  355  358  364  369  372  377  383\n",
      "  386  391  392  394  396  398  401  416  422  438  455  464  468  475\n",
      "  488  491  496  499  503  506  507  510  511  519  533  544  555  569\n",
      "  579  582  588  590  596  599  604  612  621  624  626  628  636  644\n",
      "  645  647  656  666  680  683  688  697  699  706  717  719  729  740\n",
      "  745  748  750  763  767  768  774  778  783  785  790  796  801  805\n",
      "  812  813  814  818  821  823  824  825  831  843  844  847  856  863\n",
      "  864  872  874  876  879  885  886  889  890  893  914  916  927  938\n",
      "  941  943  946  947  948  949  951  954  955  962  982  985  989  995\n",
      "  996 1001 1008 1017 1018 1025 1027 1032 1035 1036 1038 1039 1046 1055\n",
      " 1066 1071 1086 1095 1107 1108 1112 1113 1119 1120 1121 1123 1126 1133\n",
      " 1134 1136 1140 1145 1157 1158 1159 1160 1165 1167 1174 1183 1196 1197\n",
      " 1205 1215 1217 1218 1227 1231 1237 1239 1240 1245 1248 1257 1258 1259\n",
      " 1274 1278 1280 1284 1287 1304 1311 1315 1321 1327 1328 1334 1335 1339\n",
      " 1344 1345 1349 1360 1369 1377 1391 1409 1412 1414 1415 1422 1425 1426\n",
      " 1428 1431 1432 1433 1439 1448 1449 1450 1454 1456 1457 1458 1460 1462\n",
      " 1464 1472 1473 1474 1483 1484 1485 1497 1502 1518 1525 1538 1541 1547\n",
      " 1550 1553 1556 1558 1562 1567 1568 1569 1571 1580 1584 1589 1598 1616\n",
      " 1626 1634 1635 1640 1644 1645 1653 1657 1673 1681 1684 1689 1707 1710\n",
      " 1713 1718 1723 1730 1731 1732 1738 1740 1741 1745 1751 1759 1760 1766\n",
      " 1767 1774 1777 1792 1793 1799 1800 1801 1804 1811 1812 1832 1833 1835\n",
      " 1841 1842 1846 1850 1852 1857 1858 1863 1867 1872 1878 1886 1892 1893\n",
      " 1894 1895 1896 1906 1908 1913 1921 1930 1933 1934 1940 1941 1942 1943\n",
      " 1950 1963 1969 1971 1974 1976 1978 1979]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6304 - acc: 0.1350\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5838 - acc: 0.1527\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5614 - acc: 0.1638\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5382 - acc: 0.1738\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5274 - acc: 0.1780\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5184 - acc: 0.1818\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5132 - acc: 0.1841\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5099 - acc: 0.1859\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5047 - acc: 0.1879\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5002 - acc: 0.1899\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4962 - acc: 0.1916\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4930 - acc: 0.1929\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4894 - acc: 0.1944\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4863 - acc: 0.1958\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4812 - acc: 0.1976\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4787 - acc: 0.1984\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4741 - acc: 0.2006\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4696 - acc: 0.2019\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4634 - acc: 0.2047\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4560 - acc: 0.2071\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4537 - acc: 0.2079\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4474 - acc: 0.2102\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4416 - acc: 0.2120\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4348 - acc: 0.2142\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4288 - acc: 0.2162\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4213 - acc: 0.2192\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4141 - acc: 0.2211\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4060 - acc: 0.2243\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4021 - acc: 0.2258\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3963 - acc: 0.2279\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3884 - acc: 0.2298\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3848 - acc: 0.2309\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3779 - acc: 0.2333\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3751 - acc: 0.2341\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3692 - acc: 0.2358\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3616 - acc: 0.2377\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3564 - acc: 0.2394\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3541 - acc: 0.2405\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3492 - acc: 0.2423\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3452 - acc: 0.2436\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3392 - acc: 0.2454\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3368 - acc: 0.2462\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3336 - acc: 0.2472\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3290 - acc: 0.2485\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3274 - acc: 0.2490\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3224 - acc: 0.2508\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3176 - acc: 0.2521\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3126 - acc: 0.2538\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3113 - acc: 0.2544\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3098 - acc: 0.2549\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3084 - acc: 0.2551\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3035 - acc: 0.2565\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3009 - acc: 0.2576\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2972 - acc: 0.2587\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2932 - acc: 0.2603\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2922 - acc: 0.2603\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2910 - acc: 0.2608\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2882 - acc: 0.2621\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2850 - acc: 0.2627\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2852 - acc: 0.2631\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2823 - acc: 0.2637\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2808 - acc: 0.2643\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2779 - acc: 0.2653\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2751 - acc: 0.2661\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2714 - acc: 0.2675\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2699 - acc: 0.2680\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2692 - acc: 0.2685\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2674 - acc: 0.2687\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2638 - acc: 0.2705\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2628 - acc: 0.2709\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2608 - acc: 0.2710\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2594 - acc: 0.2720\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2575 - acc: 0.2722\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2562 - acc: 0.2727\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2547 - acc: 0.2735\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2526 - acc: 0.2740\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2518 - acc: 0.2745\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2514 - acc: 0.2747\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2508 - acc: 0.2746\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2489 - acc: 0.2755\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2460 - acc: 0.2764\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2437 - acc: 0.2769\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2453 - acc: 0.2766\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2443 - acc: 0.2771\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2409 - acc: 0.2784\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2395 - acc: 0.2790\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2400 - acc: 0.2785\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2380 - acc: 0.2793\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2366 - acc: 0.2796\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2342 - acc: 0.2806\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2335 - acc: 0.2811\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2330 - acc: 0.2811\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2324 - acc: 0.2815\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2302 - acc: 0.2820\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2297 - acc: 0.2824\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2291 - acc: 0.2825\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2276 - acc: 0.2830\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2261 - acc: 0.2838\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2264 - acc: 0.2837\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2252 - acc: 0.2839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.04      0.07      1222\n",
      "           1       0.63      0.64      0.63     21926\n",
      "           2       0.32      0.32      0.32      3955\n",
      "           3       0.70      0.76      0.73     31868\n",
      "           4       0.49      0.24      0.32       769\n",
      "           5       0.44      0.51      0.47     20020\n",
      "           6       0.37      0.18      0.24      8942\n",
      "           7       0.39      0.38      0.39     11393\n",
      "\n",
      "    accuracy                           0.56    100095\n",
      "   macro avg       0.51      0.38      0.40    100095\n",
      "weighted avg       0.55      0.56      0.55    100095\n",
      "\n",
      "Acur√°cia\n",
      "0.38414956515706955\n",
      "Precisao\n",
      "0.5526030514563944\n",
      "Recall\n",
      "0.5601678405514761\n",
      "F1\n",
      "0.5483999373120415\n",
      "[[   46   241    42   173     2   512    65   141]\n",
      " [    3 13997   395  2769    15  3379   362  1006]\n",
      " [    0   386  1260   858     1   766   132   552]\n",
      " [    0  2186   734 24358   110  2607   317  1556]\n",
      " [    0    78    26   322   183    94     9    57]\n",
      " [   12  3161   623  2673    30 10233  1182  2106]\n",
      " [    1  1134   315  1174    14  3279  1626  1399]\n",
      " [    2  1021   567  2328    21  2425   662  4367]]\n",
      "TRAIN: [   0    2    3 ... 1995 1998 1999] TEST: [   1   22   58   61   68   69   75   77   79   87   90   92   95   96\n",
      "  100  103  104  105  114  122  132  152  155  159  160  163  164  181\n",
      "  183  195  197  205  209  218  223  226  228  229  230  232  236  240\n",
      "  243  257  259  262  271  275  281  289  292  293  303  307  314  323\n",
      "  325  327  328  329  335  344  349  351  367  371  374  379  380  384\n",
      "  388  389  399  403  411  415  424  426  428  434  437  439  442  444\n",
      "  456  458  465  476  477  480  481  493  494  501  505  512  513  523\n",
      "  525  526  528  537  539  554  558  577  578  592  600  601  602  606\n",
      "  607  609  619  634  638  639  641  648  652  654  655  661  662  674\n",
      "  682  696  698  700  705  710  711  718  720  721  726  728  731  733\n",
      "  735  737  738  743  751  757  760  770  772  779  786  791  792  794\n",
      "  797  808  810  826  837  839  848  852  853  858  871  878  880  895\n",
      "  896  899  900  902  910  919  923  935  936  940  942  944  950  953\n",
      "  959  961  965  973  974  976  979  987  992  993 1002 1003 1006 1007\n",
      " 1037 1054 1058 1069 1070 1077 1082 1083 1087 1088 1093 1099 1102 1105\n",
      " 1111 1114 1127 1139 1147 1151 1155 1161 1172 1175 1179 1181 1187 1188\n",
      " 1194 1195 1199 1201 1202 1203 1208 1214 1236 1253 1261 1266 1271 1275\n",
      " 1279 1282 1293 1295 1300 1308 1320 1325 1326 1330 1338 1340 1343 1347\n",
      " 1351 1353 1364 1365 1370 1371 1372 1384 1385 1388 1397 1402 1407 1410\n",
      " 1420 1438 1442 1465 1471 1477 1479 1480 1486 1491 1492 1495 1500 1509\n",
      " 1511 1513 1520 1521 1524 1529 1530 1532 1537 1542 1546 1548 1557 1564\n",
      " 1565 1570 1572 1578 1579 1583 1588 1591 1595 1597 1602 1604 1606 1609\n",
      " 1612 1614 1618 1623 1625 1627 1632 1637 1651 1659 1664 1666 1667 1669\n",
      " 1674 1675 1677 1680 1683 1691 1695 1697 1702 1709 1712 1717 1722 1726\n",
      " 1733 1734 1742 1746 1750 1752 1764 1765 1768 1775 1782 1788 1789 1791\n",
      " 1803 1807 1808 1817 1820 1822 1831 1834 1840 1844 1847 1849 1860 1869\n",
      " 1875 1876 1880 1883 1887 1890 1891 1898 1901 1903 1912 1918 1920 1922\n",
      " 1924 1928 1929 1936 1949 1956 1958 1960 1961 1962 1973 1977 1980 1981\n",
      " 1982 1983 1984 1985 1991 1992 1996 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.6228 - acc: 0.1391\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5743 - acc: 0.1549\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5535 - acc: 0.1646\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5288 - acc: 0.1743\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5176 - acc: 0.1791\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5120 - acc: 0.1817\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5121 - acc: 0.1815\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5070 - acc: 0.1838\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5033 - acc: 0.1856\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4980 - acc: 0.1879\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4917 - acc: 0.1906\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4885 - acc: 0.1918\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4863 - acc: 0.1926\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4801 - acc: 0.1950\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4774 - acc: 0.1962\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4719 - acc: 0.1979\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4657 - acc: 0.2006\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4593 - acc: 0.2026\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4539 - acc: 0.2048\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4472 - acc: 0.2073\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4593 - acc: 0.2030\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4488 - acc: 0.2063\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4387 - acc: 0.2098\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4287 - acc: 0.2131\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4217 - acc: 0.2159\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4125 - acc: 0.2186\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.4071 - acc: 0.2206\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3999 - acc: 0.2228\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3932 - acc: 0.2251\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3906 - acc: 0.2255\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3838 - acc: 0.2279\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3791 - acc: 0.2293\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3715 - acc: 0.2320\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3685 - acc: 0.2329\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3628 - acc: 0.2346\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3598 - acc: 0.2354\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3549 - acc: 0.2374\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3480 - acc: 0.2393\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3435 - acc: 0.2408\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3396 - acc: 0.2422\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3352 - acc: 0.2433\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3320 - acc: 0.2443\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3278 - acc: 0.2459\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3249 - acc: 0.2469\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3219 - acc: 0.2479\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3184 - acc: 0.2490\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3141 - acc: 0.2503\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3124 - acc: 0.2506\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3088 - acc: 0.2518\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3058 - acc: 0.2527\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3024 - acc: 0.2540\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2982 - acc: 0.2554\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2980 - acc: 0.2555\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2937 - acc: 0.2569\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2908 - acc: 0.2577\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2887 - acc: 0.2585\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2859 - acc: 0.2596\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2846 - acc: 0.2602\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2811 - acc: 0.2611\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2819 - acc: 0.2608\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2788 - acc: 0.2619\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2753 - acc: 0.2630\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2728 - acc: 0.2637\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2707 - acc: 0.2647\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2683 - acc: 0.2655\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2659 - acc: 0.2667\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2688 - acc: 0.2655\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2644 - acc: 0.2670\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2616 - acc: 0.2680\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2592 - acc: 0.2689\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2581 - acc: 0.2690\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2564 - acc: 0.2698\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2547 - acc: 0.2703\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2538 - acc: 0.2708\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2523 - acc: 0.2712\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2498 - acc: 0.2721\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2497 - acc: 0.2722\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2471 - acc: 0.2731\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2470 - acc: 0.2732\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2447 - acc: 0.2741\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2453 - acc: 0.2737\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2434 - acc: 0.2744\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2405 - acc: 0.2753\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2391 - acc: 0.2760\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2373 - acc: 0.2766\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2376 - acc: 0.2764\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2379 - acc: 0.2767\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2354 - acc: 0.2773\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2341 - acc: 0.2778\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2318 - acc: 0.2787\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2317 - acc: 0.2789\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2320 - acc: 0.2787\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2284 - acc: 0.2796\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2287 - acc: 0.2798\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2265 - acc: 0.2803\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2254 - acc: 0.2809\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2249 - acc: 0.2812\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2250 - acc: 0.2811\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2238 - acc: 0.2815\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2204 - acc: 0.2827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.04      0.08      1238\n",
      "           1       0.60      0.68      0.64     21760\n",
      "           2       0.34      0.29      0.31      3879\n",
      "           3       0.73      0.75      0.74     33826\n",
      "           4       0.44      0.19      0.26       639\n",
      "           5       0.47      0.51      0.49     21220\n",
      "           6       0.36      0.18      0.24      8895\n",
      "           7       0.40      0.41      0.40     11687\n",
      "\n",
      "    accuracy                           0.57    103144\n",
      "   macro avg       0.50      0.38      0.39    103144\n",
      "weighted avg       0.56      0.57      0.56    103144\n",
      "\n",
      "Acur√°cia\n",
      "0.38101333578110685\n",
      "Precisao\n",
      "0.5594671220147064\n",
      "Recall\n",
      "0.5696405025983091\n",
      "F1\n",
      "0.5576181262075469\n",
      "[[   53   222    42   164     2   514    84   157]\n",
      " [    7 14803   250  2405    15  3030   362   888]\n",
      " [    0   535  1108   779     6   771   130   550]\n",
      " [    1  3075   544 25416    87  2442   352  1909]\n",
      " [    0    77    14   296   119    64    14    55]\n",
      " [   15  3531   540  2722    22 10864  1226  2300]\n",
      " [    2  1320   305  1135    11  3122  1596  1404]\n",
      " [    4  1185   497  2123    10  2366   706  4796]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_28 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_30 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_31 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_32 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 1,548,808\n",
      "Trainable params: 1,548,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.37620547233751583, 0.3616369102715842, 0.4045777853983672, 0.38414956515706955, 0.38101333578110685]\n",
      "0.3815166137891287\n",
      "Precision total\n",
      "[0.5444429480042022, 0.5448686072738022, 0.5665625540634561, 0.5526030514563944, 0.5594671220147064]\n",
      "0.5535888565625123\n",
      "Recalls total\n",
      "[0.5530461117984894, 0.5562294490336034, 0.5726450748696359, 0.5601678405514761, 0.5696405025983091]\n",
      "0.5623457957703029\n",
      "F1 total\n",
      "[0.5404005461392015, 0.5403885523459083, 0.5612860414992079, 0.5483999373120415, 0.5576181262075469]\n",
      "0.5496186407007813\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accuq8)\n",
    "a = np.array(accuq8)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "p = np.array(precisionsq8)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "r = np.array(recallsq8)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1q8)\n",
    "f = np.array(f1q8)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
