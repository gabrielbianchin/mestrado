{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn = criarRede, epochs = 50, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 50, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "previsores_trei_vald, previsores_testes, classes_trei_vald, classes_testes = train_test_split(previsores, classes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 235s 184ms/step - loss: 0.9753 - acc: 0.5238\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 241s 188ms/step - loss: 0.8782 - acc: 0.5989\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 243s 190ms/step - loss: 0.8403 - acc: 0.6231\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 243s 189ms/step - loss: 0.8309 - acc: 0.6286\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.8171 - acc: 0.6352\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.8053 - acc: 0.6430\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.8032 - acc: 0.6433\n",
      "Epoch 8/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.7911 - acc: 0.6496\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 245s 192ms/step - loss: 0.7967 - acc: 0.6480\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.7860 - acc: 0.6519\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.7850 - acc: 0.6520\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 249s 195ms/step - loss: 0.7791 - acc: 0.6551\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 249s 194ms/step - loss: 0.7721 - acc: 0.6581\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 250s 195ms/step - loss: 0.7662 - acc: 0.6614\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 253s 198ms/step - loss: 0.7647 - acc: 0.6627\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 250s 195ms/step - loss: 0.7640 - acc: 0.6627\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 253s 197ms/step - loss: 0.7567 - acc: 0.6664\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 250s 196ms/step - loss: 0.7521 - acc: 0.6688\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 258s 202ms/step - loss: 0.7620 - acc: 0.6643\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 250s 195ms/step - loss: 0.7435 - acc: 0.6743\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 250s 196ms/step - loss: 0.7405 - acc: 0.6750\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 251s 196ms/step - loss: 0.7380 - acc: 0.6751\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 256s 200ms/step - loss: 0.7240 - acc: 0.6833\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 256s 200ms/step - loss: 0.7243 - acc: 0.6825\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 252s 197ms/step - loss: 0.7131 - acc: 0.6877\n",
      "Epoch 26/50\n",
      "1280/1280 [==============================] - 252s 197ms/step - loss: 0.7119 - acc: 0.6877\n",
      "Epoch 27/50\n",
      "1280/1280 [==============================] - 253s 197ms/step - loss: 0.6989 - acc: 0.6940\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 250s 195ms/step - loss: 0.6972 - acc: 0.6962\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 252s 197ms/step - loss: 0.6846 - acc: 0.7015\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 252s 197ms/step - loss: 0.6747 - acc: 0.7059\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 252s 197ms/step - loss: 0.6667 - acc: 0.7093\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 252s 197ms/step - loss: 0.6539 - acc: 0.7157\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 252s 197ms/step - loss: 0.6494 - acc: 0.7172\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 254s 198ms/step - loss: 0.6420 - acc: 0.7211\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 253s 198ms/step - loss: 0.6291 - acc: 0.7270\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 253s 197ms/step - loss: 0.6199 - acc: 0.7325\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 253s 198ms/step - loss: 0.6204 - acc: 0.7302\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 254s 198ms/step - loss: 0.6146 - acc: 0.7344\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 253s 198ms/step - loss: 0.6012 - acc: 0.7403\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 255s 199ms/step - loss: 0.5870 - acc: 0.7468\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 255s 200ms/step - loss: 0.5771 - acc: 0.7510\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 252s 197ms/step - loss: 0.5717 - acc: 0.7546\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 259s 202ms/step - loss: 0.5603 - acc: 0.7590\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 257s 201ms/step - loss: 0.5498 - acc: 0.7634\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 258s 201ms/step - loss: 0.5404 - acc: 0.7685\n",
      "Epoch 46/50\n",
      "1280/1280 [==============================] - 259s 202ms/step - loss: 0.5358 - acc: 0.7696\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 258s 201ms/step - loss: 0.5255 - acc: 0.7754\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 257s 201ms/step - loss: 0.5194 - acc: 0.7780\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 257s 200ms/step - loss: 0.5168 - acc: 0.7801\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 258s 201ms/step - loss: 0.5081 - acc: 0.7829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.72      0.69     35388\n",
      "           1       0.63      0.56      0.60     20057\n",
      "           2       0.71      0.67      0.69     29836\n",
      "\n",
      "    accuracy                           0.67     85281\n",
      "   macro avg       0.67      0.65      0.66     85281\n",
      "weighted avg       0.67      0.67      0.67     85281\n",
      "\n",
      "Acur√°cia\n",
      "0.6530079220725975\n",
      "Precisao\n",
      "0.6683259032817873\n",
      "Recall\n",
      "0.6678275348553605\n",
      "F1\n",
      "0.6667076678306993\n",
      "[[25563  4182  5643]\n",
      " [ 6200 11295  2562]\n",
      " [ 7372  2369 20095]]\n",
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.6110 - acc: 0.7438\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.5815 - acc: 0.7555\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 244s 190ms/step - loss: 0.5600 - acc: 0.7635\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.5492 - acc: 0.7669\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.5400 - acc: 0.7704\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.5288 - acc: 0.7755\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.5195 - acc: 0.7796\n",
      "Epoch 8/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.5115 - acc: 0.7831\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 245s 192ms/step - loss: 0.4970 - acc: 0.7885\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.4922 - acc: 0.7912\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.4883 - acc: 0.7937\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.4853 - acc: 0.7939\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.4751 - acc: 0.7987\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.4679 - acc: 0.8027\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.4628 - acc: 0.8035\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 244s 190ms/step - loss: 0.4598 - acc: 0.8058\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.4535 - acc: 0.8080\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.4486 - acc: 0.8095\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.4394 - acc: 0.8135\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 242s 189ms/step - loss: 0.4359 - acc: 0.8156\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 243s 190ms/step - loss: 0.4332 - acc: 0.8178\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 244s 190ms/step - loss: 0.4268 - acc: 0.8197\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 245s 192ms/step - loss: 0.4248 - acc: 0.8205\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.4294 - acc: 0.8189\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.4256 - acc: 0.8201\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 243s 190ms/step - loss: 0.4153 - acc: 0.8245\n",
      "Epoch 27/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.4061 - acc: 0.8295\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.4023 - acc: 0.8303\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 243s 190ms/step - loss: 0.3990 - acc: 0.8321\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 243s 190ms/step - loss: 0.3928 - acc: 0.8353\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.3917 - acc: 0.8357\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3861 - acc: 0.8380\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3872 - acc: 0.8371\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3836 - acc: 0.8391\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 245s 192ms/step - loss: 0.3802 - acc: 0.8413\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.3732 - acc: 0.8434\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3695 - acc: 0.8454\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3664 - acc: 0.8467\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 245s 192ms/step - loss: 0.3642 - acc: 0.8473\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3621 - acc: 0.8489\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3622 - acc: 0.8485\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 244s 190ms/step - loss: 0.3579 - acc: 0.8499\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3546 - acc: 0.8518\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 244s 191ms/step - loss: 0.3539 - acc: 0.8524\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3535 - acc: 0.8528\n",
      "Epoch 46/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3487 - acc: 0.8545\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3473 - acc: 0.8557\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3433 - acc: 0.8568\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3408 - acc: 0.8584\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3400 - acc: 0.8582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73     34326\n",
      "           1       0.72      0.67      0.70     18074\n",
      "           2       0.80      0.79      0.79     30283\n",
      "\n",
      "    accuracy                           0.75     82683\n",
      "   macro avg       0.74      0.74      0.74     82683\n",
      "weighted avg       0.75      0.75      0.75     82683\n",
      "\n",
      "Acur√°cia\n",
      "0.7355175516924017\n",
      "Precisao\n",
      "0.7461575764942763\n",
      "Recall\n",
      "0.7454736765719676\n",
      "F1\n",
      "0.7454009772782282\n",
      "[[25677  3790  4859]\n",
      " [ 4780 12150  1144]\n",
      " [ 5626   846 23811]]\n",
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 246s 193ms/step - loss: 0.4506 - acc: 0.8177\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.4211 - acc: 0.8269\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.4061 - acc: 0.8314\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3942 - acc: 0.8365\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3868 - acc: 0.8400\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 245s 192ms/step - loss: 0.3788 - acc: 0.8432\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3764 - acc: 0.8432\n",
      "Epoch 8/50\n",
      "1280/1280 [==============================] - 245s 191ms/step - loss: 0.3708 - acc: 0.8462\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3638 - acc: 0.8496\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3600 - acc: 0.8506\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3565 - acc: 0.8518\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3571 - acc: 0.8521\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3511 - acc: 0.8549\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 248s 193ms/step - loss: 0.3484 - acc: 0.8555\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 246s 193ms/step - loss: 0.3430 - acc: 0.8571\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 245s 192ms/step - loss: 0.3413 - acc: 0.8588\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3389 - acc: 0.8602\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3380 - acc: 0.8603\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3348 - acc: 0.8612\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3335 - acc: 0.8619\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3316 - acc: 0.8629\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3251 - acc: 0.8654\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3232 - acc: 0.8658\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3205 - acc: 0.8673\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3226 - acc: 0.8672\n",
      "Epoch 26/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3188 - acc: 0.8681\n",
      "Epoch 27/50\n",
      "1280/1280 [==============================] - 249s 194ms/step - loss: 0.3182 - acc: 0.8694\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3143 - acc: 0.8699\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 249s 195ms/step - loss: 0.3108 - acc: 0.8713\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3110 - acc: 0.8714\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3100 - acc: 0.8720\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3075 - acc: 0.8734\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3041 - acc: 0.8747\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3016 - acc: 0.8762\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 249s 195ms/step - loss: 0.3031 - acc: 0.8753\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3047 - acc: 0.8745\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3017 - acc: 0.8755\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2961 - acc: 0.8782\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 248s 193ms/step - loss: 0.2922 - acc: 0.8799\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2904 - acc: 0.8806\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2908 - acc: 0.8803\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 249s 195ms/step - loss: 0.2902 - acc: 0.8808\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 249s 194ms/step - loss: 0.2887 - acc: 0.8809\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2868 - acc: 0.8818\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2826 - acc: 0.8841\n",
      "Epoch 46/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2841 - acc: 0.8835\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 249s 194ms/step - loss: 0.2803 - acc: 0.8845\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2795 - acc: 0.8860\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2794 - acc: 0.8854\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2850 - acc: 0.8831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80     33419\n",
      "           1       0.81      0.72      0.77     16804\n",
      "           2       0.88      0.86      0.87     31118\n",
      "\n",
      "    accuracy                           0.82     81341\n",
      "   macro avg       0.82      0.81      0.81     81341\n",
      "weighted avg       0.82      0.82      0.82     81341\n",
      "\n",
      "Acur√°cia\n",
      "0.8055254525938572\n",
      "Precisao\n",
      "0.8216535097472362\n",
      "Recall\n",
      "0.8203489015379698\n",
      "F1\n",
      "0.8201215477172763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27643  2408  3368]\n",
      " [ 4151 12179   474]\n",
      " [ 3832   380 26906]]\n",
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3628 - acc: 0.8528\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3389 - acc: 0.8619\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3223 - acc: 0.8670\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3136 - acc: 0.8716\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3102 - acc: 0.8727\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.3050 - acc: 0.8749\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.3005 - acc: 0.8767\n",
      "Epoch 8/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2979 - acc: 0.8778\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2985 - acc: 0.8776\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2910 - acc: 0.8803\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 248s 193ms/step - loss: 0.2907 - acc: 0.8807\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 248s 193ms/step - loss: 0.2874 - acc: 0.8816\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2871 - acc: 0.8819\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2861 - acc: 0.8829\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 248s 193ms/step - loss: 0.2827 - acc: 0.8837\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 249s 194ms/step - loss: 0.2831 - acc: 0.8842\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 248s 193ms/step - loss: 0.2777 - acc: 0.8857\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2776 - acc: 0.8866\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2741 - acc: 0.8872\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 249s 194ms/step - loss: 0.2737 - acc: 0.8882\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2731 - acc: 0.8879\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2726 - acc: 0.8884\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2711 - acc: 0.8892\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 248s 193ms/step - loss: 0.2668 - acc: 0.8901\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2688 - acc: 0.8898\n",
      "Epoch 26/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2693 - acc: 0.8894\n",
      "Epoch 27/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2673 - acc: 0.8906\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2648 - acc: 0.8917\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2609 - acc: 0.8931\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.2622 - acc: 0.8925\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2622 - acc: 0.8923\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.2582 - acc: 0.8950\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2554 - acc: 0.8958\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2564 - acc: 0.8952\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2586 - acc: 0.8945\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2554 - acc: 0.8952\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2527 - acc: 0.8964\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.2518 - acc: 0.8973\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2504 - acc: 0.8974\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.2491 - acc: 0.8987\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2497 - acc: 0.8981\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 249s 194ms/step - loss: 0.2503 - acc: 0.8983\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2485 - acc: 0.8981\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2482 - acc: 0.8982\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 248s 194ms/step - loss: 0.2449 - acc: 0.9003\n",
      "Epoch 46/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2451 - acc: 0.8999\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2445 - acc: 0.8997\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2455 - acc: 0.8994\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2448 - acc: 0.8998\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 247s 193ms/step - loss: 0.2438 - acc: 0.9001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84     32761\n",
      "           1       0.86      0.80      0.83     18387\n",
      "           2       0.91      0.91      0.91     30142\n",
      "\n",
      "    accuracy                           0.87     81290\n",
      "   macro avg       0.87      0.86      0.86     81290\n",
      "weighted avg       0.87      0.87      0.87     81290\n",
      "\n",
      "Acur√°cia\n",
      "0.8581201841283845\n",
      "Precisao\n",
      "0.8667008110024461\n",
      "Recall\n",
      "0.8659613728625907\n",
      "F1\n",
      "0.865956057625578\n",
      "[[28255  2160  2346]\n",
      " [ 3338 14799   250]\n",
      " [ 2585   217 27340]]\n",
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 246s 192ms/step - loss: 0.3032 - acc: 0.8775\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 246s 193ms/step - loss: 0.2922 - acc: 0.8808\n",
      "Epoch 3/50\n",
      "1056/1280 [=======================>......] - ETA: 43s - loss: 0.2780 - acc: 0.8862"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "model = criarRede()\n",
    "\n",
    "for train_index, test_index in kf.split(previsores_trei_vald):\n",
    "  \n",
    "  train_and_evaluate_model(model, previsores_trei_vald[train_index], classes_trei_vald[train_index],\n",
    "                           previsores_trei_vald[test_index], classes_trei_vald[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 700, 20)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 700, 200)          96800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 700, 200)          240800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 700, 200)          240800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 579,003\n",
      "Trainable params: 579,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.4983293988761603, 0.5385032668352153]\n",
      "Precision total\n",
      "[0.5431734577221526, 0.579448819714287]\n",
      "Recalls total\n",
      "[0.5495433504316277, 0.585276697226186]\n",
      "F1 total\n",
      "[0.5294485440868589, 0.5679336472036717]\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accu)\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "print('F1 total')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 438s 273ms/step - loss: 0.8695 - acc: 0.6037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.73      0.67     41905\n",
      "           1       0.59      0.40      0.48     22884\n",
      "           2       0.64      0.63      0.64     38573\n",
      "\n",
      "    accuracy                           0.62    103362\n",
      "   macro avg       0.62      0.59      0.59    103362\n",
      "weighted avg       0.62      0.62      0.61    103362\n",
      "\n",
      "Acur√°cia\n",
      "0.5880482947706657\n",
      "Precisao\n",
      "0.6193174865503472\n",
      "Recall\n",
      "0.6208858187728565\n",
      "F1\n",
      "0.6140843943794403\n",
      "[[30590  3373  7942]\n",
      " [ 7852  9196  5836]\n",
      " [11232  2951 24390]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model(model, previsores_trei_vald, classes_trei_vald,\n",
    "                           previsores_testes, classes_testes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
