{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 12:38:34.553101  9516 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 12:38:34.558100  9516 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 12:38:34.559084  9516 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 12:38:34.560082  9516 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    3 ... 1996 1997 1998] TEST: [   2   10   14   17   19   25   26   28   33   35   39   49   50   53\n",
      "   54   60   61   63   65   71   75   77   79   86   95   98   99  118\n",
      "  119  122  124  126  131  132  133  143  150  151  152  153  163  168\n",
      "  169  171  175  179  189  198  202  210  225  229  231  232  241  242\n",
      "  254  258  259  264  266  268  269  270  278  282  291  292  294  308\n",
      "  310  322  329  330  332  333  336  337  345  349  357  363  365  368\n",
      "  371  376  377  383  385  399  400  409  410  411  422  431  433  434\n",
      "  436  442  444  453  455  460  464  466  474  475  484  487  489  498\n",
      "  504  505  510  516  519  526  528  532  541  549  550  553  562  566\n",
      "  571  573  579  580  586  587  588  592  603  607  613  615  616  625\n",
      "  636  637  643  644  645  646  653  660  663  665  670  674  683  689\n",
      "  695  696  705  708  711  714  720  723  729  730  741  742  744  748\n",
      "  749  751  758  761  777  785  791  794  798  801  806  815  824  825\n",
      "  832  839  840  841  842  844  845  848  849  853  858  860  873  874\n",
      "  880  882  886  888  892  901  908  914  923  924  927  928  935  942\n",
      "  944  948  953  969  972  975  980  985  989  998  999 1010 1013 1018\n",
      " 1020 1021 1022 1023 1024 1026 1033 1034 1035 1046 1049 1050 1055 1061\n",
      " 1062 1064 1077 1078 1083 1088 1089 1092 1095 1103 1105 1110 1126 1140\n",
      " 1146 1152 1157 1168 1171 1175 1178 1182 1188 1193 1197 1199 1205 1208\n",
      " 1221 1229 1236 1238 1242 1243 1247 1248 1257 1259 1261 1265 1268 1273\n",
      " 1275 1283 1288 1298 1299 1304 1314 1330 1332 1337 1338 1351 1357 1359\n",
      " 1367 1371 1396 1398 1400 1404 1405 1407 1412 1414 1417 1422 1425 1430\n",
      " 1431 1433 1436 1441 1443 1444 1448 1452 1456 1458 1464 1465 1467 1470\n",
      " 1483 1496 1509 1516 1524 1533 1536 1543 1548 1551 1555 1556 1557 1563\n",
      " 1571 1579 1606 1611 1648 1649 1653 1661 1669 1680 1684 1695 1696 1698\n",
      " 1701 1704 1713 1723 1726 1727 1730 1735 1745 1747 1753 1755 1756 1780\n",
      " 1789 1790 1801 1802 1805 1808 1811 1821 1825 1835 1843 1852 1865 1882\n",
      " 1885 1903 1917 1925 1927 1930 1945 1951 1952 1953 1955 1959 1973 1975\n",
      " 1976 1978 1982 1990 1991 1993 1995 1999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 12:38:35.617525  9516 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.6181 - acc: 0.1421\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5602 - acc: 0.1618\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5326 - acc: 0.1723\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5204 - acc: 0.1778\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5141 - acc: 0.1813\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5095 - acc: 0.1831\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5046 - acc: 0.1857\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4999 - acc: 0.1879\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4998 - acc: 0.1874\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4921 - acc: 0.1908\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4887 - acc: 0.1923\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4849 - acc: 0.1937\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4824 - acc: 0.1945\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4815 - acc: 0.1947\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4768 - acc: 0.1968\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4728 - acc: 0.1985\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4709 - acc: 0.1989\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4667 - acc: 0.2007\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4621 - acc: 0.2022\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4604 - acc: 0.2026\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4562 - acc: 0.2045\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4524 - acc: 0.2063\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4450 - acc: 0.2088\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4407 - acc: 0.2103\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4530 - acc: 0.2058\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4414 - acc: 0.2107\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4327 - acc: 0.2139\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4252 - acc: 0.2160\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4229 - acc: 0.2166\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4172 - acc: 0.2185\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4147 - acc: 0.2191\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4081 - acc: 0.2216\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4031 - acc: 0.2235\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3997 - acc: 0.2242\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3935 - acc: 0.2263\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3903 - acc: 0.2274\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3868 - acc: 0.2286\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3844 - acc: 0.2290\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3776 - acc: 0.2316\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3739 - acc: 0.2326\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3722 - acc: 0.2332\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3672 - acc: 0.2350\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3632 - acc: 0.2362\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3609 - acc: 0.2373\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3576 - acc: 0.2385\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3538 - acc: 0.2394\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3484 - acc: 0.2411\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3468 - acc: 0.2420\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3442 - acc: 0.2421\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3394 - acc: 0.2442\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3366 - acc: 0.2445\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3336 - acc: 0.2458\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3321 - acc: 0.2462\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3310 - acc: 0.2466\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3246 - acc: 0.2486\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3243 - acc: 0.2485\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3215 - acc: 0.2494\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3190 - acc: 0.2504\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3190 - acc: 0.2501\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3284 - acc: 0.2469\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3141 - acc: 0.2517\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3105 - acc: 0.2530\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3090 - acc: 0.2532\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3072 - acc: 0.2541\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3055 - acc: 0.2547\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3030 - acc: 0.2554\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3005 - acc: 0.2562\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2988 - acc: 0.2570\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2974 - acc: 0.2573\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2956 - acc: 0.2580\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2928 - acc: 0.2590\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2926 - acc: 0.2588\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2910 - acc: 0.2594\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2892 - acc: 0.2599\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2872 - acc: 0.2606\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2859 - acc: 0.2612\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2846 - acc: 0.2615\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2839 - acc: 0.2617\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2815 - acc: 0.2627\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2823 - acc: 0.2625\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2790 - acc: 0.2634\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2779 - acc: 0.2640\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2761 - acc: 0.2644\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2759 - acc: 0.2647\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2768 - acc: 0.2643\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2738 - acc: 0.2654\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2755 - acc: 0.2646\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2737 - acc: 0.2654\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2699 - acc: 0.2666\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2682 - acc: 0.2670\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2671 - acc: 0.2677\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2650 - acc: 0.2682\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2643 - acc: 0.2683\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2638 - acc: 0.2686\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2634 - acc: 0.2686\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2635 - acc: 0.2688\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2633 - acc: 0.2686\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2609 - acc: 0.2696\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2604 - acc: 0.2700\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2580 - acc: 0.2706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.01      0.02      1224\n",
      "           1       0.59      0.63      0.61     21473\n",
      "           2       0.32      0.25      0.28      3858\n",
      "           3       0.70      0.74      0.72     34141\n",
      "           4       0.56      0.23      0.33       687\n",
      "           5       0.42      0.54      0.47     20512\n",
      "           6       0.35      0.11      0.17      9100\n",
      "           7       0.40      0.37      0.38     11691\n",
      "\n",
      "    accuracy                           0.55    102686\n",
      "   macro avg       0.48      0.36      0.37    102686\n",
      "weighted avg       0.54      0.55      0.53    102686\n",
      "\n",
      "Acurácia\n",
      "0.35915306700300176\n",
      "Precisao\n",
      "0.5358692747285666\n",
      "Recall\n",
      "0.5482441618136844\n",
      "F1\n",
      "0.531212445417416\n",
      "[[   10   273    46   181     1   569    31   113]\n",
      " [    0 13622   241  2851    23  3681   234   821]\n",
      " [    0   470   950   934     7   906    91   500]\n",
      " [    1  3095   483 25155    59  3342   209  1797]\n",
      " [    0   103    17   289   158    53    13    54]\n",
      " [    4  3262   525  2992    11 11133   752  1833]\n",
      " [    5  1147   242  1357     2  4032   994  1321]\n",
      " [    1  1130   421  2391    20  2896   557  4275]]\n",
      "TRAIN: [   0    2    4 ... 1997 1998 1999] TEST: [   1    3    6    7   22   27   34   38   62   67   69   78   83   88\n",
      "   90   93  100  101  105  116  117  123  130  136  139  142  144  148\n",
      "  157  158  161  164  174  178  181  182  187  190  192  197  207  208\n",
      "  222  224  226  227  230  235  240  244  252  253  255  260  261  271\n",
      "  273  276  281  284  285  296  303  304  306  309  312  321  324  325\n",
      "  331  335  342  348  352  358  364  378  379  380  382  386  397  398\n",
      "  413  418  419  420  424  425  427  432  435  437  438  439  446  449\n",
      "  457  459  470  478  483  491  494  496  503  506  512  523  533  536\n",
      "  538  551  552  555  557  560  564  565  567  576  582  590  597  600\n",
      "  618  619  624  626  628  629  631  633  635  639  651  656  658  662\n",
      "  680  686  687  688  700  701  721  724  725  727  728  736  739  745\n",
      "  755  757  759  766  768  782  803  804  807  812  813  817  819  820\n",
      "  828  829  838  846  847  850  851  852  854  856  862  872  878  883\n",
      "  885  889  891  905  911  913  918  925  930  931  936  946  952  955\n",
      "  957  960  964  968  973  974  979  987  992  997 1008 1012 1014 1015\n",
      " 1025 1041 1042 1057 1058 1063 1076 1084 1085 1086 1087 1093 1098 1099\n",
      " 1100 1101 1104 1107 1108 1109 1111 1113 1114 1117 1122 1123 1131 1144\n",
      " 1150 1151 1153 1156 1162 1167 1173 1174 1176 1181 1185 1186 1196 1207\n",
      " 1211 1214 1226 1232 1234 1235 1237 1240 1244 1250 1251 1263 1264 1291\n",
      " 1296 1297 1306 1311 1321 1327 1339 1344 1345 1347 1352 1356 1361 1369\n",
      " 1373 1374 1376 1378 1380 1381 1386 1393 1401 1406 1419 1421 1423 1424\n",
      " 1426 1437 1445 1447 1450 1455 1460 1462 1466 1468 1478 1479 1480 1486\n",
      " 1488 1499 1502 1503 1504 1506 1517 1519 1525 1532 1537 1539 1540 1546\n",
      " 1547 1553 1566 1577 1584 1596 1607 1609 1624 1626 1629 1632 1636 1643\n",
      " 1644 1647 1654 1655 1663 1664 1670 1672 1674 1682 1683 1687 1691 1693\n",
      " 1702 1706 1712 1719 1721 1734 1741 1742 1743 1748 1751 1752 1760 1763\n",
      " 1771 1784 1785 1792 1798 1816 1833 1834 1839 1840 1844 1853 1866 1867\n",
      " 1871 1873 1888 1896 1901 1908 1913 1920 1923 1924 1941 1942 1946 1958\n",
      " 1964 1965 1970 1974 1977 1987 1988 1994]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.6111 - acc: 0.1449\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5592 - acc: 0.1602\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5285 - acc: 0.1721\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5188 - acc: 0.1769\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5117 - acc: 0.1805\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5075 - acc: 0.1823\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5042 - acc: 0.1840\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4973 - acc: 0.1870\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4957 - acc: 0.1877\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4919 - acc: 0.1895\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4873 - acc: 0.1914\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4844 - acc: 0.1923\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4819 - acc: 0.1931\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4781 - acc: 0.1949\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4754 - acc: 0.1958\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4715 - acc: 0.1975\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4680 - acc: 0.1989\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4643 - acc: 0.2004\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4601 - acc: 0.2019\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4560 - acc: 0.2034\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4507 - acc: 0.2052\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4517 - acc: 0.2046\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4434 - acc: 0.2080\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4380 - acc: 0.2099\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4384 - acc: 0.2098\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4318 - acc: 0.2119\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4276 - acc: 0.2133\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4220 - acc: 0.2156\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4151 - acc: 0.2176\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4110 - acc: 0.2191\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4061 - acc: 0.2205\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4019 - acc: 0.2220\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3988 - acc: 0.2229\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3964 - acc: 0.2237\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3895 - acc: 0.2261\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3852 - acc: 0.2276\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3798 - acc: 0.2291\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3769 - acc: 0.2302\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3726 - acc: 0.2316\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3701 - acc: 0.2322\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3649 - acc: 0.2342\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3614 - acc: 0.2352\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3579 - acc: 0.2363\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3567 - acc: 0.2363\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3529 - acc: 0.2377\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3492 - acc: 0.2393\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3451 - acc: 0.2405\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3429 - acc: 0.2409\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3415 - acc: 0.2416\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3376 - acc: 0.2428\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3334 - acc: 0.2441\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3312 - acc: 0.2447\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3288 - acc: 0.2456\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3255 - acc: 0.2466\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3242 - acc: 0.2471\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3208 - acc: 0.2482\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3177 - acc: 0.2491\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3164 - acc: 0.2495\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3156 - acc: 0.2497\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3133 - acc: 0.2505\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3112 - acc: 0.2513\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3092 - acc: 0.2519\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3070 - acc: 0.2527\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3050 - acc: 0.2533\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3018 - acc: 0.2547\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3007 - acc: 0.2551\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3000 - acc: 0.2551\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2994 - acc: 0.2553\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2985 - acc: 0.2555\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2957 - acc: 0.2567\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2913 - acc: 0.2580\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2900 - acc: 0.2585\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2893 - acc: 0.2585\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2874 - acc: 0.2595\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2867 - acc: 0.2597\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2851 - acc: 0.2603\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2832 - acc: 0.2608\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2820 - acc: 0.2612\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2799 - acc: 0.2620\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2807 - acc: 0.2617\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2795 - acc: 0.2619\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2780 - acc: 0.2628\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2754 - acc: 0.2634\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2735 - acc: 0.2640\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2791 - acc: 0.2622\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2771 - acc: 0.2631\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2718 - acc: 0.2649\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2700 - acc: 0.2654\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2690 - acc: 0.2657\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2679 - acc: 0.2663\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2656 - acc: 0.2669\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2657 - acc: 0.2670\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2651 - acc: 0.2670\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2633 - acc: 0.2678\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2621 - acc: 0.2682\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2623 - acc: 0.2679\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2626 - acc: 0.2679\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2607 - acc: 0.2688\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2589 - acc: 0.2692\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2604 - acc: 0.2688\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.00      0.00      1296\n",
      "           1       0.62      0.62      0.62     21927\n",
      "           2       0.35      0.26      0.30      4131\n",
      "           3       0.70      0.76      0.73     33686\n",
      "           4       0.61      0.25      0.35       721\n",
      "           5       0.43      0.54      0.48     21248\n",
      "           6       0.33      0.10      0.16      9246\n",
      "           7       0.37      0.39      0.38     11842\n",
      "\n",
      "    accuracy                           0.55    104097\n",
      "   macro avg       0.48      0.37      0.38    104097\n",
      "weighted avg       0.54      0.55      0.54    104097\n",
      "\n",
      "Acurácia\n",
      "0.3657686597248555\n",
      "Precisao\n",
      "0.539195778418488\n",
      "Recall\n",
      "0.5525903724410886\n",
      "F1\n",
      "0.5355347926101575\n",
      "[[    3   233    33   201     1   622    44   159]\n",
      " [    2 13599   286  2891    20  3868   241  1020]\n",
      " [    0   399  1091   895     5  1009    99   633]\n",
      " [    0  2247   488 25588    52  3033   235  2043]\n",
      " [    0    70     4   341   178    73     8    47]\n",
      " [    1  3237   501  2922    12 11521   778  2276]\n",
      " [    1  1196   280  1275     5  3921   960  1608]\n",
      " [    0  1107   424  2368    17  2792   551  4583]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1996 1998 1999] TEST: [  11   15   18   20   21   30   36   43   44   45   48   56   59   64\n",
      "   66   70   72   76   81   87   89   91   92   94   96  109  110  114\n",
      "  115  120  134  135  137  140  145  146  147  154  162  166  177  180\n",
      "  184  193  205  217  218  219  223  228  233  237  238  245  248  250\n",
      "  275  277  280  283  286  287  297  298  307  311  315  319  320  327\n",
      "  340  341  343  347  353  354  356  369  370  374  375  381  387  390\n",
      "  391  394  403  406  408  421  426  428  429  430  440  443  448  450\n",
      "  451  465  473  477  481  500  514  515  518  524  527  529  535  537\n",
      "  543  544  547  559  561  575  581  585  594  610  622  627  634  640\n",
      "  641  647  654  655  667  669  673  685  691  692  697  698  699  704\n",
      "  718  726  732  735  738  740  743  746  747  752  754  756  760  767\n",
      "  771  773  774  776  780  781  783  788  790  797  800  802  805  816\n",
      "  818  826  830  833  855  857  859  864  865  890  894  906  910  912\n",
      "  920  921  929  934  938  945  947  949  951  959  962  970  977  984\n",
      "  986  990  994  995  996 1000 1001 1002 1003 1006 1009 1017 1019 1028\n",
      " 1030 1038 1039 1040 1044 1045 1047 1052 1053 1059 1066 1072 1074 1080\n",
      " 1082 1106 1115 1116 1125 1128 1133 1137 1138 1139 1149 1154 1158 1161\n",
      " 1177 1179 1190 1192 1195 1202 1204 1206 1209 1219 1222 1223 1225 1233\n",
      " 1246 1253 1262 1274 1276 1281 1285 1286 1287 1289 1290 1293 1294 1309\n",
      " 1315 1320 1322 1325 1326 1328 1334 1343 1350 1354 1360 1365 1368 1377\n",
      " 1383 1388 1395 1411 1415 1427 1428 1429 1440 1446 1449 1463 1471 1474\n",
      " 1485 1487 1489 1492 1493 1497 1498 1510 1512 1514 1520 1522 1529 1534\n",
      " 1538 1552 1558 1573 1576 1582 1589 1593 1594 1597 1601 1602 1605 1613\n",
      " 1616 1617 1625 1628 1638 1650 1651 1656 1657 1659 1668 1689 1694 1699\n",
      " 1707 1710 1711 1714 1717 1720 1725 1733 1736 1738 1744 1746 1750 1754\n",
      " 1762 1766 1770 1774 1775 1776 1778 1793 1812 1813 1814 1819 1820 1823\n",
      " 1827 1845 1847 1851 1854 1863 1864 1868 1870 1872 1875 1876 1880 1881\n",
      " 1884 1886 1892 1895 1900 1904 1911 1912 1915 1918 1928 1929 1931 1935\n",
      " 1937 1938 1967 1968 1969 1984 1989 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.6132 - acc: 0.1465\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5595 - acc: 0.1623\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5337 - acc: 0.1722\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5225 - acc: 0.1772\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5151 - acc: 0.1803\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5073 - acc: 0.1844\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5027 - acc: 0.1868\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4984 - acc: 0.1883\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4953 - acc: 0.1894\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4909 - acc: 0.1913\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4866 - acc: 0.1932\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4858 - acc: 0.1938\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4808 - acc: 0.1955\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4805 - acc: 0.1956\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4774 - acc: 0.1966\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4736 - acc: 0.1981\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4688 - acc: 0.2002\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4653 - acc: 0.2015\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4630 - acc: 0.2028\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4580 - acc: 0.2042\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4555 - acc: 0.2056\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4506 - acc: 0.2073\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4446 - acc: 0.2094\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4422 - acc: 0.2101\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4371 - acc: 0.2118\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4327 - acc: 0.2132\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4282 - acc: 0.2149\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4239 - acc: 0.2164\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4183 - acc: 0.2181\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4136 - acc: 0.2199\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4092 - acc: 0.2215\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4098 - acc: 0.2208\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4148 - acc: 0.2193\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3998 - acc: 0.2245\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3945 - acc: 0.2261\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3892 - acc: 0.2283\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3854 - acc: 0.2290\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3806 - acc: 0.2308\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3771 - acc: 0.2319\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3745 - acc: 0.2325\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3689 - acc: 0.2343\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3657 - acc: 0.2353\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3610 - acc: 0.2370\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3578 - acc: 0.2378\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3548 - acc: 0.2388\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3522 - acc: 0.2395\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3480 - acc: 0.2410\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3447 - acc: 0.2417\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3427 - acc: 0.2425\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3425 - acc: 0.2426\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3379 - acc: 0.2441\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3331 - acc: 0.2458\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3313 - acc: 0.2461\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3281 - acc: 0.2471\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3268 - acc: 0.2477\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3250 - acc: 0.2481\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3226 - acc: 0.2489\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3198 - acc: 0.2501\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3169 - acc: 0.2508\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3162 - acc: 0.2511\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3132 - acc: 0.2519\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3111 - acc: 0.2528\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3122 - acc: 0.2522\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3085 - acc: 0.2535\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3062 - acc: 0.2543\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3031 - acc: 0.2557\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3019 - acc: 0.2559\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3008 - acc: 0.2563\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2978 - acc: 0.2571\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2957 - acc: 0.2578\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2937 - acc: 0.2585\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2931 - acc: 0.2587\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2907 - acc: 0.2597\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2898 - acc: 0.2599\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2891 - acc: 0.2602\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2860 - acc: 0.2614\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2856 - acc: 0.2614\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2853 - acc: 0.2614\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2837 - acc: 0.2619\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2814 - acc: 0.2627\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2798 - acc: 0.2633\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2789 - acc: 0.2636\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2768 - acc: 0.2643\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2746 - acc: 0.2650\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2751 - acc: 0.2651\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2738 - acc: 0.2655\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2735 - acc: 0.2656\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2710 - acc: 0.2664\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2707 - acc: 0.2668\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2701 - acc: 0.2666\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2680 - acc: 0.2672\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2670 - acc: 0.2675\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2659 - acc: 0.2682\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2640 - acc: 0.2687\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2632 - acc: 0.2693\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2635 - acc: 0.2694\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2621 - acc: 0.2696\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2615 - acc: 0.2695\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2608 - acc: 0.2697\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2580 - acc: 0.2710\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.00      0.01      1292\n",
      "           1       0.64      0.63      0.63     21413\n",
      "           2       0.35      0.28      0.31      3974\n",
      "           3       0.71      0.78      0.74     34089\n",
      "           4       0.56      0.24      0.34       835\n",
      "           5       0.44      0.55      0.49     20486\n",
      "           6       0.35      0.11      0.16      8874\n",
      "           7       0.38      0.40      0.39     11359\n",
      "\n",
      "    accuracy                           0.57    102322\n",
      "   macro avg       0.52      0.37      0.38    102322\n",
      "weighted avg       0.56      0.57      0.55    102322\n",
      "\n",
      "Acurácia\n",
      "0.3730990359060886\n",
      "Precisao\n",
      "0.5569088715949322\n",
      "Recall\n",
      "0.5657434373839448\n",
      "F1\n",
      "0.5486665733108644\n",
      "[[    6   243    37   186     1   632    48   139]\n",
      " [    0 13443   253  2783    16  3624   202  1092]\n",
      " [    0   411  1118   867     2   872    89   615]\n",
      " [    2  2050   583 26423    91  2938   186  1816]\n",
      " [    0    46    15   404   201    84     1    84]\n",
      " [    0  2891   468  2964    18 11174   744  2227]\n",
      " [    0  1051   234  1318    12  3739   954  1566]\n",
      " [    0   944   473  2371    19  2487   496  4569]]\n",
      "TRAIN: [   1    2    3 ... 1996 1997 1999] TEST: [   0    9   12   13   23   24   29   31   32   37   52   57   58   73\n",
      "   74   82   84   85   97  102  103  106  111  125  129  138  155  156\n",
      "  159  160  170  172  176  183  188  194  195  199  200  204  211  212\n",
      "  213  215  216  221  234  236  247  249  256  265  288  289  293  295\n",
      "  299  300  313  323  326  338  344  351  359  360  366  367  372  373\n",
      "  384  388  389  392  393  395  401  402  405  407  412  414  417  423\n",
      "  456  458  461  468  469  476  480  488  490  492  493  497  499  508\n",
      "  511  513  520  521  522  531  539  540  542  548  556  563  569  577\n",
      "  584  589  593  595  596  599  602  605  606  609  611  612  620  623\n",
      "  642  650  657  661  666  668  671  675  678  679  681  682  709  717\n",
      "  719  722  731  733  734  737  764  769  770  772  787  793  795  809\n",
      "  810  821  823  831  834  861  867  868  870  876  877  884  895  900\n",
      "  903  904  907  917  926  932  937  939  950  956  958  967  978  981\n",
      "  982  988  993 1011 1037 1043 1048 1054 1056 1065 1068 1071 1073 1079\n",
      " 1090 1096 1097 1112 1121 1124 1132 1135 1141 1147 1155 1159 1166 1169\n",
      " 1180 1184 1191 1194 1198 1200 1203 1210 1212 1213 1216 1217 1224 1227\n",
      " 1230 1231 1241 1245 1252 1254 1255 1256 1258 1260 1267 1272 1278 1279\n",
      " 1280 1282 1301 1303 1305 1308 1316 1318 1323 1324 1329 1331 1335 1336\n",
      " 1341 1349 1362 1389 1390 1394 1403 1408 1410 1418 1420 1432 1434 1435\n",
      " 1442 1451 1459 1461 1473 1475 1477 1481 1490 1491 1495 1501 1507 1515\n",
      " 1518 1521 1523 1531 1535 1542 1545 1549 1550 1561 1562 1564 1567 1570\n",
      " 1578 1585 1587 1590 1591 1599 1608 1610 1612 1614 1615 1620 1621 1622\n",
      " 1630 1631 1633 1634 1635 1639 1642 1645 1667 1671 1675 1676 1677 1678\n",
      " 1688 1692 1697 1700 1703 1709 1715 1716 1718 1722 1729 1731 1732 1739\n",
      " 1757 1764 1765 1767 1769 1779 1781 1783 1786 1787 1796 1803 1804 1806\n",
      " 1809 1815 1817 1818 1822 1829 1830 1831 1832 1836 1838 1848 1849 1850\n",
      " 1856 1858 1859 1860 1869 1878 1879 1887 1889 1891 1893 1897 1898 1899\n",
      " 1902 1905 1907 1914 1916 1919 1921 1934 1939 1940 1943 1944 1950 1954\n",
      " 1961 1972 1979 1981 1983 1985 1986 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.6271 - acc: 0.1383\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5660 - acc: 0.1609\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5360 - acc: 0.1718\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5220 - acc: 0.1780\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5137 - acc: 0.1825\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5084 - acc: 0.1847\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5027 - acc: 0.1872\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4977 - acc: 0.1893\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4932 - acc: 0.1913\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4907 - acc: 0.1923\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4866 - acc: 0.1935\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4845 - acc: 0.1947\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4802 - acc: 0.1963\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4787 - acc: 0.1973\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4747 - acc: 0.1984\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4713 - acc: 0.2000\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4676 - acc: 0.2013\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4648 - acc: 0.2023\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4618 - acc: 0.2037\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4576 - acc: 0.2054\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4575 - acc: 0.2051\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4493 - acc: 0.2083\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4452 - acc: 0.2099\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4422 - acc: 0.2106\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4388 - acc: 0.2119\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4326 - acc: 0.2141\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4285 - acc: 0.2154\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4232 - acc: 0.2170\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4184 - acc: 0.2193\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4141 - acc: 0.2203\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4094 - acc: 0.2222\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4048 - acc: 0.2234\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3997 - acc: 0.2250\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3944 - acc: 0.2271\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3914 - acc: 0.2277\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3855 - acc: 0.2295\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3839 - acc: 0.2300\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3800 - acc: 0.2314\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3774 - acc: 0.2324\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3697 - acc: 0.2348\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3674 - acc: 0.2355\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3649 - acc: 0.2362\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3590 - acc: 0.2381\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3554 - acc: 0.2392\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3520 - acc: 0.2404\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3503 - acc: 0.2407\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3469 - acc: 0.2420\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3447 - acc: 0.2426\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3412 - acc: 0.2437\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3388 - acc: 0.2444\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3353 - acc: 0.2458\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3317 - acc: 0.2467\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3272 - acc: 0.2483\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3267 - acc: 0.2483\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3234 - acc: 0.2497\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3227 - acc: 0.2496\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3181 - acc: 0.2511\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3161 - acc: 0.2519\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3153 - acc: 0.2522\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3125 - acc: 0.2529\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3106 - acc: 0.2537\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3070 - acc: 0.2549\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3055 - acc: 0.2554\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3039 - acc: 0.2559\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3027 - acc: 0.2566\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2995 - acc: 0.2572\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2978 - acc: 0.2579\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2963 - acc: 0.2583\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2940 - acc: 0.2594\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2936 - acc: 0.2595\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2918 - acc: 0.2599\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2884 - acc: 0.2610\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2867 - acc: 0.2618\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2857 - acc: 0.2621\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2854 - acc: 0.2622\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2842 - acc: 0.2627\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2815 - acc: 0.2636\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2807 - acc: 0.2641\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2800 - acc: 0.2637\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2774 - acc: 0.2650\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2768 - acc: 0.2650\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2753 - acc: 0.2656\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2736 - acc: 0.2661\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2730 - acc: 0.2663\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2725 - acc: 0.2666\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2700 - acc: 0.2675\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2682 - acc: 0.2678\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2677 - acc: 0.2681\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2667 - acc: 0.2685\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2671 - acc: 0.2687\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2649 - acc: 0.2691\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2653 - acc: 0.2690\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2668 - acc: 0.2685\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2631 - acc: 0.2699\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2619 - acc: 0.2705\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2596 - acc: 0.2711\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2590 - acc: 0.2712\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2592 - acc: 0.2710\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2559 - acc: 0.2726\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2550 - acc: 0.2727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.01      0.02      1280\n",
      "           1       0.62      0.66      0.64     22176\n",
      "           2       0.35      0.27      0.30      3866\n",
      "           3       0.72      0.74      0.73     32152\n",
      "           4       0.51      0.27      0.35       655\n",
      "           5       0.44      0.54      0.48     20920\n",
      "           6       0.32      0.14      0.19      9197\n",
      "           7       0.38      0.40      0.39     11385\n",
      "\n",
      "    accuracy                           0.56    101631\n",
      "   macro avg       0.51      0.38      0.39    101631\n",
      "weighted avg       0.55      0.56      0.54    101631\n",
      "\n",
      "Acurácia\n",
      "0.37638366504933796\n",
      "Precisao\n",
      "0.5490842678202248\n",
      "Recall\n",
      "0.5568281331483504\n",
      "F1\n",
      "0.5431847405399131\n",
      "[[   10   257    35   150     0   612    66   150]\n",
      " [    1 14684   196  2462    19  3468   337  1009]\n",
      " [    0   419  1030   783     4   902   142   586]\n",
      " [    0  2633   490 23720    99  3102   332  1776]\n",
      " [    0    93    11   261   176    68     2    44]\n",
      " [    2  3429   450  2564    19 11203  1076  2177]\n",
      " [    0  1183   255  1153     7  3799  1248  1552]\n",
      " [    0  1074   473  2010    22  2569   717  4520]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   4    5    8   16   40   41   42   46   47   51   55   68   80  104\n",
      "  107  108  112  113  121  127  128  141  149  165  167  173  185  186\n",
      "  191  196  201  203  206  209  214  220  239  243  246  251  257  262\n",
      "  263  267  272  274  279  290  301  302  305  314  316  317  318  328\n",
      "  334  339  346  350  355  361  362  396  404  415  416  441  445  447\n",
      "  452  454  462  463  467  471  472  479  482  485  486  495  501  502\n",
      "  507  509  517  525  530  534  545  546  554  558  568  570  572  574\n",
      "  578  583  591  598  601  604  608  614  617  621  630  632  638  648\n",
      "  649  652  659  664  672  676  677  684  690  693  694  702  703  706\n",
      "  707  710  712  713  715  716  750  753  762  763  765  775  778  779\n",
      "  784  786  789  792  796  799  808  811  814  822  827  835  836  837\n",
      "  843  863  866  869  871  875  879  881  887  893  896  897  898  899\n",
      "  902  909  915  916  919  922  933  940  941  943  954  961  963  965\n",
      "  966  971  976  983  991 1004 1005 1007 1016 1027 1029 1031 1032 1036\n",
      " 1051 1060 1067 1069 1070 1075 1081 1091 1094 1102 1118 1119 1120 1127\n",
      " 1129 1130 1134 1136 1142 1143 1145 1148 1160 1163 1164 1165 1170 1172\n",
      " 1183 1187 1189 1201 1215 1218 1220 1228 1239 1249 1266 1269 1270 1271\n",
      " 1277 1284 1292 1295 1300 1302 1307 1310 1312 1313 1317 1319 1333 1340\n",
      " 1342 1346 1348 1353 1355 1358 1363 1364 1366 1370 1372 1375 1379 1382\n",
      " 1384 1385 1387 1391 1392 1397 1399 1402 1409 1413 1416 1438 1439 1453\n",
      " 1454 1457 1469 1472 1476 1482 1484 1494 1500 1505 1508 1511 1513 1526\n",
      " 1527 1528 1530 1541 1544 1554 1559 1560 1565 1568 1569 1572 1574 1575\n",
      " 1580 1581 1583 1586 1588 1592 1595 1598 1600 1603 1604 1618 1619 1623\n",
      " 1627 1637 1640 1641 1646 1652 1658 1660 1662 1665 1666 1673 1679 1681\n",
      " 1685 1686 1690 1705 1708 1724 1728 1737 1740 1749 1758 1759 1761 1768\n",
      " 1772 1773 1777 1782 1788 1791 1794 1795 1797 1799 1800 1807 1810 1824\n",
      " 1826 1828 1837 1841 1842 1846 1855 1857 1861 1862 1874 1877 1883 1890\n",
      " 1894 1906 1909 1910 1922 1926 1932 1933 1936 1947 1948 1949 1956 1957\n",
      " 1960 1962 1963 1966 1971 1980 1992 1996]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.6140 - acc: 0.1429\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5574 - acc: 0.1606\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5288 - acc: 0.1720\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5172 - acc: 0.1772\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5140 - acc: 0.1787\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5077 - acc: 0.1818\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5011 - acc: 0.1843\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4971 - acc: 0.1859\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4925 - acc: 0.1884\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4880 - acc: 0.1902\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4846 - acc: 0.1912\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4848 - acc: 0.1913\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4805 - acc: 0.1931\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4785 - acc: 0.1939\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4738 - acc: 0.1956\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4688 - acc: 0.1973\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4658 - acc: 0.1986\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4630 - acc: 0.1998\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4587 - acc: 0.2014\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4537 - acc: 0.2032\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4507 - acc: 0.2043\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4453 - acc: 0.2063\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4425 - acc: 0.2075\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4422 - acc: 0.2072\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4370 - acc: 0.2089\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4312 - acc: 0.2114\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4282 - acc: 0.2127\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4210 - acc: 0.2155\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4165 - acc: 0.2169\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4108 - acc: 0.2188\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4089 - acc: 0.2196\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4049 - acc: 0.2206\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4015 - acc: 0.2217\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3986 - acc: 0.2223\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3929 - acc: 0.2246\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3881 - acc: 0.2257\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3833 - acc: 0.2272\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3779 - acc: 0.2293\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3765 - acc: 0.2295\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3748 - acc: 0.2297\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3696 - acc: 0.2317\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3628 - acc: 0.2339\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3581 - acc: 0.2353\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3566 - acc: 0.2358\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3538 - acc: 0.2366\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3498 - acc: 0.2381\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3469 - acc: 0.2389\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3437 - acc: 0.2399\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3416 - acc: 0.2406\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3370 - acc: 0.2425\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3345 - acc: 0.2429\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3319 - acc: 0.2436\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3280 - acc: 0.2452\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3271 - acc: 0.2454\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3263 - acc: 0.2455\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3224 - acc: 0.2468\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3221 - acc: 0.2467\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3171 - acc: 0.2485\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3154 - acc: 0.2492\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3131 - acc: 0.2499\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3118 - acc: 0.2503\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3098 - acc: 0.2510\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3072 - acc: 0.2519\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3052 - acc: 0.2523\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3026 - acc: 0.2532\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3006 - acc: 0.2537\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2987 - acc: 0.2544\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2974 - acc: 0.2549\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2951 - acc: 0.2559\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2935 - acc: 0.2562\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2920 - acc: 0.2566\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2916 - acc: 0.2569\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2882 - acc: 0.2581\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2864 - acc: 0.2589\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2854 - acc: 0.2592\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2870 - acc: 0.2586\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2837 - acc: 0.2598\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2828 - acc: 0.2600\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2805 - acc: 0.2611\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2804 - acc: 0.2607\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2780 - acc: 0.2615\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2766 - acc: 0.2623\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2752 - acc: 0.2626\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2744 - acc: 0.2630\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2725 - acc: 0.2638\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2712 - acc: 0.2640\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2703 - acc: 0.2643\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2691 - acc: 0.2646\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2669 - acc: 0.2655\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2655 - acc: 0.2661\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2649 - acc: 0.2662\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2635 - acc: 0.2668\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2661 - acc: 0.2658\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2642 - acc: 0.2662\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2622 - acc: 0.2672\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2608 - acc: 0.2678\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2595 - acc: 0.2683\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2596 - acc: 0.2678\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2577 - acc: 0.2689\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2571 - acc: 0.2689\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.00      0.01      1172\n",
      "           1       0.61      0.63      0.62     21943\n",
      "           2       0.28      0.24      0.26      4042\n",
      "           3       0.70      0.75      0.73     35075\n",
      "           4       0.37      0.26      0.31       619\n",
      "           5       0.42      0.51      0.46     20919\n",
      "           6       0.31      0.10      0.15      9171\n",
      "           7       0.38      0.38      0.38     12131\n",
      "\n",
      "    accuracy                           0.55    105072\n",
      "   macro avg       0.46      0.36      0.36    105072\n",
      "weighted avg       0.54      0.55      0.53    105072\n",
      "\n",
      "Acurácia\n",
      "0.36126651444129454\n",
      "Precisao\n",
      "0.5354863437177183\n",
      "Recall\n",
      "0.5502227044312471\n",
      "F1\n",
      "0.5336637122813451\n",
      "[[    4   235    50   184     1   510    55   133]\n",
      " [    2 13901   302  2981    25  3537   235   960]\n",
      " [    0   438   966   961     8   926    98   645]\n",
      " [    0  2389   649 26476   179  3124   263  1995]\n",
      " [    0    60    16   262   163    69     5    44]\n",
      " [    1  3319   597  3172    22 10751   826  2231]\n",
      " [    0  1203   315  1407    10  3758   919  1559]\n",
      " [    0  1222   574  2431    33  2677   561  4633]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_12 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 582,408\n",
      "Trainable params: 582,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.35915306700300176, 0.3657686597248555, 0.3730990359060886, 0.37638366504933796, 0.36126651444129454]\n",
      "0.36713418842491563\n",
      "Precision total\n",
      "[0.5358692747285666, 0.539195778418488, 0.5569088715949322, 0.5490842678202248, 0.5354863437177183]\n",
      "0.543308907255986\n",
      "Recalls total\n",
      "[0.5482441618136844, 0.5525903724410886, 0.5657434373839448, 0.5568281331483504, 0.5502227044312471]\n",
      "0.554725761843663\n",
      "F1 total\n",
      "[0.531212445417416, 0.5355347926101575, 0.5486665733108644, 0.5431847405399131, 0.5336637122813451]\n",
      "0.5384524528319392\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accu)\n",
    "a = np.array(accu)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "p = np.array(precisions)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "r = np.array(recalls)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f = np.array(f1)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
