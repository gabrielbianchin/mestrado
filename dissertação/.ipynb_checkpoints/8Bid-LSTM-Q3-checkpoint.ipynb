{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 13:03:52.373173  6716 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 13:03:52.381153  6716 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 13:03:52.382150  6716 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 13:03:52.384145  6716 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1995 1998 1999] TEST: [   6    9   10   14   20   22   25   27   29   31   39   40   44   50\n",
      "   67   73   75   80   83   84   86   92  101  106  115  117  118  135\n",
      "  145  150  152  161  166  169  178  187  191  192  193  194  197  198\n",
      "  202  207  210  212  213  214  216  219  220  226  232  233  241  256\n",
      "  257  258  263  274  276  283  284  287  288  292  297  306  311  312\n",
      "  319  322  324  328  329  331  332  334  336  345  349  358  360  364\n",
      "  366  371  375  376  391  393  394  398  401  416  417  426  428  436\n",
      "  437  438  443  450  452  456  460  467  470  471  475  477  490  492\n",
      "  506  508  515  525  527  532  541  549  552  555  557  566  583  591\n",
      "  593  596  598  603  614  624  639  648  652  661  669  671  674  675\n",
      "  676  678  679  682  683  685  687  688  691  694  696  697  698  700\n",
      "  702  705  710  711  737  738  744  751  752  753  756  763  768  771\n",
      "  773  778  779  796  798  800  801  810  813  819  827  834  842  843\n",
      "  850  855  863  869  873  875  885  886  893  900  905  920  929  936\n",
      "  941  942  943  954  963  968  975  977  980  981  982  987  994  997\n",
      " 1008 1010 1011 1013 1016 1019 1027 1037 1038 1039 1053 1055 1062 1063\n",
      " 1067 1073 1076 1077 1080 1081 1082 1083 1087 1092 1102 1107 1118 1119\n",
      " 1122 1138 1148 1151 1153 1157 1168 1175 1178 1179 1180 1181 1193 1194\n",
      " 1204 1208 1212 1217 1227 1238 1241 1242 1248 1249 1250 1251 1256 1264\n",
      " 1281 1289 1292 1294 1295 1302 1312 1328 1334 1338 1340 1341 1350 1358\n",
      " 1362 1364 1375 1380 1386 1391 1397 1400 1401 1404 1406 1407 1412 1419\n",
      " 1422 1425 1428 1437 1443 1447 1452 1460 1467 1472 1488 1490 1494 1498\n",
      " 1509 1519 1551 1558 1560 1564 1569 1576 1588 1598 1601 1606 1611 1613\n",
      " 1617 1625 1626 1630 1638 1644 1649 1656 1665 1674 1675 1682 1684 1686\n",
      " 1688 1690 1691 1692 1704 1707 1708 1713 1714 1720 1721 1726 1739 1741\n",
      " 1742 1750 1758 1759 1770 1772 1779 1786 1787 1789 1791 1792 1806 1809\n",
      " 1813 1825 1826 1830 1833 1835 1841 1847 1855 1861 1864 1867 1869 1874\n",
      " 1878 1879 1883 1885 1897 1907 1910 1913 1916 1925 1930 1931 1937 1946\n",
      " 1962 1971 1972 1974 1984 1989 1996 1997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 13:03:54.489804  6716 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 27s 17ms/sample - loss: 0.3685 - acc: 0.7895\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3363 - acc: 0.8405\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.3147 - acc: 0.8561\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3067 - acc: 0.8615\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.3007 - acc: 0.8648\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2972 - acc: 0.8658\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2950 - acc: 0.8668\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2909 - acc: 0.8690\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2876 - acc: 0.8708\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2871 - acc: 0.8702\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2837 - acc: 0.8720\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.2817 - acc: 0.8735\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2780 - acc: 0.8757\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2819 - acc: 0.8728\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2773 - acc: 0.8752\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2719 - acc: 0.8779\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2700 - acc: 0.8793\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2677 - acc: 0.8800\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2610 - acc: 0.8829\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2586 - acc: 0.8850\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.2547 - acc: 0.8868\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2504 - acc: 0.8880\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2484 - acc: 0.8898\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2445 - acc: 0.8916\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2385 - acc: 0.8934\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2323 - acc: 0.8965\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2259 - acc: 0.8993\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.2222 - acc: 0.9009\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2183 - acc: 0.9028\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.2127 - acc: 0.9060\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2082 - acc: 0.9084\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2040 - acc: 0.9093\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2017 - acc: 0.9099\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1976 - acc: 0.9121\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1930 - acc: 0.9158\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1881 - acc: 0.9175\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1852 - acc: 0.9192\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1832 - acc: 0.9199\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.1783 - acc: 0.9223\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1766 - acc: 0.9228\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1735 - acc: 0.9233\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1693 - acc: 0.9267\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1654 - acc: 0.9275\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1618 - acc: 0.9284\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1620 - acc: 0.9283\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1579 - acc: 0.9310\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1575 - acc: 0.9312\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1538 - acc: 0.9326\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1498 - acc: 0.9349\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1481 - acc: 0.9349\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1471 - acc: 0.9348\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1442 - acc: 0.9366\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1420 - acc: 0.9374\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1388 - acc: 0.9397\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1373 - acc: 0.9398\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1372 - acc: 0.9403\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1349 - acc: 0.9410\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1339 - acc: 0.9402\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1314 - acc: 0.9428\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1299 - acc: 0.9428\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1290 - acc: 0.9435\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1283 - acc: 0.9442\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1271 - acc: 0.9443\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1246 - acc: 0.9451\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1226 - acc: 0.9459\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1226 - acc: 0.9447\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1218 - acc: 0.9453\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1195 - acc: 0.9465\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1170 - acc: 0.9478\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1160 - acc: 0.9484\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1160 - acc: 0.9483\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1170 - acc: 0.9483\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1134 - acc: 0.9494\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1122 - acc: 0.9500\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1116 - acc: 0.9501\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1109 - acc: 0.9503\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1105 - acc: 0.9509\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1092 - acc: 0.9514\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1096 - acc: 0.9516\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1106 - acc: 0.9505\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1068 - acc: 0.9523\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1060 - acc: 0.9526\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1057 - acc: 0.9531\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1043 - acc: 0.9534\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1035 - acc: 0.9538\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1039 - acc: 0.9541\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1009 - acc: 0.9542\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1027 - acc: 0.9544\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1004 - acc: 0.9554\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0994 - acc: 0.9555\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0990 - acc: 0.9554\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0979 - acc: 0.9564\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0971 - acc: 0.9566\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0963 - acc: 0.9570\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0960 - acc: 0.9569\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0958 - acc: 0.9566\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0950 - acc: 0.9569\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.0937 - acc: 0.9575\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0937 - acc: 0.9574\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0934 - acc: 0.9582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.72     43262\n",
      "           1       0.68      0.59      0.63     23926\n",
      "           2       0.73      0.73      0.73     37543\n",
      "\n",
      "    accuracy                           0.70    104731\n",
      "   macro avg       0.70      0.69      0.69    104731\n",
      "weighted avg       0.70      0.70      0.70    104731\n",
      "\n",
      "Acur√°cia\n",
      "0.6870986938717634\n",
      "Precisao\n",
      "0.7027390236549323\n",
      "Recall\n",
      "0.7032301801758791\n",
      "F1\n",
      "0.7018547320452446\n",
      "[[32106  4149  7007]\n",
      " [ 6714 14024  3188]\n",
      " [ 7536  2487 27520]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1998] TEST: [   7    8   11   18   28   33   47   48   59   65   68   71   78   85\n",
      "  107  113  127  131  133  137  143  155  160  164  167  172  174  186\n",
      "  188  195  196  203  204  205  208  211  221  224  225  238  239  261\n",
      "  266  277  281  285  286  293  302  309  315  317  320  323  326  330\n",
      "  337  338  339  346  353  354  367  372  377  383  403  407  410  411\n",
      "  414  421  422  431  432  433  439  440  441  445  453  459  464  465\n",
      "  478  479  480  486  502  503  507  510  511  512  513  518  522  523\n",
      "  530  536  542  545  547  550  551  562  577  587  589  602  607  609\n",
      "  611  616  632  633  641  645  646  649  651  657  663  664  667  668\n",
      "  670  692  693  695  714  718  731  732  733  735  739  740  743  758\n",
      "  759  766  769  770  777  783  786  787  793  806  811  815  820  824\n",
      "  825  833  837  838  839  841  846  847  853  858  860  867  870  871\n",
      "  872  876  878  879  884  887  904  909  913  914  924  926  927  928\n",
      "  935  940  945  956  964  966  970  988  995 1001 1007 1009 1014 1024\n",
      " 1032 1034 1035 1044 1049 1054 1059 1065 1084 1088 1089 1094 1097 1103\n",
      " 1104 1105 1112 1113 1115 1120 1121 1123 1124 1126 1129 1131 1132 1133\n",
      " 1134 1141 1144 1152 1154 1162 1163 1174 1186 1187 1200 1201 1203 1205\n",
      " 1210 1214 1218 1226 1243 1247 1253 1254 1261 1262 1269 1270 1273 1274\n",
      " 1276 1283 1284 1287 1288 1291 1296 1297 1298 1310 1315 1321 1325 1327\n",
      " 1329 1330 1332 1335 1336 1337 1339 1342 1360 1368 1370 1377 1384 1387\n",
      " 1390 1394 1396 1405 1411 1416 1418 1434 1435 1439 1440 1445 1448 1455\n",
      " 1459 1461 1463 1464 1465 1466 1468 1470 1471 1473 1477 1478 1480 1481\n",
      " 1482 1489 1491 1502 1511 1515 1539 1552 1562 1563 1566 1574 1575 1581\n",
      " 1587 1593 1596 1609 1610 1615 1628 1634 1637 1646 1654 1658 1659 1661\n",
      " 1666 1670 1679 1685 1687 1689 1699 1701 1702 1705 1712 1718 1719 1727\n",
      " 1735 1757 1761 1762 1764 1766 1771 1774 1801 1804 1808 1810 1815 1818\n",
      " 1827 1828 1838 1845 1853 1856 1857 1862 1866 1873 1890 1893 1895 1898\n",
      " 1900 1901 1911 1915 1927 1932 1939 1942 1945 1950 1952 1955 1956 1973\n",
      " 1976 1977 1980 1981 1985 1986 1987 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 0.3702 - acc: 0.7936\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.3426 - acc: 0.8368\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.3212 - acc: 0.8518\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3120 - acc: 0.8577\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.3060 - acc: 0.8610\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.3150 - acc: 0.8593\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.3013 - acc: 0.8641\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2992 - acc: 0.8650\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2988 - acc: 0.8651\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2940 - acc: 0.8667\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.2924 - acc: 0.8681\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2885 - acc: 0.8686\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2874 - acc: 0.8701\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2878 - acc: 0.8708\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2834 - acc: 0.8724\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2809 - acc: 0.8742\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2785 - acc: 0.8751\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.2746 - acc: 0.8778\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2715 - acc: 0.8782\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2696 - acc: 0.8795\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2665 - acc: 0.8810\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2610 - acc: 0.8852\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2589 - acc: 0.8867\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2547 - acc: 0.8868\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2507 - acc: 0.8897\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2464 - acc: 0.8901\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2414 - acc: 0.8938\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2366 - acc: 0.8953\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2323 - acc: 0.8973\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2284 - acc: 0.8998\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2231 - acc: 0.9011\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2187 - acc: 0.9036\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2150 - acc: 0.9049\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2103 - acc: 0.9087\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2052 - acc: 0.9102\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2006 - acc: 0.9111\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1978 - acc: 0.9133\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1933 - acc: 0.9164\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1904 - acc: 0.9174\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1868 - acc: 0.9153\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1813 - acc: 0.9180\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1796 - acc: 0.9189\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1759 - acc: 0.9212\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1750 - acc: 0.9207\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1704 - acc: 0.9228\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1671 - acc: 0.9254\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1649 - acc: 0.9277\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1629 - acc: 0.9273\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1601 - acc: 0.9277\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1560 - acc: 0.9294\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1536 - acc: 0.9301\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1511 - acc: 0.9324\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1504 - acc: 0.9328\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1486 - acc: 0.9330\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1470 - acc: 0.9334\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1436 - acc: 0.9340\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1420 - acc: 0.9346\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1416 - acc: 0.9361\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1385 - acc: 0.9386\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1362 - acc: 0.9388\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1347 - acc: 0.9380\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1346 - acc: 0.9387\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1336 - acc: 0.9388\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1305 - acc: 0.9417\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1290 - acc: 0.9439\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1290 - acc: 0.9411\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1307 - acc: 0.9409\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1281 - acc: 0.9422\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.1239 - acc: 0.9422\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1234 - acc: 0.9423\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1210 - acc: 0.9444\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1199 - acc: 0.9449\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1179 - acc: 0.9466\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1174 - acc: 0.9466\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1194 - acc: 0.9457\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1175 - acc: 0.9458\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1149 - acc: 0.9475\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1136 - acc: 0.9486\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1122 - acc: 0.9500\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1121 - acc: 0.9510\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1111 - acc: 0.9510\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1114 - acc: 0.9500\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1104 - acc: 0.9496\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1096 - acc: 0.9489\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1087 - acc: 0.9501\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1071 - acc: 0.9506\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1064 - acc: 0.9501\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1058 - acc: 0.9522\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1065 - acc: 0.9527\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1045 - acc: 0.9529\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1034 - acc: 0.9542\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1025 - acc: 0.9533\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1010 - acc: 0.9530\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1009 - acc: 0.9537\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1011 - acc: 0.9534\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1012 - acc: 0.9534\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0994 - acc: 0.9539\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0974 - acc: 0.9552\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0972 - acc: 0.9553\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0968 - acc: 0.9554\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69     40573\n",
      "           1       0.65      0.61      0.63     22284\n",
      "           2       0.72      0.74      0.73     37853\n",
      "\n",
      "    accuracy                           0.69    100710\n",
      "   macro avg       0.68      0.68      0.68    100710\n",
      "weighted avg       0.69      0.69      0.69    100710\n",
      "\n",
      "Acur√°cia\n",
      "0.6781749660882762\n",
      "Precisao\n",
      "0.6889349108824949\n",
      "Recall\n",
      "0.6898024029391322\n",
      "F1\n",
      "0.6891544913027156\n",
      "[[28043  4761  7769]\n",
      " [ 5672 13487  3125]\n",
      " [ 7340  2573 27940]]\n",
      "TRAIN: [   0    2    4 ... 1997 1998 1999] TEST: [   1    3    5   16   23   24   30   34   35   37   38   41   42   43\n",
      "   52   53   58   63   69   72   74   76   81   87   93   94   97   98\n",
      "  102  103  116  119  121  122  124  134  136  141  146  147  151  159\n",
      "  171  175  184  190  199  201  215  218  223  230  245  248  249  253\n",
      "  262  268  280  282  289  290  301  303  304  308  310  316  325  344\n",
      "  355  356  359  361  362  363  368  370  374  378  379  382  384  390\n",
      "  400  418  420  423  424  427  442  461  463  466  472  473  483  509\n",
      "  514  516  519  526  529  534  537  539  558  560  567  570  575  576\n",
      "  581  582  588  599  601  604  606  615  618  620  623  626  634  644\n",
      "  647  650  653  658  665  666  681  689  701  703  704  707  709  712\n",
      "  720  730  734  748  749  750  755  760  761  765  776  788  789  792\n",
      "  797  799  805  809  814  816  826  844  852  857  859  861  866  874\n",
      "  880  881  888  889  890  898  903  910  915  919  930  931  933  934\n",
      "  949  955  960  962  965  967  971  974  976  985  998 1003 1004 1017\n",
      " 1018 1022 1023 1025 1030 1041 1042 1043 1046 1047 1050 1060 1071 1072\n",
      " 1086 1095 1096 1098 1100 1101 1106 1108 1114 1135 1136 1140 1143 1149\n",
      " 1159 1160 1161 1173 1177 1182 1185 1190 1199 1207 1209 1216 1219 1220\n",
      " 1230 1235 1239 1244 1255 1257 1259 1266 1268 1271 1275 1278 1286 1290\n",
      " 1299 1306 1311 1313 1314 1316 1317 1318 1319 1322 1323 1324 1333 1344\n",
      " 1349 1353 1354 1355 1357 1359 1361 1363 1366 1378 1381 1388 1395 1399\n",
      " 1414 1420 1423 1424 1429 1433 1458 1462 1474 1475 1476 1479 1484 1487\n",
      " 1492 1497 1507 1510 1518 1521 1527 1534 1540 1541 1543 1546 1548 1549\n",
      " 1550 1561 1572 1577 1582 1586 1589 1592 1595 1597 1600 1603 1623 1629\n",
      " 1635 1641 1642 1647 1648 1662 1671 1673 1677 1678 1683 1694 1695 1697\n",
      " 1710 1715 1724 1728 1729 1731 1734 1737 1738 1740 1743 1744 1746 1754\n",
      " 1763 1775 1776 1777 1778 1781 1784 1785 1788 1790 1793 1796 1798 1800\n",
      " 1817 1823 1824 1829 1837 1843 1848 1850 1854 1860 1868 1872 1886 1887\n",
      " 1889 1899 1906 1912 1918 1920 1921 1928 1934 1938 1940 1944 1948 1953\n",
      " 1954 1959 1961 1968 1975 1979 1982 1993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.3661 - acc: 0.7893\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3356 - acc: 0.8416\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3137 - acc: 0.8567\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3050 - acc: 0.8610\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.3005 - acc: 0.8639\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2980 - acc: 0.8657\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2940 - acc: 0.8669\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2899 - acc: 0.8692\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2885 - acc: 0.8698\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2864 - acc: 0.8713\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2878 - acc: 0.8707\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2822 - acc: 0.8728\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2812 - acc: 0.8732\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2813 - acc: 0.8741\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2767 - acc: 0.8752\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2710 - acc: 0.8795\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2682 - acc: 0.8810\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2668 - acc: 0.8818\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.2658 - acc: 0.8820\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2595 - acc: 0.8852\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2552 - acc: 0.8878\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2533 - acc: 0.8890\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2515 - acc: 0.8895\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2445 - acc: 0.8935\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2389 - acc: 0.8964\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2367 - acc: 0.8972\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2314 - acc: 0.8994\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2288 - acc: 0.9006\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2214 - acc: 0.9043\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2184 - acc: 0.9054\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2163 - acc: 0.9056\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2096 - acc: 0.9094\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2053 - acc: 0.9110\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.2005 - acc: 0.9132\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.1964 - acc: 0.9149\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1959 - acc: 0.9144\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1921 - acc: 0.9168\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1882 - acc: 0.9186\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1905 - acc: 0.9167\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1850 - acc: 0.9193\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1803 - acc: 0.9220\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1752 - acc: 0.9243\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1721 - acc: 0.9256\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1714 - acc: 0.9250\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1671 - acc: 0.9271\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1632 - acc: 0.9300\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1609 - acc: 0.9310\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1574 - acc: 0.9321\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1550 - acc: 0.9340\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1529 - acc: 0.9349\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1552 - acc: 0.9336\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1523 - acc: 0.9349\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1485 - acc: 0.9368\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1475 - acc: 0.9372\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1489 - acc: 0.9367\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1418 - acc: 0.9396\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1394 - acc: 0.9406\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1376 - acc: 0.9407\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1369 - acc: 0.9414\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1358 - acc: 0.9420\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1326 - acc: 0.9430\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1308 - acc: 0.9428\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1305 - acc: 0.9435\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1294 - acc: 0.9440\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1279 - acc: 0.9455\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1259 - acc: 0.9462\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1245 - acc: 0.9469\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.1228 - acc: 0.9474\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1219 - acc: 0.9477\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1200 - acc: 0.9484\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1192 - acc: 0.9491\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1200 - acc: 0.9488\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1170 - acc: 0.9502\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1161 - acc: 0.9508\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1145 - acc: 0.9510\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1138 - acc: 0.9519\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1151 - acc: 0.9511\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1134 - acc: 0.9514\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1110 - acc: 0.9524\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1095 - acc: 0.9536\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1096 - acc: 0.9536\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1088 - acc: 0.9534\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1086 - acc: 0.9543\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1064 - acc: 0.9553\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1067 - acc: 0.9549\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1058 - acc: 0.9543\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1045 - acc: 0.9542\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1045 - acc: 0.9551\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1025 - acc: 0.9565\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1038 - acc: 0.9567\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1023 - acc: 0.9565\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1009 - acc: 0.9569\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1005 - acc: 0.9569\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.0997 - acc: 0.9574\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.0987 - acc: 0.9585\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.0986 - acc: 0.9584\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.0975 - acc: 0.9587\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.0990 - acc: 0.9581\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.0975 - acc: 0.9587\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.0995 - acc: 0.9575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71     43250\n",
      "           1       0.66      0.61      0.63     23174\n",
      "           2       0.72      0.75      0.74     37554\n",
      "\n",
      "    accuracy                           0.70    103978\n",
      "   macro avg       0.70      0.69      0.69    103978\n",
      "weighted avg       0.70      0.70      0.70    103978\n",
      "\n",
      "Acur√°cia\n",
      "0.6898920856991454\n",
      "Precisao\n",
      "0.7016937280962863\n",
      "Recall\n",
      "0.7027640462405509\n",
      "F1\n",
      "0.7018398156761868\n",
      "[[30810  4783  7657]\n",
      " [ 5867 14037  3270]\n",
      " [ 6914  2415 28225]]\n",
      "TRAIN: [   0    1    3 ... 1997 1998 1999] TEST: [   2    4   12   17   21   26   45   49   51   55   56   61   62   70\n",
      "   82   89   91   95   99  104  112  114  120  123  125  126  128  130\n",
      "  132  138  140  153  158  168  170  176  179  200  206  209  217  222\n",
      "  231  234  236  237  240  244  246  247  250  259  260  265  270  271\n",
      "  272  275  278  295  298  299  300  305  307  313  314  321  340  341\n",
      "  343  347  352  365  373  387  389  392  396  397  399  402  404  405\n",
      "  408  409  412  413  415  419  425  434  435  446  448  449  454  455\n",
      "  468  476  481  485  488  489  496  498  500  520  528  535  543  544\n",
      "  546  553  561  563  571  573  574  578  580  584  585  590  597  605\n",
      "  608  610  612  613  619  627  628  629  635  636  637  638  654  659\n",
      "  677  680  684  690  706  708  713  715  716  717  719  721  723  725\n",
      "  727  729  741  745  747  754  757  762  764  767  772  774  780  785\n",
      "  794  817  818  831  832  848  849  856  864  865  894  896  899  916\n",
      "  918  921  923  925  938  939  944  947  950  952  957  958  969  973\n",
      "  978  989  991  999 1000 1015 1020 1028 1033 1036 1040 1045 1052 1066\n",
      " 1070 1074 1078 1090 1091 1093 1110 1116 1117 1130 1139 1145 1147 1150\n",
      " 1155 1156 1165 1170 1171 1172 1184 1189 1197 1198 1202 1223 1224 1229\n",
      " 1231 1234 1236 1237 1240 1245 1252 1258 1260 1263 1279 1301 1303 1304\n",
      " 1308 1309 1326 1331 1345 1346 1348 1351 1356 1365 1367 1369 1371 1372\n",
      " 1373 1376 1382 1385 1389 1393 1398 1413 1417 1426 1427 1432 1438 1441\n",
      " 1444 1446 1449 1450 1451 1453 1457 1469 1483 1485 1493 1496 1499 1500\n",
      " 1506 1508 1512 1513 1517 1523 1526 1528 1530 1531 1532 1535 1537 1542\n",
      " 1544 1545 1553 1554 1555 1556 1559 1567 1568 1570 1573 1578 1583 1594\n",
      " 1604 1605 1608 1614 1619 1621 1624 1631 1639 1643 1650 1653 1655 1657\n",
      " 1676 1680 1693 1696 1698 1700 1706 1711 1722 1723 1730 1732 1733 1747\n",
      " 1752 1753 1755 1756 1760 1767 1768 1782 1783 1795 1799 1803 1805 1807\n",
      " 1811 1812 1822 1836 1839 1840 1842 1851 1858 1859 1863 1871 1876 1877\n",
      " 1881 1882 1888 1894 1896 1902 1905 1914 1922 1926 1941 1943 1951 1957\n",
      " 1963 1964 1965 1967 1970 1983 1992 1994]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.3794 - acc: 0.7227\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3464 - acc: 0.8276\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.3299 - acc: 0.8452\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3124 - acc: 0.8567\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3060 - acc: 0.8602\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.3019 - acc: 0.8619\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2987 - acc: 0.8634\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2953 - acc: 0.8659\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2938 - acc: 0.8672\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2925 - acc: 0.8674\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2899 - acc: 0.8689\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2879 - acc: 0.8701\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2856 - acc: 0.8708\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2826 - acc: 0.8720\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2804 - acc: 0.8730\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2760 - acc: 0.8759\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2737 - acc: 0.8773\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2718 - acc: 0.8782\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2675 - acc: 0.8808\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2645 - acc: 0.8827\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2580 - acc: 0.8860\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2560 - acc: 0.8872\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2517 - acc: 0.8875\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2460 - acc: 0.8911\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2422 - acc: 0.8940\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2369 - acc: 0.8949\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2336 - acc: 0.8964\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2287 - acc: 0.8987\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2248 - acc: 0.9012\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2186 - acc: 0.9041\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2148 - acc: 0.9057\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2097 - acc: 0.9091\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2043 - acc: 0.9110\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2004 - acc: 0.9128\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1972 - acc: 0.9139\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1931 - acc: 0.9160\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1902 - acc: 0.9168\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1865 - acc: 0.9184\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1815 - acc: 0.9196\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1777 - acc: 0.9222\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1751 - acc: 0.9223\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1719 - acc: 0.9237\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1702 - acc: 0.9248\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1659 - acc: 0.9268\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1637 - acc: 0.9273\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1617 - acc: 0.9288\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1582 - acc: 0.9301\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1566 - acc: 0.9306\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1528 - acc: 0.9311\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1520 - acc: 0.9323\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1494 - acc: 0.9337\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1464 - acc: 0.9348\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1443 - acc: 0.9362\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1423 - acc: 0.9366\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1405 - acc: 0.9383\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1389 - acc: 0.9387\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1391 - acc: 0.9389\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1357 - acc: 0.9393\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1324 - acc: 0.9413\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1324 - acc: 0.9407\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1300 - acc: 0.9418\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1282 - acc: 0.9431\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1271 - acc: 0.9441\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1262 - acc: 0.9457\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 21s 13ms/sample - loss: 0.1260 - acc: 0.9451\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1237 - acc: 0.9457\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1217 - acc: 0.9459\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1198 - acc: 0.9463\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1190 - acc: 0.9473\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1188 - acc: 0.9466\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1173 - acc: 0.9464\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1163 - acc: 0.9472\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1151 - acc: 0.9484\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1138 - acc: 0.9486\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1125 - acc: 0.9491\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1121 - acc: 0.9500\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1114 - acc: 0.9507\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1097 - acc: 0.9520\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1086 - acc: 0.9517\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1092 - acc: 0.9513\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1071 - acc: 0.9525\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1074 - acc: 0.9530\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1063 - acc: 0.9531\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1048 - acc: 0.9534\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1041 - acc: 0.9540\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1038 - acc: 0.9535\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1018 - acc: 0.9547\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1012 - acc: 0.9549\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.1013 - acc: 0.9540\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1009 - acc: 0.9548\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1007 - acc: 0.9552\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.0991 - acc: 0.9554\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.0989 - acc: 0.9554\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.0997 - acc: 0.9550\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.0981 - acc: 0.9561\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.0973 - acc: 0.9560\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.0952 - acc: 0.9570\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.0956 - acc: 0.9570\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.0946 - acc: 0.9571\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.0948 - acc: 0.9573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70     41395\n",
      "           1       0.66      0.62      0.64     23673\n",
      "           2       0.71      0.74      0.73     36656\n",
      "\n",
      "    accuracy                           0.69    101724\n",
      "   macro avg       0.69      0.69      0.69    101724\n",
      "weighted avg       0.69      0.69      0.69    101724\n",
      "\n",
      "Acur√°cia\n",
      "0.6854872972649743\n",
      "Precisao\n",
      "0.6939628627768943\n",
      "Recall\n",
      "0.6947917895481892\n",
      "F1\n",
      "0.6941002262556054\n",
      "[[28813  5090  7492]\n",
      " [ 5669 14593  3411]\n",
      " [ 6940  2445 27271]]\n",
      "TRAIN: [   1    2    3 ... 1996 1997 1999] TEST: [   0   13   15   19   32   36   46   54   57   60   64   66   77   79\n",
      "   88   90   96  100  105  108  109  110  111  129  139  142  144  148\n",
      "  149  154  156  157  162  163  165  173  177  180  181  182  183  185\n",
      "  189  227  228  229  235  242  243  251  252  254  255  264  267  269\n",
      "  273  279  291  294  296  318  327  333  335  342  348  350  351  357\n",
      "  369  380  381  385  386  388  395  406  429  430  444  447  451  457\n",
      "  458  462  469  474  482  484  487  491  493  494  495  497  499  501\n",
      "  504  505  517  521  524  531  533  538  540  548  554  556  559  564\n",
      "  565  568  569  572  579  586  592  594  595  600  617  621  622  625\n",
      "  630  631  640  642  643  655  656  660  662  672  673  686  699  722\n",
      "  724  726  728  736  742  746  775  781  782  784  790  791  795  802\n",
      "  803  804  807  808  812  821  822  823  828  829  830  835  836  840\n",
      "  845  851  854  862  868  877  882  883  891  892  895  897  901  902\n",
      "  906  907  908  911  912  917  922  932  937  946  948  951  953  959\n",
      "  961  972  979  983  984  986  990  992  993  996 1002 1005 1006 1012\n",
      " 1021 1026 1029 1031 1048 1051 1056 1057 1058 1061 1064 1068 1069 1075\n",
      " 1079 1085 1099 1109 1111 1125 1127 1128 1137 1142 1146 1158 1164 1166\n",
      " 1167 1169 1176 1183 1188 1191 1192 1195 1196 1206 1211 1213 1215 1221\n",
      " 1222 1225 1228 1232 1233 1246 1265 1267 1272 1277 1280 1282 1285 1293\n",
      " 1300 1305 1307 1320 1343 1347 1352 1374 1379 1383 1392 1402 1403 1408\n",
      " 1409 1410 1415 1421 1430 1431 1436 1442 1454 1456 1486 1495 1501 1503\n",
      " 1504 1505 1514 1516 1520 1522 1524 1525 1529 1533 1536 1538 1547 1557\n",
      " 1565 1571 1579 1580 1584 1585 1590 1591 1599 1602 1607 1612 1616 1618\n",
      " 1620 1622 1627 1632 1633 1636 1640 1645 1651 1652 1660 1663 1664 1667\n",
      " 1668 1669 1672 1681 1703 1709 1716 1717 1725 1736 1745 1748 1749 1751\n",
      " 1765 1769 1773 1780 1794 1797 1802 1814 1816 1819 1820 1821 1831 1832\n",
      " 1834 1844 1846 1849 1852 1865 1870 1875 1880 1884 1891 1892 1903 1904\n",
      " 1908 1909 1917 1919 1923 1924 1929 1933 1935 1936 1947 1949 1958 1960\n",
      " 1966 1969 1978 1988 1990 1991 1995 1998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 26s 16ms/sample - loss: 0.3664 - acc: 0.7603\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3340 - acc: 0.8420\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3147 - acc: 0.8552\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.3039 - acc: 0.8621\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3035 - acc: 0.8632\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.3027 - acc: 0.8625\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2964 - acc: 0.8662\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2947 - acc: 0.8673\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2914 - acc: 0.8688\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2898 - acc: 0.8693\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2842 - acc: 0.8724\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2840 - acc: 0.8713\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2794 - acc: 0.8738\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2777 - acc: 0.8745\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2784 - acc: 0.8746\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2748 - acc: 0.8757\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2700 - acc: 0.8784\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2687 - acc: 0.8798\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2650 - acc: 0.8819\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2656 - acc: 0.8808\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2641 - acc: 0.8823\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2586 - acc: 0.8854\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2544 - acc: 0.8867\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2506 - acc: 0.8883\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2430 - acc: 0.8920\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2385 - acc: 0.8944\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2357 - acc: 0.8959\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2310 - acc: 0.8983\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2242 - acc: 0.9017\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2203 - acc: 0.9037\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.2160 - acc: 0.9062\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2128 - acc: 0.9079\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2058 - acc: 0.9111\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.2031 - acc: 0.9125\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.2013 - acc: 0.9136\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1957 - acc: 0.9158\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1919 - acc: 0.9179\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1887 - acc: 0.9186\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1868 - acc: 0.9195\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1809 - acc: 0.9225\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1792 - acc: 0.9233\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1770 - acc: 0.9238\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1718 - acc: 0.9263\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1690 - acc: 0.9274\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1655 - acc: 0.9286\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1641 - acc: 0.9297\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1600 - acc: 0.9314\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1597 - acc: 0.9317\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.1579 - acc: 0.9321\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1553 - acc: 0.9336\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 25s 16ms/sample - loss: 0.1533 - acc: 0.9347\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1489 - acc: 0.9364\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1466 - acc: 0.9374\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1442 - acc: 0.9381\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1418 - acc: 0.9388\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.1411 - acc: 0.9394\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1390 - acc: 0.9404\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.1363 - acc: 0.9406\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.1375 - acc: 0.9397\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 20s 13ms/sample - loss: 0.1358 - acc: 0.9404\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1326 - acc: 0.9417\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1300 - acc: 0.9427\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1281 - acc: 0.9435\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1282 - acc: 0.9446\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1262 - acc: 0.9453\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1238 - acc: 0.9456\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1240 - acc: 0.9465\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1233 - acc: 0.9465\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1216 - acc: 0.9477\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1196 - acc: 0.9481\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1193 - acc: 0.9480\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1174 - acc: 0.9494\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1167 - acc: 0.9492\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1164 - acc: 0.9499\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1151 - acc: 0.9498\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1138 - acc: 0.9498\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1131 - acc: 0.9498\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1105 - acc: 0.9508\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1109 - acc: 0.9514\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1106 - acc: 0.9514\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1084 - acc: 0.9522\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1074 - acc: 0.9518\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1072 - acc: 0.9516\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1057 - acc: 0.9532\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1063 - acc: 0.9535\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1046 - acc: 0.9536\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1032 - acc: 0.9549\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1032 - acc: 0.9546\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1034 - acc: 0.9541\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1018 - acc: 0.9555\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1019 - acc: 0.9554\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1007 - acc: 0.9559\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.1016 - acc: 0.9556\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0999 - acc: 0.9556\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0980 - acc: 0.9564\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0983 - acc: 0.9567\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0968 - acc: 0.9572\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0960 - acc: 0.9576\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0945 - acc: 0.9585\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.0945 - acc: 0.9582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70     43118\n",
      "           1       0.64      0.60      0.62     22139\n",
      "           2       0.72      0.73      0.72     39408\n",
      "\n",
      "    accuracy                           0.69    104665\n",
      "   macro avg       0.68      0.68      0.68    104665\n",
      "weighted avg       0.69      0.69      0.69    104665\n",
      "\n",
      "Acur√°cia\n",
      "0.678848743104188\n",
      "Precisao\n",
      "0.6918376393446475\n",
      "Recall\n",
      "0.692676634978264\n",
      "F1\n",
      "0.6920944512076532\n",
      "[[30471  4534  8113]\n",
      " [ 5651 13306  3182]\n",
      " [ 7721  2965 28722]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_28 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_30 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_31 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_32 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 1,547,803\n",
      "Trainable params: 1,547,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.68709869 0.67817497 0.68989209 0.6854873  0.67884874]\n",
      "0.6839003572056694\n",
      "Precision total\n",
      "[0.70273902 0.68893491 0.70169373 0.69396286 0.69183764]\n",
      "0.6958336329510512\n",
      "Recalls total\n",
      "[0.70323018 0.6898024  0.70276405 0.69479179 0.69267663]\n",
      "0.6966530107764031\n",
      "F1 total\n",
      "[0.70185473 0.68915449 0.70183982 0.69410023 0.69209445]\n",
      "0.6958087432974811\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
