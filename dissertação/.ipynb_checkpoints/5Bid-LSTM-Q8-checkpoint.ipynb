{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn = criarRede, epochs = 50, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 50, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "previsores_trei_vald, previsores_testes, classes_trei_vald, classes_testes = train_test_split(previsores, classes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 09:26:58.689466  5112 deprecation_wrapper.py:119] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0809 09:26:58.724371  5112 deprecation_wrapper.py:119] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0809 09:26:58.733349  5112 deprecation_wrapper.py:119] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0809 09:26:59.153228  5112 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0809 09:26:59.434473  5112 deprecation_wrapper.py:119] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0809 09:26:59.454423  5112 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0809 09:27:02.466373  5112 deprecation_wrapper.py:119] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0809 09:27:02.496293  5112 deprecation_wrapper.py:119] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 530s 414ms/step - loss: 1.7375 - acc: 0.3219\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 541s 423ms/step - loss: 1.5882 - acc: 0.4050\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 852s 665ms/step - loss: 1.5109 - acc: 0.4422\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 970s 757ms/step - loss: 1.4546 - acc: 0.4632\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 970s 758ms/step - loss: 1.4323 - acc: 0.4702\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 947s 740ms/step - loss: 1.4028 - acc: 0.4844\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 960s 750ms/step - loss: 1.3866 - acc: 0.4930\n",
      "Epoch 8/50\n",
      "1280/1280 [==============================] - 986s 770ms/step - loss: 1.3766 - acc: 0.4975\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 993s 776ms/step - loss: 1.3703 - acc: 0.4999\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 993s 776ms/step - loss: 1.3589 - acc: 0.5056\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 1003s 784ms/step - loss: 1.3543 - acc: 0.5065\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 987s 771ms/step - loss: 1.3427 - acc: 0.5113\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 986s 770ms/step - loss: 1.3388 - acc: 0.5136\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 991s 774ms/step - loss: 1.3257 - acc: 0.5196\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 979s 765ms/step - loss: 1.3130 - acc: 0.5249\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 990s 773ms/step - loss: 1.3081 - acc: 0.5270\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 988s 772ms/step - loss: 1.2970 - acc: 0.5302\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 996s 778ms/step - loss: 1.2996 - acc: 0.5304\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 989s 772ms/step - loss: 1.2900 - acc: 0.5334\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 1000s 781ms/step - loss: 1.2741 - acc: 0.5395\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 995s 777ms/step - loss: 1.2683 - acc: 0.5415\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 1015s 793ms/step - loss: 1.2574 - acc: 0.5452\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 1001s 782ms/step - loss: 1.2485 - acc: 0.5481\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 1001s 782ms/step - loss: 1.2313 - acc: 0.5541\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 1008s 788ms/step - loss: 1.2216 - acc: 0.5590\n",
      "Epoch 26/50\n",
      "1280/1280 [==============================] - 1002s 783ms/step - loss: 1.2087 - acc: 0.5628\n",
      "Epoch 27/50\n",
      "1280/1280 [==============================] - 999s 781ms/step - loss: 1.1928 - acc: 0.5683\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 1003s 784ms/step - loss: 1.1870 - acc: 0.5714\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 1011s 790ms/step - loss: 1.1715 - acc: 0.5761\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 791s 618ms/step - loss: 1.1572 - acc: 0.5812\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 677s 529ms/step - loss: 1.1577 - acc: 0.5819\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 677s 529ms/step - loss: 1.1405 - acc: 0.5873\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 686s 536ms/step - loss: 1.1129 - acc: 0.5963\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 672s 525ms/step - loss: 1.0976 - acc: 0.6017\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 679s 530ms/step - loss: 1.0858 - acc: 0.6057\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 1.0708 - acc: 0.6109\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 671s 525ms/step - loss: 1.0615 - acc: 0.6135\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 1.0484 - acc: 0.6181\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 664s 519ms/step - loss: 1.0444 - acc: 0.6183\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 660s 515ms/step - loss: 1.0255 - acc: 0.6252\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 670s 523ms/step - loss: 1.0092 - acc: 0.6305\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 667s 521ms/step - loss: 0.9977 - acc: 0.6336\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 658s 514ms/step - loss: 0.9825 - acc: 0.6392\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 668s 522ms/step - loss: 0.9682 - acc: 0.6435\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 667s 521ms/step - loss: 0.9601 - acc: 0.6464\n",
      "Epoch 46/50\n",
      "1280/1280 [==============================] - 658s 514ms/step - loss: 0.9454 - acc: 0.6516\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 652s 509ms/step - loss: 0.9363 - acc: 0.6549\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 652s 509ms/step - loss: 0.9220 - acc: 0.6586\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 651s 508ms/step - loss: 0.9153 - acc: 0.6613\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 654s 511ms/step - loss: 0.9129 - acc: 0.6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       925\n",
      "           1       0.55      0.69      0.61     16628\n",
      "           2       0.30      0.17      0.22      3104\n",
      "           3       0.69      0.73      0.71     25422\n",
      "           4       0.42      0.20      0.27       549\n",
      "           5       0.43      0.50      0.46     15736\n",
      "           6       0.34      0.06      0.10      6815\n",
      "           7       0.37      0.37      0.37      8733\n",
      "\n",
      "    accuracy                           0.54     77912\n",
      "   macro avg       0.39      0.34      0.34     77912\n",
      "weighted avg       0.52      0.54      0.52     77912\n",
      "\n",
      "Acur√°cia\n",
      "0.3397924016597385\n",
      "Precisao\n",
      "0.5152537860271407\n",
      "Recall\n",
      "0.5398911592565971\n",
      "F1\n",
      "0.5153906769780009\n",
      "[[    0   232    29   136     0   401    30    97]\n",
      " [    0 11496   160  1926     9  2269    86   682]\n",
      " [    0   523   541   747     9   766    34   484]\n",
      " [    0  3067   368 18441    81  2162    82  1221]\n",
      " [    0    98     4   235   110    60    10    32]\n",
      " [    0  3294   300  2318    27  7855   326  1616]\n",
      " [    0  1210   147  1023    11  2764   407  1253]\n",
      " [    0  1094   249  1935    18  2016   207  3214]]\n",
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 642s 502ms/step - loss: 1.0439 - acc: 0.6268\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 643s 503ms/step - loss: 0.9979 - acc: 0.6395\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 641s 501ms/step - loss: 0.9746 - acc: 0.6459\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 643s 502ms/step - loss: 0.9646 - acc: 0.6495\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 646s 505ms/step - loss: 0.9457 - acc: 0.6549\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 653s 510ms/step - loss: 0.9245 - acc: 0.6618\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 656s 512ms/step - loss: 0.9092 - acc: 0.6658\n",
      "Epoch 8/50\n",
      "1280/1280 [==============================] - 646s 505ms/step - loss: 0.8988 - acc: 0.6701\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 647s 505ms/step - loss: 0.8891 - acc: 0.6718\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 649s 507ms/step - loss: 0.8769 - acc: 0.6756\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 650s 508ms/step - loss: 0.8717 - acc: 0.6769\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 655s 511ms/step - loss: 0.8544 - acc: 0.6838\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 647s 506ms/step - loss: 0.8499 - acc: 0.6839\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 645s 504ms/step - loss: 0.8528 - acc: 0.6834\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 658s 514ms/step - loss: 0.8388 - acc: 0.6879\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 655s 511ms/step - loss: 0.8276 - acc: 0.6916\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 653s 510ms/step - loss: 0.8185 - acc: 0.6944\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 656s 513ms/step - loss: 0.8107 - acc: 0.6986\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 656s 513ms/step - loss: 0.8008 - acc: 0.7004\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 648s 506ms/step - loss: 0.7948 - acc: 0.7023\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 648s 506ms/step - loss: 0.7890 - acc: 0.7041\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 644s 503ms/step - loss: 0.7785 - acc: 0.7072\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 651s 509ms/step - loss: 0.7742 - acc: 0.7093\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 648s 506ms/step - loss: 0.7685 - acc: 0.7110\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 654s 511ms/step - loss: 0.7603 - acc: 0.7136\n",
      "Epoch 26/50\n",
      "1280/1280 [==============================] - 656s 512ms/step - loss: 0.7578 - acc: 0.7153\n",
      "Epoch 27/50\n",
      "1280/1280 [==============================] - 656s 513ms/step - loss: 0.7577 - acc: 0.7142\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 652s 510ms/step - loss: 0.7509 - acc: 0.7177\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 651s 508ms/step - loss: 0.7494 - acc: 0.7173\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 652s 509ms/step - loss: 0.7419 - acc: 0.7197\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 657s 513ms/step - loss: 0.7343 - acc: 0.7221\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 661s 516ms/step - loss: 0.7265 - acc: 0.7250\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 657s 513ms/step - loss: 0.7241 - acc: 0.7257\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 660s 516ms/step - loss: 0.7191 - acc: 0.7277\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 659s 515ms/step - loss: 0.7139 - acc: 0.7305\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 656s 513ms/step - loss: 0.7072 - acc: 0.7317\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 660s 516ms/step - loss: 0.7044 - acc: 0.7317\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 658s 514ms/step - loss: 0.7062 - acc: 0.7321\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 663s 518ms/step - loss: 0.7090 - acc: 0.7306\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 663s 518ms/step - loss: 0.6980 - acc: 0.7351\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 662s 517ms/step - loss: 0.6908 - acc: 0.7376\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 658s 514ms/step - loss: 0.6943 - acc: 0.7372\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 662s 517ms/step - loss: 0.6850 - acc: 0.7396\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 669s 522ms/step - loss: 0.6793 - acc: 0.7419\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 672s 525ms/step - loss: 0.6831 - acc: 0.7407\n",
      "Epoch 46/50\n",
      "1280/1280 [==============================] - 674s 526ms/step - loss: 0.6763 - acc: 0.7430\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 666s 520ms/step - loss: 0.6700 - acc: 0.7446\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 666s 520ms/step - loss: 0.6657 - acc: 0.7453\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 668s 522ms/step - loss: 0.6618 - acc: 0.7475\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 668s 522ms/step - loss: 0.6597 - acc: 0.7482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.02      0.04       980\n",
      "           1       0.71      0.72      0.72     17783\n",
      "           2       0.38      0.28      0.32      3092\n",
      "           3       0.79      0.86      0.82     27562\n",
      "           4       0.57      0.29      0.38       563\n",
      "           5       0.47      0.57      0.52     17165\n",
      "           6       0.38      0.18      0.24      7823\n",
      "           7       0.42      0.42      0.42      9810\n",
      "\n",
      "    accuracy                           0.62     84778\n",
      "   macro avg       0.55      0.42      0.43     84778\n",
      "weighted avg       0.61      0.62      0.61     84778\n",
      "\n",
      "Acur√°cia\n",
      "0.4170390889929207\n",
      "Precisao\n",
      "0.6117065041056942\n",
      "Recall\n",
      "0.624006228030857\n",
      "F1\n",
      "0.6088217831426905\n",
      "[[   18   169    22   106     2   478    68   117]\n",
      " [    0 12841   141   988     6  2811   284   712]\n",
      " [    1   234   865   558     5   800   100   529]\n",
      " [    0   773   218 23697    83  1513   145  1133]\n",
      " [    0     9    16   310   162    30     6    30]\n",
      " [    5  2381   380  1712     8  9820  1049  1810]\n",
      " [    1   917   257   763     4  3202  1388  1291]\n",
      " [    1   700   407  1783    12  2162   634  4111]]\n",
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 663s 518ms/step - loss: 0.8241 - acc: 0.7013\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 666s 520ms/step - loss: 0.7878 - acc: 0.7092\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 669s 522ms/step - loss: 0.7621 - acc: 0.7169\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 668s 522ms/step - loss: 0.7475 - acc: 0.7218\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 665s 519ms/step - loss: 0.7332 - acc: 0.7252\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 672s 525ms/step - loss: 0.7313 - acc: 0.7252\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 672s 525ms/step - loss: 0.7232 - acc: 0.7279\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 674s 526ms/step - loss: 0.7117 - acc: 0.7324\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 672s 525ms/step - loss: 0.7114 - acc: 0.7312\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 674s 527ms/step - loss: 0.7095 - acc: 0.7319\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 673s 526ms/step - loss: 0.6999 - acc: 0.7351\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 669s 523ms/step - loss: 0.6921 - acc: 0.7383\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 666s 521ms/step - loss: 0.6870 - acc: 0.7392\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 664s 518ms/step - loss: 0.6830 - acc: 0.7411\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 666s 520ms/step - loss: 0.6789 - acc: 0.7422\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 668s 522ms/step - loss: 0.6759 - acc: 0.7433\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 668s 522ms/step - loss: 0.6721 - acc: 0.7439\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 669s 523ms/step - loss: 0.6647 - acc: 0.7468\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 672s 525ms/step - loss: 0.6603 - acc: 0.7482\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 673s 526ms/step - loss: 0.6617 - acc: 0.7485\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 675s 528ms/step - loss: 0.6527 - acc: 0.7510\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 675s 528ms/step - loss: 0.6585 - acc: 0.7492\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 675s 527ms/step - loss: 0.6517 - acc: 0.7507\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 0.6443 - acc: 0.7536\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 675s 527ms/step - loss: 0.6422 - acc: 0.7553\n",
      "Epoch 26/50\n",
      "1280/1280 [==============================] - 677s 529ms/step - loss: 0.6397 - acc: 0.7561\n",
      "Epoch 27/50\n",
      "1280/1280 [==============================] - 678s 529ms/step - loss: 0.6379 - acc: 0.7562\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 672s 525ms/step - loss: 0.6339 - acc: 0.7573\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 679s 530ms/step - loss: 0.6319 - acc: 0.7578\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 680s 531ms/step - loss: 0.6285 - acc: 0.7597\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 0.6272 - acc: 0.7595\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 674s 526ms/step - loss: 0.6230 - acc: 0.7619\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 678s 529ms/step - loss: 0.6217 - acc: 0.7619\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 679s 530ms/step - loss: 0.6197 - acc: 0.7626\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 0.6196 - acc: 0.7631\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 0.6140 - acc: 0.7645\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 673s 526ms/step - loss: 0.6093 - acc: 0.7661\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 679s 530ms/step - loss: 0.6072 - acc: 0.7674\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 680s 532ms/step - loss: 0.6060 - acc: 0.7670\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 680s 531ms/step - loss: 0.6040 - acc: 0.7681\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 677s 529ms/step - loss: 0.5991 - acc: 0.7696\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 680s 531ms/step - loss: 0.5996 - acc: 0.7698\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 0.6006 - acc: 0.7693\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 680s 532ms/step - loss: 0.5939 - acc: 0.7720\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 0.5952 - acc: 0.7722\n",
      "Epoch 46/50\n",
      "1280/1280 [==============================] - 676s 528ms/step - loss: 0.5938 - acc: 0.7719\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 680s 531ms/step - loss: 0.5900 - acc: 0.7731\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 682s 533ms/step - loss: 0.5884 - acc: 0.7738\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 687s 537ms/step - loss: 0.5846 - acc: 0.7751\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 686s 536ms/step - loss: 0.5837 - acc: 0.7764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.05      0.09       972\n",
      "           1       0.81      0.84      0.83     17402\n",
      "           2       0.64      0.51      0.57      3153\n",
      "           3       0.88      0.92      0.90     26953\n",
      "           4       0.82      0.56      0.67       520\n",
      "           5       0.55      0.66      0.60     16436\n",
      "           6       0.48      0.24      0.32      7137\n",
      "           7       0.54      0.56      0.55      9242\n",
      "\n",
      "    accuracy                           0.72     81815\n",
      "   macro avg       0.67      0.54      0.56     81815\n",
      "weighted avg       0.71      0.72      0.71     81815\n",
      "\n",
      "Acur√°cia\n",
      "0.5422367121851244\n",
      "Precisao\n",
      "0.7143682068413978\n",
      "Recall\n",
      "0.7216036179184746\n",
      "F1\n",
      "0.7097562874981472\n",
      "[[   49   108    18    55     0   549    72   121]\n",
      " [    0 14621    63   254     1  1786   197   480]\n",
      " [    0   126  1612   284     2   584    75   470]\n",
      " [    0   281   155 24752    40   858    93   774]\n",
      " [    0    11     5   157   291    31     4    21]\n",
      " [   20  1835   255  1016     6 10818   942  1544]\n",
      " [    3   617   148   407     7  3199  1687  1069]\n",
      " [    2   436   280  1117     7  1734   458  5208]]\n",
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 692s 541ms/step - loss: 0.6822 - acc: 0.7465\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 686s 536ms/step - loss: 0.6616 - acc: 0.7507\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 683s 534ms/step - loss: 0.6444 - acc: 0.7556\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 685s 535ms/step - loss: 0.6319 - acc: 0.7608\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 678s 530ms/step - loss: 0.6286 - acc: 0.7603\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 678s 530ms/step - loss: 0.6210 - acc: 0.7637\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 685s 535ms/step - loss: 0.6145 - acc: 0.7659\n",
      "Epoch 8/50\n",
      "1280/1280 [==============================] - 687s 536ms/step - loss: 0.6127 - acc: 0.7668\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 682s 533ms/step - loss: 0.6113 - acc: 0.7662\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 679s 530ms/step - loss: 0.6055 - acc: 0.7687\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 689s 538ms/step - loss: 0.6005 - acc: 0.7701\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 689s 539ms/step - loss: 0.5974 - acc: 0.7714\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 689s 538ms/step - loss: 0.5953 - acc: 0.7724\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 692s 540ms/step - loss: 0.5930 - acc: 0.7730\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 694s 543ms/step - loss: 0.5878 - acc: 0.7751\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 698s 545ms/step - loss: 0.5826 - acc: 0.7762\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 694s 542ms/step - loss: 0.5802 - acc: 0.7772\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 700s 547ms/step - loss: 0.5799 - acc: 0.7779\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 698s 545ms/step - loss: 0.5812 - acc: 0.7774\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 692s 541ms/step - loss: 0.5743 - acc: 0.7801\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 696s 544ms/step - loss: 0.5704 - acc: 0.7812\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 693s 542ms/step - loss: 0.5726 - acc: 0.7807\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 689s 539ms/step - loss: 0.5736 - acc: 0.7803\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 696s 544ms/step - loss: 0.5684 - acc: 0.7826\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 716s 559ms/step - loss: 0.5659 - acc: 0.7829\n",
      "Epoch 26/50\n",
      "1280/1280 [==============================] - 737s 576ms/step - loss: 0.5625 - acc: 0.7835\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 714s 558ms/step - loss: 0.5624 - acc: 0.7842\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 712s 556ms/step - loss: 0.5599 - acc: 0.7855\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 711s 555ms/step - loss: 0.5577 - acc: 0.7863\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 707s 553ms/step - loss: 0.5551 - acc: 0.7878\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 725s 567ms/step - loss: 0.5537 - acc: 0.7876\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 725s 566ms/step - loss: 0.5503 - acc: 0.7877\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5499 - acc: 0.7888\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5473 - acc: 0.7893\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 717s 560ms/step - loss: 0.5421 - acc: 0.7919\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 712s 556ms/step - loss: 0.5448 - acc: 0.7908\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 716s 559ms/step - loss: 0.5454 - acc: 0.7900\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5435 - acc: 0.7904\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 716s 559ms/step - loss: 0.5448 - acc: 0.7907\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 716s 560ms/step - loss: 0.5380 - acc: 0.7929\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5395 - acc: 0.7919\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 717s 560ms/step - loss: 0.5403 - acc: 0.7931\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 725s 567ms/step - loss: 0.5358 - acc: 0.7939\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5393 - acc: 0.7931\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 715s 558ms/step - loss: 0.5322 - acc: 0.7954\n",
      "Epoch 46/50\n",
      "1280/1280 [==============================] - 717s 560ms/step - loss: 0.5313 - acc: 0.7956\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 713s 557ms/step - loss: 0.5308 - acc: 0.7964\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5289 - acc: 0.7952\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 717s 560ms/step - loss: 0.5267 - acc: 0.7966\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5255 - acc: 0.7978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.03      0.06      1011\n",
      "           1       0.84      0.88      0.86     17557\n",
      "           2       0.68      0.66      0.67      3210\n",
      "           3       0.91      0.94      0.92     27695\n",
      "           4       0.86      0.69      0.77       624\n",
      "           5       0.60      0.69      0.64     17289\n",
      "           6       0.50      0.31      0.38      7385\n",
      "           7       0.62      0.60      0.61      9457\n",
      "\n",
      "    accuracy                           0.76     84228\n",
      "   macro avg       0.73      0.60      0.61     84228\n",
      "weighted avg       0.75      0.76      0.75     84228\n",
      "\n",
      "Acur√°cia\n",
      "0.6005652117003493\n",
      "Precisao\n",
      "0.7524993866513776\n",
      "Recall\n",
      "0.7585482262430545\n",
      "F1\n",
      "0.7485095770771379\n",
      "[[   32    97    35    37     1   611    82   116]\n",
      " [    2 15374    51   166     0  1523   189   252]\n",
      " [    0    89  2122   213     2   455    60   269]\n",
      " [    0   134   139 26013    46   643   102   618]\n",
      " [    0     0     4   146   433    17     5    19]\n",
      " [    5  1665   301   770     9 11932  1288  1319]\n",
      " [    1   500   163   357     6  3136  2308   914]\n",
      " [    0   346   290  1000     9  1544   591  5677]]\n",
      "Epoch 1/50\n",
      "1280/1280 [==============================] - 717s 560ms/step - loss: 0.6142 - acc: 0.7696\n",
      "Epoch 2/50\n",
      "1280/1280 [==============================] - 721s 563ms/step - loss: 0.5989 - acc: 0.7733\n",
      "Epoch 3/50\n",
      "1280/1280 [==============================] - 716s 559ms/step - loss: 0.5822 - acc: 0.7779\n",
      "Epoch 4/50\n",
      "1280/1280 [==============================] - 715s 558ms/step - loss: 0.5769 - acc: 0.7803\n",
      "Epoch 5/50\n",
      "1280/1280 [==============================] - 716s 559ms/step - loss: 0.5706 - acc: 0.7822\n",
      "Epoch 6/50\n",
      "1280/1280 [==============================] - 716s 559ms/step - loss: 0.5612 - acc: 0.7853\n",
      "Epoch 7/50\n",
      "1280/1280 [==============================] - 726s 567ms/step - loss: 0.5581 - acc: 0.7868\n",
      "Epoch 8/50\n",
      "1280/1280 [==============================] - 715s 559ms/step - loss: 0.5570 - acc: 0.7863\n",
      "Epoch 9/50\n",
      "1280/1280 [==============================] - 717s 560ms/step - loss: 0.5591 - acc: 0.7857\n",
      "Epoch 10/50\n",
      "1280/1280 [==============================] - 725s 567ms/step - loss: 0.5495 - acc: 0.7895\n",
      "Epoch 11/50\n",
      "1280/1280 [==============================] - 716s 559ms/step - loss: 0.5482 - acc: 0.7903\n",
      "Epoch 12/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5447 - acc: 0.7907\n",
      "Epoch 13/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5410 - acc: 0.7929\n",
      "Epoch 14/50\n",
      "1280/1280 [==============================] - 715s 559ms/step - loss: 0.5418 - acc: 0.7928\n",
      "Epoch 15/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5391 - acc: 0.7926\n",
      "Epoch 16/50\n",
      "1280/1280 [==============================] - 719s 562ms/step - loss: 0.5365 - acc: 0.7946\n",
      "Epoch 17/50\n",
      "1280/1280 [==============================] - 723s 565ms/step - loss: 0.5324 - acc: 0.7948\n",
      "Epoch 18/50\n",
      "1280/1280 [==============================] - 725s 566ms/step - loss: 0.5352 - acc: 0.7953\n",
      "Epoch 19/50\n",
      "1280/1280 [==============================] - 719s 562ms/step - loss: 0.5322 - acc: 0.7947\n",
      "Epoch 20/50\n",
      "1280/1280 [==============================] - 730s 570ms/step - loss: 0.5296 - acc: 0.7958\n",
      "Epoch 21/50\n",
      "1280/1280 [==============================] - 724s 566ms/step - loss: 0.5283 - acc: 0.7972\n",
      "Epoch 22/50\n",
      "1280/1280 [==============================] - 723s 565ms/step - loss: 0.5284 - acc: 0.7967\n",
      "Epoch 23/50\n",
      "1280/1280 [==============================] - 726s 567ms/step - loss: 0.5261 - acc: 0.7978\n",
      "Epoch 24/50\n",
      "1280/1280 [==============================] - 726s 567ms/step - loss: 0.5224 - acc: 0.7989\n",
      "Epoch 25/50\n",
      "1280/1280 [==============================] - 725s 566ms/step - loss: 0.5201 - acc: 0.8004\n",
      "Epoch 26/50\n",
      "1280/1280 [==============================] - 720s 563ms/step - loss: 0.5203 - acc: 0.8000\n",
      "Epoch 27/50\n",
      "1280/1280 [==============================] - 725s 566ms/step - loss: 0.5183 - acc: 0.8003\n",
      "Epoch 28/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.5174 - acc: 0.8013\n",
      "Epoch 29/50\n",
      "1280/1280 [==============================] - 720s 563ms/step - loss: 0.5158 - acc: 0.8012\n",
      "Epoch 30/50\n",
      "1280/1280 [==============================] - 723s 565ms/step - loss: 0.5170 - acc: 0.8007\n",
      "Epoch 31/50\n",
      "1280/1280 [==============================] - 717s 560ms/step - loss: 0.5134 - acc: 0.8024\n",
      "Epoch 32/50\n",
      "1280/1280 [==============================] - 723s 565ms/step - loss: 0.5110 - acc: 0.8033\n",
      "Epoch 33/50\n",
      "1280/1280 [==============================] - 726s 567ms/step - loss: 0.5108 - acc: 0.8029\n",
      "Epoch 34/50\n",
      "1280/1280 [==============================] - 721s 563ms/step - loss: 0.5110 - acc: 0.8028\n",
      "Epoch 35/50\n",
      "1280/1280 [==============================] - 719s 562ms/step - loss: 0.5058 - acc: 0.8048\n",
      "Epoch 36/50\n",
      "1280/1280 [==============================] - 721s 563ms/step - loss: 0.5058 - acc: 0.8057\n",
      "Epoch 37/50\n",
      "1280/1280 [==============================] - 730s 570ms/step - loss: 0.5043 - acc: 0.8061\n",
      "Epoch 38/50\n",
      "1280/1280 [==============================] - 720s 562ms/step - loss: 0.5037 - acc: 0.8060\n",
      "Epoch 39/50\n",
      "1280/1280 [==============================] - 725s 567ms/step - loss: 0.5004 - acc: 0.8070\n",
      "Epoch 40/50\n",
      "1280/1280 [==============================] - 725s 566ms/step - loss: 0.5002 - acc: 0.8075\n",
      "Epoch 41/50\n",
      "1280/1280 [==============================] - 722s 564ms/step - loss: 0.5042 - acc: 0.8060\n",
      "Epoch 42/50\n",
      "1280/1280 [==============================] - 723s 565ms/step - loss: 0.5017 - acc: 0.8073\n",
      "Epoch 43/50\n",
      "1280/1280 [==============================] - 720s 563ms/step - loss: 0.5000 - acc: 0.8075\n",
      "Epoch 44/50\n",
      "1280/1280 [==============================] - 726s 567ms/step - loss: 0.4998 - acc: 0.8073\n",
      "Epoch 45/50\n",
      "1280/1280 [==============================] - 725s 567ms/step - loss: 0.4990 - acc: 0.8081\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 720s 563ms/step - loss: 0.4956 - acc: 0.8084\n",
      "Epoch 47/50\n",
      "1280/1280 [==============================] - 726s 567ms/step - loss: 0.4977 - acc: 0.8078\n",
      "Epoch 48/50\n",
      "1280/1280 [==============================] - 718s 561ms/step - loss: 0.4927 - acc: 0.8101\n",
      "Epoch 49/50\n",
      "1280/1280 [==============================] - 727s 568ms/step - loss: 0.4889 - acc: 0.8116\n",
      "Epoch 50/50\n",
      "1280/1280 [==============================] - 725s 566ms/step - loss: 0.4901 - acc: 0.8110\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.05      0.10      1057\n",
      "           1       0.88      0.90      0.89     17997\n",
      "           2       0.77      0.75      0.76      3168\n",
      "           3       0.92      0.96      0.94     27082\n",
      "           4       0.92      0.79      0.85       538\n",
      "           5       0.65      0.71      0.68     16996\n",
      "           6       0.58      0.37      0.45      7414\n",
      "           7       0.69      0.70      0.69      9461\n",
      "\n",
      "    accuracy                           0.80     83713\n",
      "   macro avg       0.75      0.66      0.67     83713\n",
      "weighted avg       0.79      0.80      0.79     83713\n",
      "\n",
      "Acur√°cia\n",
      "0.6552128195088502\n",
      "Precisao\n",
      "0.7876634756632821\n",
      "Recall\n",
      "0.7957903790331251\n",
      "F1\n",
      "0.7861316580091192\n",
      "[[   58    74    20    41     0   688    81    95]\n",
      " [    1 16150    40   158     0  1245   158   245]\n",
      " [    0    64  2389   156     2   259    48   250]\n",
      " [    0    55    78 26126    23   348    61   391]\n",
      " [    1     0     1    89   425     8     5     9]\n",
      " [   30  1391   238   752     1 12140  1267  1177]\n",
      " [    2   429   105   325     5  2948  2750   850]\n",
      " [    3   269   246   888     6  1110   359  6580]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "model = criarRede()\n",
    "\n",
    "for train_index, test_index in kf.split(previsores_trei_vald):\n",
    "  \n",
    "  train_and_evaluate_model(model, previsores_trei_vald[train_index], classes_trei_vald[train_index],\n",
    "                           previsores_trei_vald[test_index], classes_trei_vald[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 700, 20)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 700, 200)          96800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 700, 200)          240800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 700, 200)          240800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 700, 200)          240800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 820,808\n",
      "Trainable params: 820,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.3397924016597385, 0.4170390889929207, 0.5422367121851244, 0.6005652117003493, 0.6552128195088502]\n",
      "Precision total\n",
      "[0.5152537860271407, 0.6117065041056942, 0.7143682068413978, 0.7524993866513776, 0.7876634756632821]\n",
      "Recalls total\n",
      "[0.5398911592565971, 0.624006228030857, 0.7216036179184746, 0.7585482262430545, 0.7957903790331251]\n",
      "F1 total\n",
      "[0.5153906769780009, 0.6088217831426905, 0.7097562874981472, 0.7485095770771379, 0.7861316580091192]\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accuq8)\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "print('F1 total')\n",
    "print(f1q8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1600/1600 [==============================] - 1334s 834ms/step - loss: 0.5437 - acc: 0.7940\n",
      "Epoch 2/50\n",
      "1600/1600 [==============================] - 1321s 826ms/step - loss: 0.5477 - acc: 0.7914\n",
      "Epoch 3/50\n",
      "1600/1600 [==============================] - 1355s 847ms/step - loss: 0.5311 - acc: 0.7970\n",
      "Epoch 4/50\n",
      "1600/1600 [==============================] - 1327s 829ms/step - loss: 0.5218 - acc: 0.8003\n",
      "Epoch 5/50\n",
      "1600/1600 [==============================] - 1329s 831ms/step - loss: 0.5205 - acc: 0.8003\n",
      "Epoch 6/50\n",
      "1600/1600 [==============================] - 1330s 831ms/step - loss: 0.5154 - acc: 0.8021\n",
      "Epoch 7/50\n",
      "1600/1600 [==============================] - 1324s 827ms/step - loss: 0.5138 - acc: 0.8029\n",
      "Epoch 8/50\n",
      "1600/1600 [==============================] - 1330s 831ms/step - loss: 0.5129 - acc: 0.8027\n",
      "Epoch 9/50\n",
      "1600/1600 [==============================] - 1328s 830ms/step - loss: 0.5113 - acc: 0.8031\n",
      "Epoch 10/50\n",
      "1600/1600 [==============================] - 1311s 819ms/step - loss: 0.5087 - acc: 0.8043\n",
      "Epoch 11/50\n",
      "1600/1600 [==============================] - 1331s 832ms/step - loss: 0.5075 - acc: 0.8050\n",
      "Epoch 12/50\n",
      "1600/1600 [==============================] - 1323s 827ms/step - loss: 0.5044 - acc: 0.8058\n",
      "Epoch 13/50\n",
      "1600/1600 [==============================] - 1319s 824ms/step - loss: 0.5045 - acc: 0.8058\n",
      "Epoch 14/50\n",
      "1600/1600 [==============================] - 1320s 825ms/step - loss: 0.5038 - acc: 0.8063\n",
      "Epoch 15/50\n",
      "1600/1600 [==============================] - 1315s 822ms/step - loss: 0.5050 - acc: 0.8054\n",
      "Epoch 16/50\n",
      "1600/1600 [==============================] - 1317s 823ms/step - loss: 0.5023 - acc: 0.8066\n",
      "Epoch 17/50\n",
      "1600/1600 [==============================] - 1310s 819ms/step - loss: 0.5027 - acc: 0.8066\n",
      "Epoch 18/50\n",
      "1600/1600 [==============================] - 1316s 823ms/step - loss: 0.4989 - acc: 0.8077\n",
      "Epoch 19/50\n",
      "1600/1600 [==============================] - 1312s 820ms/step - loss: 0.4987 - acc: 0.8081\n",
      "Epoch 20/50\n",
      "1600/1600 [==============================] - 1313s 820ms/step - loss: 0.4978 - acc: 0.8085\n",
      "Epoch 21/50\n",
      "1600/1600 [==============================] - 1313s 820ms/step - loss: 0.4967 - acc: 0.8088\n",
      "Epoch 22/50\n",
      "1600/1600 [==============================] - 1315s 822ms/step - loss: 0.4971 - acc: 0.8080\n",
      "Epoch 23/50\n",
      "1600/1600 [==============================] - 1312s 820ms/step - loss: 0.4978 - acc: 0.8081\n",
      "Epoch 24/50\n",
      "1600/1600 [==============================] - 1315s 822ms/step - loss: 0.4963 - acc: 0.8090\n",
      "Epoch 25/50\n",
      "1600/1600 [==============================] - 1305s 816ms/step - loss: 0.4995 - acc: 0.8080\n",
      "Epoch 26/50\n",
      "1600/1600 [==============================] - 1312s 820ms/step - loss: 0.4966 - acc: 0.8090\n",
      "Epoch 27/50\n",
      "1600/1600 [==============================] - 1325s 828ms/step - loss: 0.4912 - acc: 0.8105\n",
      "Epoch 28/50\n",
      "1600/1600 [==============================] - 879s 550ms/step - loss: 0.4919 - acc: 0.8106\n",
      "Epoch 29/50\n",
      "1600/1600 [==============================] - 880s 550ms/step - loss: 0.4927 - acc: 0.8097\n",
      "Epoch 30/50\n",
      "1600/1600 [==============================] - 872s 545ms/step - loss: 0.4910 - acc: 0.8108\n",
      "Epoch 31/50\n",
      "1600/1600 [==============================] - 877s 548ms/step - loss: 0.4886 - acc: 0.8115\n",
      "Epoch 32/50\n",
      "1600/1600 [==============================] - 875s 547ms/step - loss: 0.4862 - acc: 0.8130\n",
      "Epoch 33/50\n",
      "1600/1600 [==============================] - 881s 551ms/step - loss: 0.4871 - acc: 0.8124\n",
      "Epoch 34/50\n",
      "1600/1600 [==============================] - 883s 552ms/step - loss: 0.4847 - acc: 0.8128\n",
      "Epoch 35/50\n",
      "1600/1600 [==============================] - 897s 560ms/step - loss: 0.4859 - acc: 0.8132\n",
      "Epoch 36/50\n",
      "1600/1600 [==============================] - 880s 550ms/step - loss: 0.4851 - acc: 0.8133\n",
      "Epoch 37/50\n",
      "1600/1600 [==============================] - 880s 550ms/step - loss: 0.4988 - acc: 0.8083\n",
      "Epoch 38/50\n",
      "1600/1600 [==============================] - 879s 549ms/step - loss: 0.4965 - acc: 0.8093\n",
      "Epoch 39/50\n",
      "1600/1600 [==============================] - 874s 546ms/step - loss: 0.4867 - acc: 0.8128\n",
      "Epoch 40/50\n",
      "1600/1600 [==============================] - 875s 547ms/step - loss: 0.4841 - acc: 0.8131\n",
      "Epoch 41/50\n",
      "1600/1600 [==============================] - 885s 553ms/step - loss: 0.4824 - acc: 0.8148\n",
      "Epoch 42/50\n",
      "1600/1600 [==============================] - 878s 549ms/step - loss: 0.4836 - acc: 0.8135\n",
      "Epoch 43/50\n",
      "1600/1600 [==============================] - 882s 551ms/step - loss: 0.4799 - acc: 0.8149\n",
      "Epoch 44/50\n",
      "1600/1600 [==============================] - 883s 552ms/step - loss: 0.4824 - acc: 0.8144\n",
      "Epoch 45/50\n",
      "1600/1600 [==============================] - 874s 546ms/step - loss: 0.4803 - acc: 0.8146\n",
      "Epoch 46/50\n",
      "1600/1600 [==============================] - 877s 548ms/step - loss: 0.4786 - acc: 0.8160\n",
      "Epoch 47/50\n",
      "1600/1600 [==============================] - 878s 549ms/step - loss: 0.4771 - acc: 0.8158\n",
      "Epoch 48/50\n",
      "1600/1600 [==============================] - 873s 546ms/step - loss: 0.4743 - acc: 0.8173\n",
      "Epoch 49/50\n",
      "1600/1600 [==============================] - 873s 546ms/step - loss: 0.4757 - acc: 0.8168\n",
      "Epoch 50/50\n",
      "1600/1600 [==============================] - 871s 544ms/step - loss: 0.4757 - acc: 0.8165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.03      0.06      1319\n",
      "           1       0.64      0.62      0.63     21565\n",
      "           2       0.37      0.27      0.31      4144\n",
      "           3       0.70      0.76      0.73     34429\n",
      "           4       0.59      0.29      0.39       723\n",
      "           5       0.45      0.52      0.48     20463\n",
      "           6       0.35      0.22      0.27      9014\n",
      "           7       0.38      0.42      0.40     11705\n",
      "\n",
      "    accuracy                           0.56    103362\n",
      "   macro avg       0.51      0.39      0.41    103362\n",
      "weighted avg       0.56      0.56      0.56    103362\n",
      "\n",
      "Acur√°cia\n",
      "0.3899513717611866\n",
      "Precisao\n",
      "0.5580777530385225\n",
      "Recall\n",
      "0.5647917029469244\n",
      "F1\n",
      "0.5550941167760556\n",
      "[[   39   240    33   196     0   574    88   149]\n",
      " [    3 13320   278  2854    14  3463   530  1103]\n",
      " [    1   374  1124   958     7   821   191   668]\n",
      " [    0  2166   462 26314    95  2812   529  2051]\n",
      " [    0    70    20   326   209    49    11    38]\n",
      " [   17  2687   465  2938    11 10557  1528  2260]\n",
      " [    3   957   235  1291     5  3046  1946  1531]\n",
      " [    1   872   405  2478    13  2250   817  4869]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model(model, previsores_trei_vald, classes_trei_vald,\n",
    "                           previsores_testes, classes_testes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
