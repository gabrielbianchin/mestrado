{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 10:44:50.348475  7992 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 10:44:50.358447  7992 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 10:44:50.359444  7992 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0820 10:44:50.360442  7992 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   3    4    5 ... 1997 1998 1999] TEST: [   0    1    2    6   11   13   16   21   33   34   44   49   50   56\n",
      "   59   60   74   77   79   83   88  104  113  117  121  125  126  136\n",
      "  151  156  160  162  163  168  173  176  177  182  184  186  189  200\n",
      "  203  216  224  228  232  238  239  247  261  266  280  282  284  289\n",
      "  294  297  303  313  315  316  319  329  333  344  353  362  364  373\n",
      "  374  376  378  379  385  392  393  395  398  410  419  424  431  437\n",
      "  441  442  444  445  469  473  484  489  492  496  497  504  505  509\n",
      "  512  518  519  523  524  525  540  544  549  550  555  556  561  570\n",
      "  571  572  576  577  589  594  595  596  597  601  603  607  610  614\n",
      "  627  628  636  637  643  648  656  677  691  697  704  705  719  721\n",
      "  730  731  741  742  746  751  756  758  762  770  772  777  778  783\n",
      "  788  789  794  797  799  805  817  820  821  828  830  845  850  851\n",
      "  853  857  858  865  872  880  882  885  896  901  909  914  916  919\n",
      "  920  921  925  932  934  936  941  948  953  960  966  977  979  981\n",
      "  989  997  998  999 1003 1006 1010 1013 1026 1046 1052 1054 1058 1063\n",
      " 1066 1071 1077 1081 1082 1090 1091 1092 1093 1095 1101 1103 1107 1110\n",
      " 1117 1119 1126 1132 1135 1141 1144 1145 1149 1152 1153 1157 1160 1165\n",
      " 1172 1173 1176 1178 1186 1193 1196 1198 1203 1207 1209 1219 1224 1230\n",
      " 1234 1235 1241 1246 1253 1263 1264 1271 1279 1284 1285 1288 1289 1291\n",
      " 1296 1298 1301 1302 1303 1317 1322 1338 1340 1345 1347 1349 1352 1358\n",
      " 1381 1391 1401 1409 1413 1419 1427 1429 1439 1443 1445 1447 1455 1456\n",
      " 1458 1466 1474 1496 1500 1509 1511 1519 1521 1531 1534 1535 1537 1541\n",
      " 1544 1547 1548 1554 1559 1567 1571 1572 1573 1576 1586 1587 1589 1592\n",
      " 1595 1604 1606 1615 1616 1618 1619 1621 1627 1628 1635 1638 1643 1644\n",
      " 1649 1650 1651 1652 1653 1660 1662 1673 1674 1675 1677 1688 1689 1693\n",
      " 1700 1701 1719 1720 1726 1727 1737 1738 1740 1743 1748 1752 1753 1755\n",
      " 1756 1758 1767 1768 1771 1775 1782 1790 1806 1811 1813 1814 1824 1831\n",
      " 1833 1834 1836 1848 1856 1857 1864 1867 1869 1870 1900 1907 1917 1934\n",
      " 1936 1950 1957 1968 1975 1983 1989 1992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 10:44:52.048926  7992 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 20s 12ms/sample - loss: 0.3682 - acc: 0.7865\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.3324 - acc: 0.8451\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.3149 - acc: 0.8565\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.3059 - acc: 0.8612\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.3003 - acc: 0.8644\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2970 - acc: 0.8656\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2944 - acc: 0.8671\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2924 - acc: 0.8686\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2943 - acc: 0.8673\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2894 - acc: 0.8699\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.2884 - acc: 0.8700\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2851 - acc: 0.8718\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2834 - acc: 0.8729\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2792 - acc: 0.8745\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2779 - acc: 0.8754\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2750 - acc: 0.8776\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2715 - acc: 0.8800\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2704 - acc: 0.8806\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2682 - acc: 0.8810\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2628 - acc: 0.8834\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2591 - acc: 0.8854\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2561 - acc: 0.8874\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2552 - acc: 0.8876\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2479 - acc: 0.8913\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2454 - acc: 0.8925\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2369 - acc: 0.8965\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2349 - acc: 0.8983\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2315 - acc: 0.9003\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2348 - acc: 0.8983\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2382 - acc: 0.8955\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2511 - acc: 0.8903\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2435 - acc: 0.8947\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2276 - acc: 0.9010\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2214 - acc: 0.9045\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2135 - acc: 0.9081\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2101 - acc: 0.9099\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2095 - acc: 0.9081\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2017 - acc: 0.9129\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1964 - acc: 0.9156\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1918 - acc: 0.9175\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1902 - acc: 0.9182\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1853 - acc: 0.9206\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1817 - acc: 0.9219\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1794 - acc: 0.9231\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1759 - acc: 0.9246\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1732 - acc: 0.9254\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.1719 - acc: 0.9264\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1682 - acc: 0.9274\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1777 - acc: 0.9233\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1674 - acc: 0.9277\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1623 - acc: 0.9300\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1583 - acc: 0.9316\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1571 - acc: 0.9323\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1541 - acc: 0.9330\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1527 - acc: 0.9338\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1507 - acc: 0.9352\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1491 - acc: 0.9357\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1462 - acc: 0.9371\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1449 - acc: 0.9377\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1423 - acc: 0.9392\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1410 - acc: 0.9392\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1407 - acc: 0.9392\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1378 - acc: 0.9402\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1363 - acc: 0.9411\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1344 - acc: 0.9423\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1336 - acc: 0.9435\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1315 - acc: 0.9443\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1306 - acc: 0.9445\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1299 - acc: 0.9450\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1276 - acc: 0.9459\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1266 - acc: 0.9459\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1256 - acc: 0.9458\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1233 - acc: 0.9473\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1232 - acc: 0.9476\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1231 - acc: 0.9476\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1209 - acc: 0.9489\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1196 - acc: 0.9491\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1191 - acc: 0.9494\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1169 - acc: 0.9494\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1158 - acc: 0.9506\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1163 - acc: 0.9502\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1153 - acc: 0.9512\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1151 - acc: 0.9508\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1122 - acc: 0.9518\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1114 - acc: 0.9524\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1107 - acc: 0.9529\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1099 - acc: 0.9533\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1087 - acc: 0.9535\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1077 - acc: 0.9534\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1072 - acc: 0.9539\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1058 - acc: 0.9550\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1054 - acc: 0.9543\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1056 - acc: 0.9543\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1048 - acc: 0.9551\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1036 - acc: 0.9555\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1024 - acc: 0.9560\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1017 - acc: 0.9553\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1012 - acc: 0.9566\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1014 - acc: 0.9559\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1001 - acc: 0.9560\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70     41798\n",
      "           1       0.65      0.60      0.62     23007\n",
      "           2       0.73      0.73      0.73     37504\n",
      "\n",
      "    accuracy                           0.69    102309\n",
      "   macro avg       0.69      0.68      0.68    102309\n",
      "weighted avg       0.69      0.69      0.69    102309\n",
      "\n",
      "Acur√°cia\n",
      "0.680328345106108\n",
      "Precisao\n",
      "0.6931110677420136\n",
      "Recall\n",
      "0.6937317342560283\n",
      "F1\n",
      "0.6929647361330042\n",
      "[[29859  4727  7212]\n",
      " [ 6298 13708  3001]\n",
      " [ 7584  2512 27408]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   8   17   23   55   58   75   76   84   86   92   97   99  109  118\n",
      "  119  123  133  137  141  149  152  153  157  158  159  167  172  175\n",
      "  178  180  191  193  194  197  198  202  220  227  229  235  236  240\n",
      "  251  254  255  257  262  267  271  273  276  278  286  293  300  305\n",
      "  307  310  312  321  331  334  337  340  343  346  349  350  352  357\n",
      "  366  367  372  375  377  380  383  389  394  396  401  405  406  411\n",
      "  415  421  429  432  446  451  453  456  457  470  474  481  488  490\n",
      "  494  502  507  510  527  528  541  545  551  554  560  563  564  567\n",
      "  578  580  582  583  585  592  599  600  602  604  605  613  615  616\n",
      "  625  632  640  647  661  663  667  669  690  692  696  703  706  711\n",
      "  712  716  718  726  748  761  763  764  768  775  785  798  811  818\n",
      "  823  835  838  841  843  844  848  862  870  884  889  891  894  897\n",
      "  899  900  903  904  910  912  923  926  927  933  935  939  940  942\n",
      "  952  954  956  961  964  965  968  969  970  982  983 1004 1005 1008\n",
      " 1021 1022 1027 1028 1029 1036 1043 1060 1061 1070 1073 1079 1084 1097\n",
      " 1102 1104 1113 1118 1121 1122 1123 1130 1131 1134 1137 1142 1166 1168\n",
      " 1179 1181 1188 1190 1192 1194 1202 1210 1216 1220 1225 1226 1227 1231\n",
      " 1233 1236 1237 1238 1239 1240 1252 1259 1261 1262 1266 1270 1276 1278\n",
      " 1280 1281 1293 1295 1300 1305 1306 1309 1323 1333 1335 1341 1343 1348\n",
      " 1364 1369 1371 1375 1380 1384 1388 1393 1397 1398 1400 1406 1407 1410\n",
      " 1416 1423 1424 1425 1434 1435 1437 1442 1446 1451 1459 1464 1465 1467\n",
      " 1469 1470 1473 1478 1487 1489 1492 1493 1512 1513 1514 1517 1522 1546\n",
      " 1549 1556 1557 1565 1568 1569 1591 1596 1599 1601 1603 1608 1612 1625\n",
      " 1630 1631 1632 1639 1665 1671 1690 1691 1692 1695 1708 1712 1713 1716\n",
      " 1722 1729 1732 1733 1739 1741 1754 1763 1772 1774 1776 1777 1792 1797\n",
      " 1800 1801 1817 1826 1830 1842 1843 1845 1846 1850 1852 1853 1862 1866\n",
      " 1868 1878 1882 1887 1888 1890 1894 1902 1903 1908 1909 1910 1923 1925\n",
      " 1926 1927 1929 1930 1933 1935 1939 1942 1946 1948 1953 1956 1961 1962\n",
      " 1963 1966 1971 1976 1981 1985 1993 1995]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.3664 - acc: 0.7288\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3374 - acc: 0.8407\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3144 - acc: 0.8567\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3043 - acc: 0.8626\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3011 - acc: 0.8642\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3011 - acc: 0.8641\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2988 - acc: 0.8643\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2927 - acc: 0.8670\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2898 - acc: 0.8688\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2884 - acc: 0.8697\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2849 - acc: 0.8704\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2820 - acc: 0.8724\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2813 - acc: 0.8728\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2791 - acc: 0.8737\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2742 - acc: 0.8760\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2720 - acc: 0.8778\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2677 - acc: 0.8799\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2645 - acc: 0.8818\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2602 - acc: 0.8839\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2594 - acc: 0.8837\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2550 - acc: 0.8871\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2496 - acc: 0.8898\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2456 - acc: 0.8912\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2412 - acc: 0.8936\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2366 - acc: 0.8954\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2307 - acc: 0.8980\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2242 - acc: 0.9012\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2223 - acc: 0.9023\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2182 - acc: 0.9045\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2121 - acc: 0.9073\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2103 - acc: 0.9080\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2070 - acc: 0.9099\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2041 - acc: 0.9100\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1964 - acc: 0.9114\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1924 - acc: 0.9143\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1886 - acc: 0.9166\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1856 - acc: 0.9178\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1846 - acc: 0.9164\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1789 - acc: 0.9197\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1748 - acc: 0.9221\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1721 - acc: 0.9234\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1695 - acc: 0.9251\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1684 - acc: 0.9242\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1644 - acc: 0.9263\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1607 - acc: 0.9287\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1577 - acc: 0.9283\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1557 - acc: 0.9291\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1526 - acc: 0.9309\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1524 - acc: 0.9307\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1492 - acc: 0.9324\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1466 - acc: 0.9343\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1446 - acc: 0.9335\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1433 - acc: 0.9351\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1422 - acc: 0.9353\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1399 - acc: 0.9363\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1372 - acc: 0.9377\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1369 - acc: 0.9365\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1353 - acc: 0.9377\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1320 - acc: 0.9393\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1304 - acc: 0.9377\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1290 - acc: 0.9403\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1280 - acc: 0.9422\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1268 - acc: 0.9434\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1249 - acc: 0.9430\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1231 - acc: 0.9434\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1228 - acc: 0.9438\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1218 - acc: 0.9444\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1212 - acc: 0.9455\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1191 - acc: 0.9461\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1178 - acc: 0.9469\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1170 - acc: 0.9471\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1154 - acc: 0.9486\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1152 - acc: 0.9474\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1142 - acc: 0.9489\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1136 - acc: 0.9496\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1134 - acc: 0.9497\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1120 - acc: 0.9500\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1100 - acc: 0.9507\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1080 - acc: 0.9515\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1094 - acc: 0.9506\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1082 - acc: 0.9514\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1095 - acc: 0.9511\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1072 - acc: 0.9514\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1044 - acc: 0.9520\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1036 - acc: 0.9528\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1038 - acc: 0.9525\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1025 - acc: 0.9536\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1023 - acc: 0.9521\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1012 - acc: 0.9540\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1019 - acc: 0.9540\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0999 - acc: 0.9558\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1002 - acc: 0.9559\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0996 - acc: 0.9546\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1000 - acc: 0.9550\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0976 - acc: 0.9564\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0970 - acc: 0.9569\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0965 - acc: 0.9570\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0959 - acc: 0.9570\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0955 - acc: 0.9563\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0954 - acc: 0.9558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.70     42390\n",
      "           1       0.64      0.61      0.62     22960\n",
      "           2       0.71      0.73      0.72     36943\n",
      "\n",
      "    accuracy                           0.69    102293\n",
      "   macro avg       0.68      0.68      0.68    102293\n",
      "weighted avg       0.69      0.69      0.69    102293\n",
      "\n",
      "Acur√°cia\n",
      "0.6780512116868812\n",
      "Precisao\n",
      "0.6882573969288281\n",
      "Recall\n",
      "0.6890500816282639\n",
      "F1\n",
      "0.6885019958791311\n",
      "[[29678  5032  7680]\n",
      " [ 5787 13918  3255]\n",
      " [ 7297  2757 26889]]\n",
      "TRAIN: [   0    1    2 ... 1995 1997 1998] TEST: [   4   10   26   27   29   35   40   57   61   66   67   68   69   72\n",
      "   78   80   89   91   93   95  102  103  107  111  115  116  127  129\n",
      "  130  140  148  150  166  171  185  187  201  206  209  211  212  218\n",
      "  241  242  249  250  252  253  265  268  269  274  277  285  287  298\n",
      "  301  304  309  311  317  318  322  330  339  342  345  356  359  363\n",
      "  368  370  381  386  387  390  400  408  413  420  423  426  430  433\n",
      "  438  440  448  449  458  459  461  462  465  466  472  477  482  487\n",
      "  499  501  503  521  522  532  536  538  539  543  552  553  562  565\n",
      "  579  581  588  591  606  611  617  618  619  620  631  634  638  641\n",
      "  642  646  650  651  652  653  658  660  662  668  670  679  680  682\n",
      "  694  695  700  715  728  733  737  738  739  743  745  766  774  782\n",
      "  791  793  796  802  809  810  825  829  832  836  837  840  849  854\n",
      "  860  861  863  864  869  873  874  876  877  879  881  892  907  908\n",
      "  911  917  928  930  937  944  946  947  955  957  971  980  994 1001\n",
      " 1015 1019 1030 1031 1032 1035 1037 1038 1040 1042 1045 1047 1048 1049\n",
      " 1053 1055 1059 1087 1089 1096 1106 1116 1128 1129 1133 1136 1147 1150\n",
      " 1154 1155 1163 1171 1174 1175 1177 1183 1184 1191 1195 1197 1205 1208\n",
      " 1213 1215 1218 1228 1243 1244 1245 1248 1250 1256 1257 1258 1269 1274\n",
      " 1277 1282 1294 1297 1310 1313 1315 1328 1329 1339 1350 1354 1361 1363\n",
      " 1365 1368 1373 1376 1378 1382 1396 1399 1421 1430 1431 1432 1441 1450\n",
      " 1453 1460 1461 1462 1463 1472 1475 1476 1480 1483 1486 1490 1498 1501\n",
      " 1505 1507 1508 1516 1520 1523 1525 1527 1530 1533 1536 1540 1550 1560\n",
      " 1566 1582 1584 1590 1605 1609 1611 1613 1617 1620 1623 1624 1640 1642\n",
      " 1647 1648 1658 1661 1663 1666 1667 1668 1678 1682 1685 1697 1699 1703\n",
      " 1704 1706 1707 1711 1715 1721 1724 1730 1734 1735 1736 1742 1745 1746\n",
      " 1750 1751 1759 1762 1769 1780 1785 1786 1793 1796 1802 1804 1809 1812\n",
      " 1815 1819 1822 1832 1835 1837 1839 1854 1871 1875 1877 1880 1881 1883\n",
      " 1884 1885 1886 1892 1895 1896 1912 1916 1918 1920 1922 1924 1945 1954\n",
      " 1959 1960 1969 1980 1986 1987 1996 1999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.3710 - acc: 0.7293\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3341 - acc: 0.8427\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3117 - acc: 0.8590\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3046 - acc: 0.8625\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2998 - acc: 0.8645\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2977 - acc: 0.8661\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2940 - acc: 0.8674\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2927 - acc: 0.8672\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2893 - acc: 0.8683\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2896 - acc: 0.8690\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2840 - acc: 0.8715\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2818 - acc: 0.8726\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2826 - acc: 0.8725\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2804 - acc: 0.8728\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2745 - acc: 0.8761\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2719 - acc: 0.8774\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2682 - acc: 0.8799\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2671 - acc: 0.8795\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2666 - acc: 0.8806\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2585 - acc: 0.8844\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2544 - acc: 0.8867\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2498 - acc: 0.8901\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2470 - acc: 0.8908\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2400 - acc: 0.8946\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2380 - acc: 0.8958\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2347 - acc: 0.8971\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2278 - acc: 0.9003\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2237 - acc: 0.9021\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2206 - acc: 0.9025\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2143 - acc: 0.9062\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2119 - acc: 0.9055\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2083 - acc: 0.9084\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2025 - acc: 0.9121\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1973 - acc: 0.9127\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1943 - acc: 0.9131\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1904 - acc: 0.9155\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1874 - acc: 0.9173\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1840 - acc: 0.9191\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1821 - acc: 0.9203\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1788 - acc: 0.9209\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1757 - acc: 0.9243\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1738 - acc: 0.9234\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1709 - acc: 0.9251\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1657 - acc: 0.9278\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1627 - acc: 0.9282\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1611 - acc: 0.9294\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1600 - acc: 0.9293\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1581 - acc: 0.9314\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1542 - acc: 0.9335\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1525 - acc: 0.9348\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1553 - acc: 0.9331\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1500 - acc: 0.9345\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1473 - acc: 0.9358\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1444 - acc: 0.9373\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1424 - acc: 0.9374\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1416 - acc: 0.9383\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1383 - acc: 0.9385\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1364 - acc: 0.9395\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1343 - acc: 0.9407\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1339 - acc: 0.9424\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1324 - acc: 0.9414\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1304 - acc: 0.9433\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1284 - acc: 0.9446\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1275 - acc: 0.9432\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1291 - acc: 0.9423\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1273 - acc: 0.9438\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1240 - acc: 0.9451\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1228 - acc: 0.9465\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1217 - acc: 0.9470\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1222 - acc: 0.9471\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1201 - acc: 0.9478\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1192 - acc: 0.9477\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1177 - acc: 0.9475\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1169 - acc: 0.9488\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1157 - acc: 0.9498\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1142 - acc: 0.9504\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1154 - acc: 0.9494\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1158 - acc: 0.9498\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1124 - acc: 0.9497\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1107 - acc: 0.9502\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1102 - acc: 0.9508\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1093 - acc: 0.9516\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1089 - acc: 0.9514\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1081 - acc: 0.9513\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1063 - acc: 0.9528\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1053 - acc: 0.9533\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1050 - acc: 0.9535\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1049 - acc: 0.9527\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1041 - acc: 0.9531\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1027 - acc: 0.9534\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1021 - acc: 0.9538\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1019 - acc: 0.9538\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1015 - acc: 0.9546\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1009 - acc: 0.9543\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1011 - acc: 0.9551\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1001 - acc: 0.9548\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0986 - acc: 0.9562\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0997 - acc: 0.9565\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0983 - acc: 0.9571\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0971 - acc: 0.9576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.71     41909\n",
      "           1       0.68      0.59      0.63     22643\n",
      "           2       0.73      0.75      0.74     38621\n",
      "\n",
      "    accuracy                           0.70    103173\n",
      "   macro avg       0.70      0.69      0.69    103173\n",
      "weighted avg       0.70      0.70      0.70    103173\n",
      "\n",
      "Acur√°cia\n",
      "0.6878495638982721\n",
      "Precisao\n",
      "0.7036790688131369\n",
      "Recall\n",
      "0.7046029484458143\n",
      "F1\n",
      "0.7032244447763936\n",
      "[[30271  4042  7596]\n",
      " [ 6154 13286  3203]\n",
      " [ 7212  2270 29139]]\n",
      "TRAIN: [   0    1    2 ... 1995 1996 1999] TEST: [   3    9   12   14   15   18   22   28   30   37   38   39   46   51\n",
      "   54   64   70   73   82   85   87   94  100  112  120  131  132  134\n",
      "  138  139  142  144  145  164  165  169  170  174  183  192  195  196\n",
      "  199  204  207  210  213  214  217  222  223  231  233  243  245  248\n",
      "  256  263  264  270  272  281  292  295  296  299  306  308  323  324\n",
      "  325  327  328  335  336  338  351  355  360  361  369  371  384  391\n",
      "  402  403  409  412  418  422  427  434  436  439  447  450  452  460\n",
      "  468  476  483  485  491  493  506  511  514  520  531  533  534  542\n",
      "  546  547  557  559  569  573  575  587  612  622  623  629  633  635\n",
      "  645  649  654  655  659  664  671  672  674  675  683  685  686  687\n",
      "  688  689  693  701  707  709  720  727  732  735  740  744  749  753\n",
      "  760  765  771  779  787  792  801  804  807  808  812  824  831  839\n",
      "  846  847  855  856  866  867  868  883  886  888  893  898  913  915\n",
      "  924  929  943  949  950  951  958  962  963  967  973  984  986  991\n",
      "  993  996 1002 1007 1009 1012 1017 1018 1020 1023 1039 1041 1050 1056\n",
      " 1057 1064 1065 1076 1078 1080 1083 1085 1088 1094 1098 1108 1109 1111\n",
      " 1112 1115 1127 1139 1140 1143 1146 1158 1159 1161 1164 1169 1180 1182\n",
      " 1187 1189 1199 1201 1217 1229 1242 1247 1249 1254 1260 1267 1268 1272\n",
      " 1273 1275 1283 1290 1292 1304 1308 1311 1312 1316 1318 1319 1324 1326\n",
      " 1327 1330 1331 1332 1336 1351 1355 1359 1366 1367 1370 1372 1379 1386\n",
      " 1389 1390 1395 1402 1403 1404 1408 1415 1418 1422 1426 1433 1438 1440\n",
      " 1449 1477 1479 1484 1488 1494 1497 1499 1506 1510 1515 1518 1529 1542\n",
      " 1543 1545 1551 1552 1553 1555 1558 1561 1562 1563 1570 1574 1583 1585\n",
      " 1600 1602 1607 1614 1626 1636 1637 1645 1655 1659 1676 1680 1681 1683\n",
      " 1684 1687 1694 1696 1702 1705 1710 1717 1718 1728 1731 1744 1749 1761\n",
      " 1770 1773 1779 1781 1787 1788 1795 1799 1807 1808 1821 1823 1825 1827\n",
      " 1840 1844 1847 1849 1860 1861 1863 1865 1876 1893 1899 1905 1911 1913\n",
      " 1921 1931 1932 1937 1938 1940 1941 1943 1944 1947 1958 1967 1972 1973\n",
      " 1974 1977 1978 1988 1990 1994 1997 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.3666 - acc: 0.7484\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3335 - acc: 0.8423\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3140 - acc: 0.8564\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3054 - acc: 0.8605\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3013 - acc: 0.8634\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2955 - acc: 0.8660\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2918 - acc: 0.8684\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2898 - acc: 0.8689\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2883 - acc: 0.8693\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2854 - acc: 0.8709\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2835 - acc: 0.8718\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2814 - acc: 0.8734\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2780 - acc: 0.8750\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2774 - acc: 0.8746\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2754 - acc: 0.8757\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2708 - acc: 0.8783\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2671 - acc: 0.8801\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2638 - acc: 0.8818\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2617 - acc: 0.8832\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2614 - acc: 0.8833\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2599 - acc: 0.8842\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2626 - acc: 0.8834\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2528 - acc: 0.8881\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2465 - acc: 0.8911\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2431 - acc: 0.8931\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2363 - acc: 0.8964\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2400 - acc: 0.8949\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2338 - acc: 0.8978\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2247 - acc: 0.9031\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2225 - acc: 0.9031\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2188 - acc: 0.9046\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2129 - acc: 0.9073\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2088 - acc: 0.9095\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2034 - acc: 0.9120\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2016 - acc: 0.9126\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1992 - acc: 0.9142\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1947 - acc: 0.9157\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1881 - acc: 0.9186\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1840 - acc: 0.9204\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1807 - acc: 0.9212\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1792 - acc: 0.9224\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1748 - acc: 0.9237\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1749 - acc: 0.9239\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1694 - acc: 0.9263\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1660 - acc: 0.9283\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1648 - acc: 0.9285\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1599 - acc: 0.9308\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1583 - acc: 0.9314\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1555 - acc: 0.9321\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1538 - acc: 0.9335\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1523 - acc: 0.9340\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1491 - acc: 0.9350\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1462 - acc: 0.9364\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1455 - acc: 0.9367\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1425 - acc: 0.9378\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1416 - acc: 0.9386\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1392 - acc: 0.9392\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1384 - acc: 0.9397\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1376 - acc: 0.9406\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1357 - acc: 0.9406\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1330 - acc: 0.9416\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1315 - acc: 0.9427\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1291 - acc: 0.9437\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1290 - acc: 0.9432\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1276 - acc: 0.9445\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1269 - acc: 0.9450\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.1239 - acc: 0.9458\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.1217 - acc: 0.9468\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1225 - acc: 0.9466\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1192 - acc: 0.9473\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1187 - acc: 0.9478\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1170 - acc: 0.9481\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1181 - acc: 0.9480\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1152 - acc: 0.9489\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1145 - acc: 0.9489\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1136 - acc: 0.9495\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1126 - acc: 0.9502\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.1118 - acc: 0.9501\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1101 - acc: 0.9505\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1088 - acc: 0.9515\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1090 - acc: 0.9520\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1085 - acc: 0.9516\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1075 - acc: 0.9520\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1071 - acc: 0.9526\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1055 - acc: 0.9527\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1061 - acc: 0.9520\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1040 - acc: 0.9532\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1035 - acc: 0.9534\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1023 - acc: 0.9531\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1006 - acc: 0.9544\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0999 - acc: 0.9546\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.0996 - acc: 0.9550\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1003 - acc: 0.9547\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1010 - acc: 0.9554\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1001 - acc: 0.9553\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0967 - acc: 0.9563\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0959 - acc: 0.9574\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0961 - acc: 0.9572\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.0951 - acc: 0.9575\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0945 - acc: 0.9579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69     43842\n",
      "           1       0.65      0.57      0.61     23889\n",
      "           2       0.70      0.73      0.72     38770\n",
      "\n",
      "    accuracy                           0.68    106501\n",
      "   macro avg       0.68      0.67      0.67    106501\n",
      "weighted avg       0.68      0.68      0.68    106501\n",
      "\n",
      "Acur√°cia\n",
      "0.6659930019958127\n",
      "Precisao\n",
      "0.6800400257973842\n",
      "Recall\n",
      "0.6811767025661731\n",
      "F1\n",
      "0.6798570029510789\n",
      "[[30701  4836  8305]\n",
      " [ 6606 13593  3690]\n",
      " [ 7992  2526 28252]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   5    7   19   20   24   25   31   32   36   41   42   43   45   47\n",
      "   48   52   53   62   63   65   71   81   90   96   98  101  105  106\n",
      "  108  110  114  122  124  128  135  143  146  147  154  155  161  179\n",
      "  181  188  190  205  208  215  219  221  225  226  230  234  237  244\n",
      "  246  258  259  260  275  279  283  288  290  291  302  314  320  326\n",
      "  332  341  347  348  354  358  365  382  388  397  399  404  407  414\n",
      "  416  417  425  428  435  443  454  455  463  464  467  471  475  478\n",
      "  479  480  486  495  498  500  508  513  515  516  517  526  529  530\n",
      "  535  537  548  558  566  568  574  584  586  590  593  598  608  609\n",
      "  621  624  626  630  639  644  657  665  666  673  676  678  681  684\n",
      "  698  699  702  708  710  713  714  717  722  723  724  725  729  734\n",
      "  736  747  750  752  754  755  757  759  767  769  773  776  780  781\n",
      "  784  786  790  795  800  803  806  813  814  815  816  819  822  826\n",
      "  827  833  834  842  852  859  871  875  878  887  890  895  902  905\n",
      "  906  918  922  931  938  945  959  972  974  975  976  978  985  987\n",
      "  988  990  992  995 1000 1011 1014 1016 1024 1025 1033 1034 1044 1051\n",
      " 1062 1067 1068 1069 1072 1074 1075 1086 1099 1100 1105 1114 1120 1124\n",
      " 1125 1138 1148 1151 1156 1162 1167 1170 1185 1200 1204 1206 1211 1212\n",
      " 1214 1221 1222 1223 1232 1251 1255 1265 1286 1287 1299 1307 1314 1320\n",
      " 1321 1325 1334 1337 1342 1344 1346 1353 1356 1357 1360 1362 1374 1377\n",
      " 1383 1385 1387 1392 1394 1405 1411 1412 1414 1417 1420 1428 1436 1444\n",
      " 1448 1452 1454 1457 1468 1471 1481 1482 1485 1491 1495 1502 1503 1504\n",
      " 1524 1526 1528 1532 1538 1539 1564 1575 1577 1578 1579 1580 1581 1588\n",
      " 1593 1594 1597 1598 1610 1622 1629 1633 1634 1641 1646 1654 1656 1657\n",
      " 1664 1669 1670 1672 1679 1686 1698 1709 1714 1723 1725 1747 1757 1760\n",
      " 1764 1765 1766 1778 1783 1784 1789 1791 1794 1798 1803 1805 1810 1816\n",
      " 1818 1820 1828 1829 1838 1841 1851 1855 1858 1859 1872 1873 1874 1879\n",
      " 1889 1891 1897 1898 1901 1904 1906 1914 1915 1919 1928 1949 1951 1952\n",
      " 1955 1964 1965 1970 1979 1982 1984 1991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.3690 - acc: 0.7787\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3392 - acc: 0.8405\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3129 - acc: 0.8579\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.3050 - acc: 0.8621\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.3002 - acc: 0.8646\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2970 - acc: 0.8667\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2954 - acc: 0.8669\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2915 - acc: 0.8689\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2879 - acc: 0.8698\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2885 - acc: 0.8699\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2852 - acc: 0.8715\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2813 - acc: 0.8736\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2782 - acc: 0.8748\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2743 - acc: 0.8768\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2746 - acc: 0.8768\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2699 - acc: 0.8789\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2671 - acc: 0.8811\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2652 - acc: 0.8812\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2612 - acc: 0.8828\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2558 - acc: 0.8862\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2526 - acc: 0.8880\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2464 - acc: 0.8908\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2484 - acc: 0.8902\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2399 - acc: 0.8935\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2348 - acc: 0.8967\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2317 - acc: 0.8975\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2251 - acc: 0.8999\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2219 - acc: 0.9028\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2190 - acc: 0.9042\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2135 - acc: 0.9058\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2080 - acc: 0.9081\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2048 - acc: 0.9094\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2049 - acc: 0.9101\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1987 - acc: 0.9128\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1925 - acc: 0.9156\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1900 - acc: 0.9172\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1860 - acc: 0.9191\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1830 - acc: 0.9209\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1826 - acc: 0.9208\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1771 - acc: 0.9235\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1740 - acc: 0.9247\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1716 - acc: 0.9258\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1683 - acc: 0.9265\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1665 - acc: 0.9268\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1632 - acc: 0.9293\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1600 - acc: 0.9314\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1584 - acc: 0.9322\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1559 - acc: 0.9323\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1537 - acc: 0.9337\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1507 - acc: 0.9342\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1483 - acc: 0.9364\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1469 - acc: 0.9373\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1447 - acc: 0.9378\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1432 - acc: 0.9383\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1405 - acc: 0.9390\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1388 - acc: 0.9399\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1371 - acc: 0.9409\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1349 - acc: 0.9416\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1338 - acc: 0.9422\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1325 - acc: 0.9414\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1310 - acc: 0.9428\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1296 - acc: 0.9435\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1277 - acc: 0.9444\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1279 - acc: 0.9451\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1272 - acc: 0.9452\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1246 - acc: 0.9464\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1228 - acc: 0.9469\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1211 - acc: 0.9475\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.1209 - acc: 0.9481\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1188 - acc: 0.9487\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1181 - acc: 0.9484\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1171 - acc: 0.9495\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1157 - acc: 0.9497\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1159 - acc: 0.9491\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1142 - acc: 0.9505\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1139 - acc: 0.9509\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1130 - acc: 0.9513\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1114 - acc: 0.9521\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1102 - acc: 0.9527\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1104 - acc: 0.9523\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1089 - acc: 0.9526\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1085 - acc: 0.9535\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1067 - acc: 0.9535\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1070 - acc: 0.9533\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1052 - acc: 0.9543\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1060 - acc: 0.9533\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1046 - acc: 0.9542\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1038 - acc: 0.9544\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1032 - acc: 0.9545\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.1029 - acc: 0.9550\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.1016 - acc: 0.9556\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.1015 - acc: 0.9561\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.1008 - acc: 0.9559\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.0997 - acc: 0.9566\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.0989 - acc: 0.9570\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.0990 - acc: 0.9569\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 18s 11ms/sample - loss: 0.0977 - acc: 0.9574\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.0974 - acc: 0.9575\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0985 - acc: 0.9568\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.0983 - acc: 0.9564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70     41659\n",
      "           1       0.64      0.61      0.62     22697\n",
      "           2       0.74      0.70      0.72     37176\n",
      "\n",
      "    accuracy                           0.69    101532\n",
      "   macro avg       0.68      0.68      0.68    101532\n",
      "weighted avg       0.69      0.69      0.69    101532\n",
      "\n",
      "Acur√°cia\n",
      "0.6782691195276168\n",
      "Precisao\n",
      "0.690721368810973\n",
      "Recall\n",
      "0.689979513847851\n",
      "F1\n",
      "0.6897716334343099\n",
      "[[30142  4825  6692]\n",
      " [ 6277 13849  2571]\n",
      " [ 8151  2961 26064]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_24 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_25 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 1,306,203\n",
      "Trainable params: 1,306,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.680328345106108, 0.6780512116868812, 0.6878495638982721, 0.6659930019958127, 0.6782691195276168]\n",
      "0.6780982484429382\n",
      "Precision total\n",
      "[0.6931110677420136, 0.6882573969288281, 0.7036790688131369, 0.6800400257973842, 0.690721368810973]\n",
      "0.6911617856184671\n",
      "Recalls total\n",
      "[0.6937317342560283, 0.6890500816282639, 0.7046029484458143, 0.6811767025661731, 0.689979513847851]\n",
      "0.6917081961488261\n",
      "F1 total\n",
      "[0.6929647361330042, 0.6885019958791311, 0.7032244447763936, 0.6798570029510789, 0.6897716334343099]\n",
      "0.6908639626347834\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
