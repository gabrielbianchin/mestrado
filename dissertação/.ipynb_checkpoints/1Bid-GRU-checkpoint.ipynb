{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 700, 20)\n",
      "(5, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:3500,0:20].values\n",
    "previsores = np.reshape(previsores, (5, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:3500, 28:31].values\n",
    "classes = np.reshape(classes, (5, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "    print(predicted.shape)\n",
    "    print(predicted)\n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "    \n",
    "    print(predicted.shape)\n",
    "    print(predicted)\n",
    "    print(y_test.shape)\n",
    "    print(y_test)\n",
    "    \n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    print(predicted.shape)\n",
    "    print(predicted)\n",
    "    print(y_test.shape)\n",
    "    print(y_test)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 197ms/sample - loss: 0.1619 - acc: 0.8243\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 26ms/sample - loss: 0.1582 - acc: 0.0714\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 25ms/sample - loss: 0.1542 - acc: 0.0871\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 26ms/sample - loss: 0.1495 - acc: 0.0879\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 24ms/sample - loss: 0.1453 - acc: 0.0893\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.1409 - acc: 0.0864\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.1366 - acc: 0.0864\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.1317 - acc: 0.0857\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.1273 - acc: 0.0857\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.1253 - acc: 0.0857\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.1221 - acc: 0.0857\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.1220 - acc: 0.0857\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.1196 - acc: 0.0907\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.1158 - acc: 0.1129\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.1146 - acc: 0.1679\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.1103 - acc: 0.1364\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.1037 - acc: 0.1136\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.1027 - acc: 0.1007\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0952 - acc: 0.1229\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0933 - acc: 0.5000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0949 - acc: 0.8650\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0934 - acc: 0.9307\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0904 - acc: 0.9029\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0904 - acc: 0.1500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0881 - acc: 0.8971\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0891 - acc: 0.9186\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0876 - acc: 0.9143\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0903 - acc: 0.7879\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0857 - acc: 0.6779\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0853 - acc: 0.7636\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0850 - acc: 0.6029\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0836 - acc: 0.2993\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0854 - acc: 0.2200\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0833 - acc: 0.3586\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0823 - acc: 0.6414\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0813 - acc: 0.7950\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0799 - acc: 0.8071\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0804 - acc: 0.8429\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0781 - acc: 0.8879\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0786 - acc: 0.9021\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0799 - acc: 0.9193\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0774 - acc: 0.9129\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0778 - acc: 0.9071\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0762 - acc: 0.9143\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0756 - acc: 0.9136\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0741 - acc: 0.9150\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0731 - acc: 0.9043\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0751 - acc: 0.8971\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0734 - acc: 0.9043\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0730 - acc: 0.9079\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0731 - acc: 0.9036\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0733 - acc: 0.8979\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0716 - acc: 0.9050\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0703 - acc: 0.9214\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0710 - acc: 0.9271\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0693 - acc: 0.9236\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0706 - acc: 0.9307\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0692 - acc: 0.9436\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0687 - acc: 0.9564\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0677 - acc: 0.9521\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0697 - acc: 0.9450\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 20ms/sample - loss: 0.0673 - acc: 0.9457\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0665 - acc: 0.9486\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0670 - acc: 0.9464\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0675 - acc: 0.9450\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0652 - acc: 0.9414\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0670 - acc: 0.9457\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0657 - acc: 0.9486\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0646 - acc: 0.9429\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0645 - acc: 0.9543\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0642 - acc: 0.9507\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0636 - acc: 0.9493\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0636 - acc: 0.9443\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0644 - acc: 0.9507\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0630 - acc: 0.9679\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 20ms/sample - loss: 0.0628 - acc: 0.9736\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0640 - acc: 0.9543\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0616 - acc: 0.9621\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0620 - acc: 0.9571\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0619 - acc: 0.9321\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0602 - acc: 0.9343\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0615 - acc: 0.9593\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0598 - acc: 0.9586\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0608 - acc: 0.9443\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0595 - acc: 0.9257\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0588 - acc: 0.9264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 24ms/sample - loss: 0.0586 - acc: 0.9314\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0587 - acc: 0.9200\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0585 - acc: 0.9307\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0569 - acc: 0.9600\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0586 - acc: 0.9614\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0576 - acc: 0.9436\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0575 - acc: 0.9436\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0554 - acc: 0.9486\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0561 - acc: 0.9564\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0549 - acc: 0.9607\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 21ms/sample - loss: 0.0539 - acc: 0.9714\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 23ms/sample - loss: 0.0542 - acc: 0.9771\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0534 - acc: 0.9614\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 22ms/sample - loss: 0.0555 - acc: 0.9521\n",
      "(3, 700, 3)\n",
      "[[[0.7751508  0.21106048 0.01378879]\n",
      "  [0.76086634 0.21869192 0.02044173]\n",
      "  [0.7972701  0.17672056 0.02600936]\n",
      "  ...\n",
      "  [0.44620645 0.19829623 0.35549733]\n",
      "  [0.44881222 0.20819785 0.34299   ]\n",
      "  [0.45066896 0.21902592 0.33030513]]\n",
      "\n",
      " [[0.57759106 0.17311199 0.24929693]\n",
      "  [0.63357365 0.1439767  0.22244966]\n",
      "  [0.58058596 0.13224448 0.2871696 ]\n",
      "  ...\n",
      "  [0.44597477 0.19819422 0.35583106]\n",
      "  [0.44860014 0.20809634 0.34330356]\n",
      "  [0.45047653 0.2189252  0.33059826]]\n",
      "\n",
      " [[0.6626686  0.11284675 0.22448465]\n",
      "  [0.5262822  0.09875824 0.37495962]\n",
      "  [0.4675087  0.08193569 0.45055565]\n",
      "  ...\n",
      "  [0.44620645 0.19829623 0.35549733]\n",
      "  [0.44881222 0.20819785 0.34299   ]\n",
      "  [0.45066896 0.21902592 0.33030513]]]\n",
      "(1090, 3)\n",
      "[[0.7751508  0.21106048 0.01378879]\n",
      " [0.76086634 0.21869192 0.02044173]\n",
      " [0.7972701  0.17672056 0.02600936]\n",
      " ...\n",
      " [0.7644112  0.21808238 0.01750647]\n",
      " [0.7589208  0.22634745 0.01473171]\n",
      " [0.7752885  0.20883304 0.01587837]]\n",
      "(1090, 3)\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "(1090,)\n",
      "[0 0 0 ... 0 0 0]\n",
      "(1090,)\n",
      "[0 0 0 ... 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.84      0.59       491\n",
      "           1       0.26      0.03      0.05       167\n",
      "           2       0.35      0.14      0.20       432\n",
      "\n",
      "    accuracy                           0.44      1090\n",
      "   macro avg       0.36      0.34      0.28      1090\n",
      "weighted avg       0.38      0.44      0.35      1090\n",
      "\n",
      "Acurácia\n",
      "0.33520602116277365\n",
      "Precisao\n",
      "0.3838497275681133\n",
      "Recall\n",
      "0.43669724770642204\n",
      "F1\n",
      "0.3525740007108254\n",
      "[[412   7  72]\n",
      " [123   5  39]\n",
      " [366   7  59]]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 137ms/sample - loss: 0.5685 - acc: 0.5605\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 17ms/sample - loss: 0.5586 - acc: 0.6129\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 17ms/sample - loss: 0.5500 - acc: 0.6605\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 17ms/sample - loss: 0.5417 - acc: 0.6786\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.5344 - acc: 0.6843\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.5279 - acc: 0.6933\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 17ms/sample - loss: 0.5201 - acc: 0.7210\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.5180 - acc: 0.7048\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.5182 - acc: 0.7224\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.5192 - acc: 0.7405\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.5146 - acc: 0.7390\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.5123 - acc: 0.7405\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.5091 - acc: 0.7467\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.5050 - acc: 0.7595\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.5029 - acc: 0.7681\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.4987 - acc: 0.7848\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4983 - acc: 0.7767\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4937 - acc: 0.7652\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4906 - acc: 0.7400\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4867 - acc: 0.7290\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4820 - acc: 0.7357\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4776 - acc: 0.7505\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4710 - acc: 0.7362\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.4688 - acc: 0.7495\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4636 - acc: 0.7348\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4664 - acc: 0.7786\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.4555 - acc: 0.7852\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.4543 - acc: 0.7329\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.4474 - acc: 0.7548\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4434 - acc: 0.7833\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4420 - acc: 0.7900\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.4397 - acc: 0.7781\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4373 - acc: 0.7814\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4323 - acc: 0.7771\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.4292 - acc: 0.7705\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4235 - acc: 0.7619\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4150 - acc: 0.7929\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4129 - acc: 0.7986\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4226 - acc: 0.7267\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4487 - acc: 0.7857\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4340 - acc: 0.7929\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4151 - acc: 0.8124\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.4067 - acc: 0.8119\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4196 - acc: 0.7819\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 17ms/sample - loss: 0.4170 - acc: 0.7781\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.4162 - acc: 0.7962\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4144 - acc: 0.8029\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4086 - acc: 0.8100\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4095 - acc: 0.8138\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4064 - acc: 0.8143\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.4054 - acc: 0.8162\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.4034 - acc: 0.8171\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.4020 - acc: 0.8224\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3977 - acc: 0.8319\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3945 - acc: 0.8290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3907 - acc: 0.8238\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3891 - acc: 0.8190\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3866 - acc: 0.8195\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3869 - acc: 0.8124\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3833 - acc: 0.8090\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3820 - acc: 0.8129\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3785 - acc: 0.8210\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3752 - acc: 0.8167\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3707 - acc: 0.8243\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3695 - acc: 0.8281\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3680 - acc: 0.8324\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.3660 - acc: 0.8329\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3639 - acc: 0.8319\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.3594 - acc: 0.8343\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 16ms/sample - loss: 0.3582 - acc: 0.8357\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3538 - acc: 0.8395\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3519 - acc: 0.8338\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3499 - acc: 0.8443\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3463 - acc: 0.8400\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3429 - acc: 0.8452\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3426 - acc: 0.8414\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3390 - acc: 0.8376\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3328 - acc: 0.8490\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3329 - acc: 0.8457\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3220 - acc: 0.8533\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.3231 - acc: 0.8538\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3622 - acc: 0.8286\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.3638 - acc: 0.8267\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3585 - acc: 0.8400\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3512 - acc: 0.8567\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3536 - acc: 0.8586\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3564 - acc: 0.8610\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.3532 - acc: 0.8543\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3504 - acc: 0.8452\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3425 - acc: 0.8505\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3412 - acc: 0.8462\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3421 - acc: 0.8605\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3382 - acc: 0.8552\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.3300 - acc: 0.8643\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 14ms/sample - loss: 0.3228 - acc: 0.8610\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3242 - acc: 0.8529\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3206 - acc: 0.8538\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3205 - acc: 0.8590\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3156 - acc: 0.8681\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 15ms/sample - loss: 0.3159 - acc: 0.8610\n",
      "(2, 700, 3)\n",
      "[[[0.41051438 0.03430432 0.55518126]\n",
      "  [0.36613816 0.02157421 0.61228764]\n",
      "  [0.36295202 0.02818485 0.6088631 ]\n",
      "  ...\n",
      "  [0.44266987 0.21617615 0.34115404]\n",
      "  [0.4272315  0.22698116 0.34578732]\n",
      "  [0.41152695 0.24020705 0.348266  ]]\n",
      "\n",
      " [[0.38343558 0.09650704 0.5200574 ]\n",
      "  [0.49341792 0.12705298 0.37952906]\n",
      "  [0.5408484  0.11192514 0.34722653]\n",
      "  ...\n",
      "  [0.44266987 0.21617615 0.34115404]\n",
      "  [0.4272315  0.22698116 0.34578732]\n",
      "  [0.41152695 0.24020705 0.348266  ]]]\n",
      "(207, 3)\n",
      "[[0.41051438 0.03430432 0.55518126]\n",
      " [0.36613816 0.02157421 0.61228764]\n",
      " [0.36295202 0.02818485 0.6088631 ]\n",
      " [0.37775424 0.04190654 0.5803392 ]\n",
      " [0.4243086  0.04462753 0.53106385]\n",
      " [0.48382917 0.06924374 0.44692704]\n",
      " [0.5702969  0.0662226  0.36348054]\n",
      " [0.5493388  0.10032196 0.35033923]\n",
      " [0.5844397  0.12340127 0.292159  ]\n",
      " [0.63888174 0.11699333 0.24412495]\n",
      " [0.6627593  0.12773569 0.20950502]\n",
      " [0.7004825  0.13317592 0.16634163]\n",
      " [0.6852312  0.17015752 0.14461128]\n",
      " [0.65605    0.22216462 0.12178536]\n",
      " [0.57816106 0.3357249  0.08611403]\n",
      " [0.61353284 0.32229048 0.06417665]\n",
      " [0.6684214  0.29065168 0.04092693]\n",
      " [0.75487053 0.22156571 0.02356379]\n",
      " [0.720173   0.2519184  0.02790867]\n",
      " [0.7204387  0.25729266 0.02226873]\n",
      " [0.6998089  0.28130966 0.01888146]\n",
      " [0.64970666 0.3324881  0.01780525]\n",
      " [0.7496853  0.23385322 0.01646148]\n",
      " [0.7081662  0.2688521  0.02298175]\n",
      " [0.68456376 0.29290175 0.02253449]\n",
      " [0.6398004  0.3359527  0.02424687]\n",
      " [0.7518319  0.22339931 0.02476875]\n",
      " [0.6966864  0.2603607  0.04295287]\n",
      " [0.79695725 0.15899833 0.04404437]\n",
      " [0.7372193  0.18127607 0.08150462]\n",
      " [0.78953767 0.11725566 0.09320673]\n",
      " [0.84264666 0.0749279  0.08242547]\n",
      " [0.82409126 0.06204696 0.11386176]\n",
      " [0.7440204  0.06822884 0.18775082]\n",
      " [0.78039974 0.0564445  0.16315576]\n",
      " [0.73621076 0.04631391 0.21747532]\n",
      " [0.7069795  0.03439138 0.25862914]\n",
      " [0.7510742  0.04183522 0.20709054]\n",
      " [0.61889315 0.04808857 0.33301827]\n",
      " [0.6038631  0.05999072 0.33614618]\n",
      " [0.6171755  0.06869067 0.31413382]\n",
      " [0.5318761  0.08020155 0.38792232]\n",
      " [0.58318794 0.07200006 0.34481195]\n",
      " [0.45699668 0.07306076 0.46994254]\n",
      " [0.4135509  0.1080606  0.4783885 ]\n",
      " [0.39807975 0.13803394 0.4638863 ]\n",
      " [0.4699001  0.10792399 0.42217588]\n",
      " [0.3827448  0.12101194 0.4962433 ]\n",
      " [0.42269036 0.14109986 0.43620974]\n",
      " [0.48287806 0.1479074  0.36921448]\n",
      " [0.5147982  0.17671694 0.30848485]\n",
      " [0.43479285 0.18710747 0.3780997 ]\n",
      " [0.45452452 0.2149125  0.33056292]\n",
      " [0.42168808 0.22285937 0.35545257]\n",
      " [0.36400947 0.24015974 0.3958308 ]\n",
      " [0.3797958  0.24743249 0.37277168]\n",
      " [0.4441534  0.20544705 0.3503995 ]\n",
      " [0.60874814 0.15471792 0.23653388]\n",
      " [0.522269   0.14133735 0.33639362]\n",
      " [0.5391866  0.15718947 0.30362394]\n",
      " [0.5865548  0.10764682 0.30579838]\n",
      " [0.5439046  0.12690291 0.3291925 ]\n",
      " [0.517955   0.14991021 0.33213475]\n",
      " [0.5896731  0.10040723 0.30991966]\n",
      " [0.51903087 0.10368981 0.3772793 ]\n",
      " [0.5590694  0.10739223 0.33353832]\n",
      " [0.5913844  0.08245602 0.32615954]\n",
      " [0.60552186 0.10789637 0.28658178]\n",
      " [0.7134946  0.08396276 0.20254269]\n",
      " [0.6555866  0.09873468 0.24567871]\n",
      " [0.7436813  0.09827611 0.15804255]\n",
      " [0.68875355 0.1280583  0.18318816]\n",
      " [0.73479825 0.12246983 0.14273193]\n",
      " [0.715021   0.16726936 0.1177096 ]\n",
      " [0.7566126  0.15227729 0.09111015]\n",
      " [0.7859243  0.14316416 0.0709115 ]\n",
      " [0.7830855  0.14355808 0.07335635]\n",
      " [0.73900443 0.19758342 0.06341221]\n",
      " [0.76434153 0.18037643 0.05528197]\n",
      " [0.38343558 0.09650704 0.5200574 ]\n",
      " [0.49341792 0.12705298 0.37952906]\n",
      " [0.5408484  0.11192514 0.34722653]\n",
      " [0.53739506 0.08906674 0.3735382 ]\n",
      " [0.58869654 0.1345044  0.27679905]\n",
      " [0.56568146 0.18754189 0.24677666]\n",
      " [0.5799517  0.19941716 0.22063115]\n",
      " [0.5530574  0.29736745 0.14957519]\n",
      " [0.56527233 0.3255299  0.10919779]\n",
      " [0.575372   0.34884885 0.07577918]\n",
      " [0.5673704  0.3838236  0.04880604]\n",
      " [0.49967006 0.45494345 0.04538649]\n",
      " [0.48334116 0.4818538  0.03480509]\n",
      " [0.5644244  0.4127101  0.02286553]\n",
      " [0.55005014 0.43044353 0.01950634]\n",
      " [0.67121845 0.31303313 0.0157484 ]\n",
      " [0.7160724  0.26203412 0.02189347]\n",
      " [0.85342693 0.12545468 0.02111842]\n",
      " [0.90115994 0.07518584 0.02365418]\n",
      " [0.90602803 0.04201156 0.05196038]\n",
      " [0.89078236 0.02229965 0.08691797]\n",
      " [0.83943564 0.0161956  0.14436877]\n",
      " [0.56022733 0.01474314 0.42502946]\n",
      " [0.45341375 0.00700781 0.53957844]\n",
      " [0.2734369  0.00686532 0.7196978 ]\n",
      " [0.19956921 0.00640197 0.7940288 ]\n",
      " [0.18141115 0.00723091 0.811358  ]\n",
      " [0.17173302 0.01102314 0.8172439 ]\n",
      " [0.24543445 0.01603935 0.73852617]\n",
      " [0.32804808 0.02274342 0.64920855]\n",
      " [0.40919212 0.03439377 0.5564141 ]\n",
      " [0.477187   0.05563547 0.4671775 ]\n",
      " [0.5925032  0.06531302 0.34218383]\n",
      " [0.6951499  0.05305046 0.2517996 ]\n",
      " [0.7728287  0.06393037 0.16324094]\n",
      " [0.82475954 0.07756432 0.09767605]\n",
      " [0.8086106  0.07498152 0.11640791]\n",
      " [0.83546025 0.06271522 0.10182452]\n",
      " [0.8595054  0.06410505 0.0763895 ]\n",
      " [0.82644314 0.07302208 0.10053477]\n",
      " [0.7976488  0.09357831 0.10877295]\n",
      " [0.8242907  0.078831   0.09687828]\n",
      " [0.7904938  0.07051685 0.13898928]\n",
      " [0.80607593 0.05321768 0.14070636]\n",
      " [0.79199594 0.03017342 0.1778306 ]\n",
      " [0.7264362  0.02390913 0.24965456]\n",
      " [0.717219   0.0155872  0.26719376]\n",
      " [0.4659135  0.00918067 0.5249058 ]\n",
      " [0.34681365 0.00791216 0.6452742 ]\n",
      " [0.27646956 0.00609669 0.71743375]\n",
      " [0.22912483 0.00621187 0.7646633 ]\n",
      " [0.14025305 0.00510139 0.8546456 ]\n",
      " [0.12025865 0.0056196  0.8741217 ]\n",
      " [0.16090675 0.00804527 0.83104795]\n",
      " [0.09327243 0.00952168 0.89720595]\n",
      " [0.10415748 0.01192962 0.883913  ]\n",
      " [0.09567544 0.01285917 0.89146537]\n",
      " [0.12023403 0.02383923 0.85592675]\n",
      " [0.17177805 0.02663471 0.8015872 ]\n",
      " [0.24580772 0.04343807 0.7107542 ]\n",
      " [0.29408434 0.07623562 0.62968004]\n",
      " [0.32754657 0.10087422 0.5715793 ]\n",
      " [0.3129156  0.10478971 0.58229476]\n",
      " [0.4004298  0.11656285 0.4830073 ]\n",
      " [0.47860646 0.11788338 0.40351015]\n",
      " [0.5361216  0.12045433 0.34342405]\n",
      " [0.63558584 0.13414457 0.23026957]\n",
      " [0.5935176  0.13477527 0.27170715]\n",
      " [0.7562415  0.08572048 0.15803806]\n",
      " [0.77311504 0.06474682 0.16213813]\n",
      " [0.7885765  0.05292998 0.1584935 ]\n",
      " [0.72281766 0.05148334 0.22569904]\n",
      " [0.7457692  0.04037086 0.21385996]\n",
      " [0.7366805  0.0347616  0.22855785]\n",
      " [0.73782784 0.032929   0.2292431 ]\n",
      " [0.7300097  0.02190753 0.24808289]\n",
      " [0.7084127  0.01575317 0.27583414]\n",
      " [0.57241875 0.01208909 0.41549212]\n",
      " [0.57425493 0.00727182 0.4184733 ]\n",
      " [0.45622307 0.00761356 0.53616333]\n",
      " [0.45210844 0.00606844 0.54182315]\n",
      " [0.3405915  0.00716029 0.65224814]\n",
      " [0.3036586  0.00820328 0.6881381 ]\n",
      " [0.28364465 0.00865393 0.7077014 ]\n",
      " [0.25382775 0.00782259 0.7383496 ]\n",
      " [0.27496636 0.00737151 0.7176621 ]\n",
      " [0.2578736  0.00929763 0.7328288 ]\n",
      " [0.26372412 0.01668109 0.7195947 ]\n",
      " [0.26658723 0.0157536  0.71765924]\n",
      " [0.2843793  0.01839933 0.6972214 ]\n",
      " [0.25383213 0.0276814  0.7184864 ]\n",
      " [0.3102716  0.05620645 0.633522  ]\n",
      " [0.38034937 0.07062364 0.549027  ]\n",
      " [0.47336888 0.09360757 0.43302348]\n",
      " [0.49751693 0.15023008 0.35225302]\n",
      " [0.5366381  0.20250122 0.26086068]\n",
      " [0.6562046  0.19056751 0.1532279 ]\n",
      " [0.6261607  0.2627546  0.1110848 ]\n",
      " [0.6835468  0.23127635 0.08517692]\n",
      " [0.7861958  0.14327583 0.07052833]\n",
      " [0.8209986  0.10926421 0.06973723]\n",
      " [0.8472038  0.07555414 0.07724205]\n",
      " [0.8071642  0.05793472 0.13490105]\n",
      " [0.8622743  0.032009   0.10571676]\n",
      " [0.7522225  0.03503788 0.21273969]\n",
      " [0.74163085 0.02099564 0.23737352]\n",
      " [0.63952243 0.02284978 0.33762786]\n",
      " [0.54290825 0.02229905 0.4347927 ]\n",
      " [0.5490826  0.02417063 0.4267468 ]\n",
      " [0.5358975  0.02455312 0.4395494 ]\n",
      " [0.5283874  0.03242315 0.43918937]\n",
      " [0.6433969  0.03510895 0.32149422]\n",
      " [0.6376826  0.0484036  0.31391373]\n",
      " [0.6868894  0.07445452 0.23865607]\n",
      " [0.62982446 0.11014397 0.26003164]\n",
      " [0.67621297 0.11268973 0.2110973 ]\n",
      " [0.6732096  0.15893738 0.16785295]\n",
      " [0.61603487 0.20674609 0.177219  ]\n",
      " [0.4965341  0.34269917 0.1607667 ]\n",
      " [0.5325985  0.3234954  0.14390618]\n",
      " [0.47248742 0.37823394 0.14927861]\n",
      " [0.5349107  0.32285634 0.14223301]\n",
      " [0.49726358 0.3693671  0.13336931]\n",
      " [0.56183183 0.27967143 0.15849672]\n",
      " [0.66692513 0.21380197 0.11927291]\n",
      " [0.608289   0.21315326 0.17855777]\n",
      " [0.66493666 0.19619794 0.1388655 ]\n",
      " [0.5739593  0.24930301 0.17673768]]\n",
      "(207, 3)\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "(207,)\n",
      "[2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 2 2 0 2 2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(207,)\n",
      "[0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0\n",
      " 0 0 0 0 0 0 1 0 0 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 1 0 0 1 0 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 2 2 2 2 2 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 2 2 2\n",
      " 2 0 2 2 2 2 2 2 0 0 0 0 0 2 2 2 0 0 0 0 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.75      0.47        71\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.53      0.23      0.32       120\n",
      "\n",
      "    accuracy                           0.39       207\n",
      "   macro avg       0.29      0.32      0.26       207\n",
      "weighted avg       0.42      0.39      0.34       207\n",
      "\n",
      "Acurácia\n",
      "0.3238262910798122\n",
      "Precisao\n",
      "0.42343578085266065\n",
      "Recall\n",
      "0.3864734299516908\n",
      "F1\n",
      "0.34323150659000823\n",
      "[[53  0 18]\n",
      " [10  0  6]\n",
      " [93  0 27]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 2, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_5 (Bidirection (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 98,203\n",
      "Trainable params: 98,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.6596148944652023, 0.6714656794531297, 0.6482967316992191, 0.6524325998920619, 0.6650930165699743]\n",
      "0.6593805844159175\n",
      "Precision total\n",
      "[0.6747171318444809, 0.6879631238845804, 0.6647046384203084, 0.6747211947437044, 0.6795907843321171]\n",
      "0.6763393746450382\n",
      "Recalls total\n",
      "[0.6749291092206904, 0.6883567974625661, 0.6655946068631143, 0.674877748257672, 0.6799988387171696]\n",
      "0.6767514201042426\n",
      "F1 total\n",
      "[0.6740183598222594, 0.6873677368688802, 0.6640232744069822, 0.6728592091604129, 0.6785176761005911]\n",
      "0.6753572512718252\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
