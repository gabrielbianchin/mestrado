{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNGRU, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0822 09:24:49.282961  9300 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 09:24:49.288946  9300 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 09:24:49.289913  9300 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 09:24:49.290944  9300 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    2    3 ... 1996 1997 1999] TEST: [   1   10   12   13   32   35   36   40   53   54   58   62   69   76\n",
      "  109  112  114  115  121  125  126  129  132  134  136  137  140  141\n",
      "  142  147  158  159  165  167  172  175  178  182  190  194  206  214\n",
      "  218  222  225  227  233  251  264  266  277  281  282  285  286  293\n",
      "  294  298  299  304  312  313  344  347  352  356  361  363  364  367\n",
      "  368  370  373  375  379  394  396  403  406  424  427  430  432  436\n",
      "  449  450  457  460  461  462  464  477  485  487  494  497  498  500\n",
      "  502  507  508  511  515  516  517  522  526  527  535  538  543  555\n",
      "  560  568  569  571  572  580  581  584  593  601  603  606  612  613\n",
      "  639  643  649  655  658  663  667  675  682  683  684  695  700  702\n",
      "  714  715  717  718  730  731  733  745  748  760  762  763  764  766\n",
      "  769  775  776  782  784  786  789  796  800  813  828  830  834  839\n",
      "  840  842  846  850  864  870  882  885  888  896  901  908  921  922\n",
      "  923  924  925  936  942  943  945  951  956  959  963  966  967  971\n",
      "  972  975  976  978  982  985  987  991 1002 1009 1018 1026 1028 1031\n",
      " 1039 1048 1049 1050 1055 1056 1070 1073 1077 1081 1086 1089 1090 1091\n",
      " 1102 1106 1108 1109 1118 1123 1125 1127 1134 1135 1139 1146 1149 1151\n",
      " 1163 1167 1169 1173 1174 1177 1184 1206 1210 1211 1212 1215 1222 1231\n",
      " 1233 1234 1236 1240 1241 1253 1255 1256 1258 1266 1269 1270 1284 1285\n",
      " 1286 1296 1303 1307 1310 1316 1322 1324 1331 1338 1346 1350 1351 1353\n",
      " 1354 1358 1374 1379 1380 1383 1387 1416 1430 1433 1437 1449 1451 1454\n",
      " 1471 1473 1474 1475 1479 1486 1488 1494 1497 1515 1516 1519 1523 1546\n",
      " 1547 1550 1555 1558 1563 1579 1580 1583 1585 1594 1596 1598 1599 1613\n",
      " 1616 1621 1622 1627 1653 1657 1658 1661 1664 1671 1675 1679 1687 1695\n",
      " 1699 1706 1715 1722 1728 1730 1739 1742 1745 1749 1753 1757 1764 1772\n",
      " 1775 1776 1784 1789 1790 1793 1809 1812 1814 1815 1816 1821 1823 1824\n",
      " 1830 1835 1836 1837 1839 1842 1843 1849 1853 1855 1856 1861 1864 1865\n",
      " 1868 1874 1875 1883 1894 1909 1910 1911 1919 1923 1934 1940 1941 1943\n",
      " 1951 1962 1965 1969 1988 1990 1992 1998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 09:24:50.126676  9300 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3389 - acc: 0.8030\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3138 - acc: 0.8578\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3073 - acc: 0.8617\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3030 - acc: 0.8644\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3017 - acc: 0.8648\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2982 - acc: 0.8666\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2973 - acc: 0.8676\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2940 - acc: 0.8691\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2923 - acc: 0.8698\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2902 - acc: 0.8714\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2872 - acc: 0.8726\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2846 - acc: 0.8739\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2831 - acc: 0.8747\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2829 - acc: 0.8745\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2805 - acc: 0.8762\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2792 - acc: 0.8765\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2781 - acc: 0.8777\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2765 - acc: 0.8778\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2748 - acc: 0.8791\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2709 - acc: 0.8811\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2693 - acc: 0.8813\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2695 - acc: 0.8819\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2597 - acc: 0.8860\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2553 - acc: 0.8887\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2528 - acc: 0.8894\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2490 - acc: 0.8916\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2472 - acc: 0.8920\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2445 - acc: 0.8931\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2425 - acc: 0.8943\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2410 - acc: 0.8947\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2398 - acc: 0.8953\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2361 - acc: 0.8971\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2341 - acc: 0.8977\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2330 - acc: 0.8982\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2306 - acc: 0.8989\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2281 - acc: 0.9000\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2263 - acc: 0.9010\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2253 - acc: 0.9012\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2221 - acc: 0.9023\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2211 - acc: 0.9021\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2191 - acc: 0.9034\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2170 - acc: 0.9044\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2142 - acc: 0.9056\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2128 - acc: 0.9066\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2108 - acc: 0.9072\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2089 - acc: 0.9080\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2074 - acc: 0.9088\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2056 - acc: 0.9097\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2046 - acc: 0.9101\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2024 - acc: 0.9111\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1998 - acc: 0.9123\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1987 - acc: 0.9124\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1972 - acc: 0.9132\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1959 - acc: 0.9142\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1942 - acc: 0.9145\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1919 - acc: 0.9155\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1905 - acc: 0.9164\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1887 - acc: 0.9174\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1872 - acc: 0.9175\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1852 - acc: 0.9190\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1845 - acc: 0.9199\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1833 - acc: 0.9203\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1816 - acc: 0.9210\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1811 - acc: 0.9213\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1794 - acc: 0.9222\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1780 - acc: 0.9230\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1764 - acc: 0.9237\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1749 - acc: 0.9240\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1737 - acc: 0.9250\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1738 - acc: 0.9245\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1717 - acc: 0.9255\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1702 - acc: 0.9260\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1693 - acc: 0.9269\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1680 - acc: 0.9272\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1672 - acc: 0.9277\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1668 - acc: 0.9278\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1659 - acc: 0.9281\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1639 - acc: 0.9289\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1643 - acc: 0.9295\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1620 - acc: 0.9303\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1608 - acc: 0.9307\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1601 - acc: 0.9313\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1591 - acc: 0.9317\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1587 - acc: 0.9318\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1575 - acc: 0.9325\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1570 - acc: 0.9333\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1552 - acc: 0.9339\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1552 - acc: 0.9336\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1539 - acc: 0.9340\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1533 - acc: 0.9345\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1530 - acc: 0.9342\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1520 - acc: 0.9348\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1508 - acc: 0.9355\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1499 - acc: 0.9363\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1494 - acc: 0.9365\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1489 - acc: 0.9365\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1475 - acc: 0.9370\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1472 - acc: 0.9372\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1466 - acc: 0.9374\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1451 - acc: 0.9381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73     42188\n",
      "           1       0.71      0.60      0.65     23017\n",
      "           2       0.76      0.79      0.77     37425\n",
      "\n",
      "    accuracy                           0.73    102630\n",
      "   macro avg       0.73      0.71      0.72    102630\n",
      "weighted avg       0.73      0.73      0.73    102630\n",
      "\n",
      "Acurácia\n",
      "0.7108818622988616\n",
      "Precisao\n",
      "0.7278275997809438\n",
      "Recall\n",
      "0.7283055636753386\n",
      "F1\n",
      "0.7266779039576208\n",
      "[[31506  4030  6652]\n",
      " [ 6734 13779  2504]\n",
      " [ 6395  1569 29461]]\n",
      "TRAIN: [   1    3    4 ... 1997 1998 1999] TEST: [   0    2    8   11   14   17   18   19   22   23   27   29   30   44\n",
      "   47   63   73   74   75   77   81   82   84   85   88   90   95  106\n",
      "  107  108  119  143  155  162  163  169  173  184  188  210  219  229\n",
      "  230  234  236  244  249  252  256  257  259  261  279  280  283  287\n",
      "  291  295  301  302  319  321  328  339  341  345  346  349  351  384\n",
      "  405  409  416  418  421  422  428  431  435  439  446  456  459  463\n",
      "  468  472  496  504  509  510  512  519  525  529  536  540  541  550\n",
      "  551  553  556  558  585  588  592  602  605  607  608  611  624  627\n",
      "  635  636  637  640  644  645  648  650  651  654  656  662  669  671\n",
      "  673  677  681  686  688  693  701  704  705  719  720  721  724  725\n",
      "  726  735  738  744  751  768  780  781  785  793  797  816  818  819\n",
      "  823  826  829  833  835  848  855  858  860  869  872  877  878  879\n",
      "  881  884  886  887  890  895  897  900  912  913  915  919  931  940\n",
      "  941  944  946  947  949  954  960  961  969  980  983  995  996  998\n",
      " 1003 1008 1010 1012 1013 1014 1017 1019 1021 1022 1024 1034 1038 1041\n",
      " 1042 1052 1058 1064 1065 1067 1074 1075 1078 1094 1097 1098 1101 1105\n",
      " 1114 1115 1116 1131 1140 1145 1148 1154 1155 1157 1165 1171 1172 1183\n",
      " 1186 1191 1197 1203 1208 1221 1223 1235 1242 1246 1250 1257 1262 1278\n",
      " 1282 1288 1297 1298 1299 1311 1314 1315 1318 1319 1332 1336 1337 1345\n",
      " 1352 1357 1360 1361 1362 1368 1371 1375 1384 1385 1391 1394 1400 1407\n",
      " 1408 1411 1412 1413 1422 1423 1426 1428 1429 1431 1443 1445 1455 1458\n",
      " 1459 1469 1477 1481 1491 1493 1498 1499 1500 1510 1520 1522 1529 1530\n",
      " 1531 1532 1539 1543 1545 1548 1557 1564 1566 1574 1586 1602 1604 1610\n",
      " 1615 1617 1619 1631 1632 1634 1635 1640 1646 1647 1651 1665 1667 1668\n",
      " 1672 1674 1677 1678 1681 1691 1696 1701 1704 1710 1712 1713 1718 1724\n",
      " 1733 1744 1752 1754 1758 1770 1778 1795 1797 1799 1801 1805 1810 1818\n",
      " 1822 1826 1832 1834 1847 1848 1851 1854 1858 1869 1871 1873 1879 1887\n",
      " 1889 1900 1901 1907 1914 1926 1927 1929 1930 1938 1944 1953 1956 1961\n",
      " 1963 1971 1974 1977 1982 1984 1986 1995]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3342 - acc: 0.8358\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3139 - acc: 0.8582\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3066 - acc: 0.8623\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3021 - acc: 0.8647\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3000 - acc: 0.8654\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2976 - acc: 0.8669\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2944 - acc: 0.8682\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2921 - acc: 0.8697\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2906 - acc: 0.8704\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2906 - acc: 0.8706\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2846 - acc: 0.8729\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2843 - acc: 0.8736\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2820 - acc: 0.8746\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2811 - acc: 0.8753\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2798 - acc: 0.8758\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2788 - acc: 0.8762\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2756 - acc: 0.8780\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2744 - acc: 0.8784\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2715 - acc: 0.8804\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2669 - acc: 0.8829\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2622 - acc: 0.8857\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2571 - acc: 0.8887\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2548 - acc: 0.8894\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2515 - acc: 0.8909\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2495 - acc: 0.8925\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2478 - acc: 0.8928\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2460 - acc: 0.8937\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2423 - acc: 0.8955\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2422 - acc: 0.8959\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2397 - acc: 0.8967\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2380 - acc: 0.8971\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2352 - acc: 0.8987\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2333 - acc: 0.8997\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2319 - acc: 0.9004\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2298 - acc: 0.9013\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2266 - acc: 0.9027\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2273 - acc: 0.9023\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2240 - acc: 0.9036\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2209 - acc: 0.9053\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2182 - acc: 0.9064\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2173 - acc: 0.9069\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2154 - acc: 0.9079\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2133 - acc: 0.9088\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2124 - acc: 0.9091\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2093 - acc: 0.9104\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2085 - acc: 0.9108\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2060 - acc: 0.9120\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2038 - acc: 0.9130\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2018 - acc: 0.9139\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2000 - acc: 0.9148\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1983 - acc: 0.9153\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1977 - acc: 0.9156\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1960 - acc: 0.9166\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1943 - acc: 0.9173\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1933 - acc: 0.9177\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1909 - acc: 0.9188\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1882 - acc: 0.9198\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1865 - acc: 0.9206\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1854 - acc: 0.9212\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1850 - acc: 0.9213\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1826 - acc: 0.9226\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1809 - acc: 0.9232\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1797 - acc: 0.9235\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1792 - acc: 0.9240\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1777 - acc: 0.9246\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1755 - acc: 0.9259\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1748 - acc: 0.9262\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1729 - acc: 0.9270\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1715 - acc: 0.9274\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1706 - acc: 0.9278\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1699 - acc: 0.9283\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1676 - acc: 0.9290\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1667 - acc: 0.9294\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1659 - acc: 0.9298\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1652 - acc: 0.9302\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1634 - acc: 0.9312\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1626 - acc: 0.9315\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1622 - acc: 0.9318\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1605 - acc: 0.9326\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1586 - acc: 0.9331\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1582 - acc: 0.9334\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1585 - acc: 0.9330\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1573 - acc: 0.9340\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1552 - acc: 0.9345\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1553 - acc: 0.9345\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1539 - acc: 0.9353\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1531 - acc: 0.9358\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1525 - acc: 0.9359\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1510 - acc: 0.9369\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1502 - acc: 0.9371\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1494 - acc: 0.9374\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1489 - acc: 0.9375\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1484 - acc: 0.9380\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1467 - acc: 0.9387\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1464 - acc: 0.9387\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1454 - acc: 0.9390\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1448 - acc: 0.9394\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1440 - acc: 0.9401\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1436 - acc: 0.9400\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1425 - acc: 0.9405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.74      0.71     42288\n",
      "           1       0.68      0.59      0.63     23249\n",
      "           2       0.77      0.75      0.76     37256\n",
      "\n",
      "    accuracy                           0.71    102793\n",
      "   macro avg       0.71      0.69      0.70    102793\n",
      "weighted avg       0.71      0.71      0.71    102793\n",
      "\n",
      "Acurácia\n",
      "0.6944941986547039\n",
      "Precisao\n",
      "0.7110932565471653\n",
      "Recall\n",
      "0.7104569377292228\n",
      "F1\n",
      "0.7095720255019374\n",
      "[[31304  4742  6242]\n",
      " [ 7148 13805  2296]\n",
      " [ 7562  1773 27921]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [  25   26   28   34   37   39   41   46   49   51   56   57   61   66\n",
      "   67   68   70   71   78   79   86   89   96   97  100  101  110  116\n",
      "  122  123  124  130  131  138  139  145  149  157  164  166  168  174\n",
      "  186  192  193  195  200  203  208  211  228  239  263  267  268  270\n",
      "  272  274  276  284  288  289  290  297  303  305  308  309  310  311\n",
      "  314  316  318  323  324  333  334  340  342  343  348  355  357  359\n",
      "  371  380  385  386  389  390  395  397  400  404  408  410  413  415\n",
      "  419  423  433  437  438  440  451  453  466  471  474  475  482  489\n",
      "  490  495  499  503  518  521  523  524  532  533  539  542  544  546\n",
      "  548  561  563  567  575  576  577  594  599  600  610  620  621  622\n",
      "  628  631  634  638  646  653  659  660  665  666  680  689  692  696\n",
      "  697  699  703  706  707  710  713  722  723  734  736  746  747  749\n",
      "  750  752  756  758  773  777  779  790  801  805  808  812  821  822\n",
      "  824  825  836  845  847  854  856  857  861  862  863  873  874  875\n",
      "  892  894  899  903  904  905  909  914  917  918  926  927  930  934\n",
      "  937  939  948  952  953  955  957  977  984  986 1000 1011 1015 1016\n",
      " 1023 1032 1033 1035 1043 1053 1059 1068 1071 1072 1076 1083 1085 1087\n",
      " 1088 1092 1103 1107 1113 1122 1126 1129 1136 1141 1152 1158 1166 1170\n",
      " 1179 1180 1189 1195 1198 1201 1204 1205 1228 1237 1238 1252 1260 1261\n",
      " 1263 1267 1271 1273 1274 1293 1295 1306 1309 1313 1317 1320 1325 1326\n",
      " 1327 1328 1334 1359 1363 1370 1377 1386 1388 1392 1397 1398 1424 1427\n",
      " 1432 1438 1442 1444 1446 1447 1453 1460 1462 1468 1480 1487 1489 1490\n",
      " 1495 1506 1508 1509 1521 1525 1527 1528 1536 1540 1541 1544 1553 1554\n",
      " 1560 1562 1565 1569 1570 1588 1590 1593 1600 1606 1609 1611 1614 1641\n",
      " 1642 1643 1648 1649 1652 1656 1659 1676 1684 1694 1708 1716 1717 1723\n",
      " 1729 1734 1746 1747 1750 1751 1760 1761 1768 1774 1780 1781 1786 1791\n",
      " 1807 1808 1811 1813 1825 1828 1831 1841 1857 1859 1860 1862 1863 1884\n",
      " 1890 1896 1898 1902 1904 1912 1916 1917 1920 1931 1932 1935 1937 1942\n",
      " 1950 1959 1964 1970 1975 1979 1987 1991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3393 - acc: 0.8387\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3163 - acc: 0.8566\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3108 - acc: 0.8602\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3048 - acc: 0.8636\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3033 - acc: 0.8645\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3012 - acc: 0.8653\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2979 - acc: 0.8667\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2950 - acc: 0.8686\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2941 - acc: 0.8686\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2919 - acc: 0.8700\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2922 - acc: 0.8696\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2870 - acc: 0.8725\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2873 - acc: 0.8723\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2850 - acc: 0.8734\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2839 - acc: 0.8747\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2809 - acc: 0.8748\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2788 - acc: 0.8768\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2789 - acc: 0.8769\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2761 - acc: 0.8786\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2741 - acc: 0.8798\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2710 - acc: 0.8813\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2671 - acc: 0.8831\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2611 - acc: 0.8860\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2563 - acc: 0.8884\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2540 - acc: 0.8896\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2514 - acc: 0.8909\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2492 - acc: 0.8920\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2480 - acc: 0.8923\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2446 - acc: 0.8942\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2425 - acc: 0.8951\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2419 - acc: 0.8953\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2390 - acc: 0.8967\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2363 - acc: 0.8981\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2375 - acc: 0.8975\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2331 - acc: 0.8997\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2309 - acc: 0.9003\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2280 - acc: 0.9019\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2270 - acc: 0.9023\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2244 - acc: 0.9037\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2226 - acc: 0.9046\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2211 - acc: 0.9051\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2198 - acc: 0.9058\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2165 - acc: 0.9071\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2147 - acc: 0.9080\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2122 - acc: 0.9092\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2103 - acc: 0.9101\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2095 - acc: 0.9104\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2067 - acc: 0.9117\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2050 - acc: 0.9125\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2031 - acc: 0.9131\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2027 - acc: 0.9133\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2004 - acc: 0.9143\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1985 - acc: 0.9149\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1960 - acc: 0.9164\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1946 - acc: 0.9167\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1937 - acc: 0.9173\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1916 - acc: 0.9182\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1906 - acc: 0.9183\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1903 - acc: 0.9186\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1869 - acc: 0.9202\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1869 - acc: 0.9207\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1845 - acc: 0.9213\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1832 - acc: 0.9219\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1815 - acc: 0.9223\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1792 - acc: 0.9236\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1793 - acc: 0.9236\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1786 - acc: 0.9237\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1764 - acc: 0.9246\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1747 - acc: 0.9257\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1747 - acc: 0.9255\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1724 - acc: 0.9262\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1712 - acc: 0.9262\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1699 - acc: 0.9277\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1693 - acc: 0.9273\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1680 - acc: 0.9283\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1669 - acc: 0.9289\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1669 - acc: 0.9291\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1651 - acc: 0.9298\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1636 - acc: 0.9302\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1627 - acc: 0.9305\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1619 - acc: 0.9310\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1611 - acc: 0.9308\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1599 - acc: 0.9310\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1594 - acc: 0.9311\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1602 - acc: 0.9314\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1588 - acc: 0.9320\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1573 - acc: 0.9325\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1556 - acc: 0.9333\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1551 - acc: 0.9324\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1535 - acc: 0.9322\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1547 - acc: 0.9321\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1524 - acc: 0.9328\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1514 - acc: 0.9338\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1518 - acc: 0.9341\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1502 - acc: 0.9352\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1498 - acc: 0.9348\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1482 - acc: 0.9349\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1473 - acc: 0.9356\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1466 - acc: 0.9356\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1473 - acc: 0.9354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72     40955\n",
      "           1       0.71      0.60      0.65     22297\n",
      "           2       0.76      0.77      0.76     36610\n",
      "\n",
      "    accuracy                           0.72     99862\n",
      "   macro avg       0.72      0.70      0.71     99862\n",
      "weighted avg       0.72      0.72      0.72     99862\n",
      "\n",
      "Acurácia\n",
      "0.7042798908313416\n",
      "Precisao\n",
      "0.7218043947773508\n",
      "Recall\n",
      "0.7214455949209909\n",
      "F1\n",
      "0.7201343010489973\n",
      "[[30605  3857  6493]\n",
      " [ 6660 13324  2313]\n",
      " [ 6892  1602 28116]]\n",
      "TRAIN: [   0    1    2 ... 1992 1995 1998] TEST: [   5    6   15   20   21   42   45   64   65   72   80   83   87   91\n",
      "   98  102  103  117  120  127  146  151  154  156  161  179  181  183\n",
      "  187  201  202  204  205  209  213  220  224  226  232  238  242  243\n",
      "  247  248  250  254  255  260  269  275  278  292  296  322  327  332\n",
      "  335  336  337  350  353  354  358  362  366  369  381  387  388  391\n",
      "  392  393  398  402  407  426  429  434  441  443  447  448  454  458\n",
      "  465  470  473  476  478  479  483  484  488  492  506  513  514  530\n",
      "  534  547  549  562  564  565  570  578  582  587  589  591  595  596\n",
      "  614  615  616  629  633  641  668  670  672  676  679  685  687  690\n",
      "  691  694  698  708  716  727  729  732  740  741  753  765  771  774\n",
      "  778  783  792  795  803  804  806  814  815  832  841  843  844  849\n",
      "  859  865  871  893  898  902  906  911  928  929  933  958  973  974\n",
      "  981  988  989  997  999 1001 1005 1007 1025 1027 1029 1036 1037 1040\n",
      " 1045 1047 1057 1060 1079 1084 1093 1095 1100 1104 1117 1119 1120 1124\n",
      " 1130 1132 1137 1142 1143 1144 1147 1150 1159 1160 1161 1168 1176 1182\n",
      " 1190 1199 1202 1214 1216 1219 1224 1225 1226 1227 1232 1239 1243 1254\n",
      " 1264 1275 1276 1277 1279 1280 1281 1283 1290 1291 1292 1294 1300 1301\n",
      " 1305 1308 1312 1321 1329 1330 1335 1339 1340 1341 1342 1344 1349 1355\n",
      " 1364 1372 1376 1378 1395 1396 1399 1402 1405 1406 1409 1418 1421 1425\n",
      " 1434 1435 1436 1440 1441 1450 1461 1464 1465 1466 1467 1470 1472 1476\n",
      " 1478 1482 1483 1484 1492 1496 1501 1502 1503 1507 1512 1513 1514 1517\n",
      " 1524 1533 1534 1535 1537 1538 1542 1551 1559 1561 1567 1568 1571 1573\n",
      " 1575 1576 1578 1581 1582 1595 1601 1603 1624 1626 1628 1629 1633 1638\n",
      " 1639 1645 1650 1654 1662 1673 1680 1685 1686 1688 1692 1693 1698 1703\n",
      " 1705 1721 1725 1731 1732 1735 1736 1737 1740 1743 1748 1755 1759 1762\n",
      " 1763 1767 1769 1777 1779 1783 1788 1792 1794 1798 1803 1804 1806 1817\n",
      " 1827 1833 1838 1844 1845 1852 1872 1877 1878 1891 1892 1895 1903 1905\n",
      " 1906 1915 1918 1922 1936 1939 1947 1948 1949 1952 1954 1960 1967 1973\n",
      " 1980 1983 1989 1993 1994 1996 1997 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3350 - acc: 0.8149\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3128 - acc: 0.8588\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3050 - acc: 0.8635\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3021 - acc: 0.8652\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3000 - acc: 0.8662\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2964 - acc: 0.8682\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2937 - acc: 0.8698\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2919 - acc: 0.8703\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2896 - acc: 0.8714\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2880 - acc: 0.8722\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2856 - acc: 0.8732\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2840 - acc: 0.8735\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2822 - acc: 0.8751\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2799 - acc: 0.8761\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2785 - acc: 0.8769\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2866 - acc: 0.8733\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2814 - acc: 0.8762\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2769 - acc: 0.8781\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2747 - acc: 0.8794\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2725 - acc: 0.8805\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2678 - acc: 0.8829\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2608 - acc: 0.8866\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2575 - acc: 0.8883\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2525 - acc: 0.8905\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2510 - acc: 0.8915\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2491 - acc: 0.8923\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2469 - acc: 0.8932\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2441 - acc: 0.8947\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2422 - acc: 0.8953\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2400 - acc: 0.8968\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2384 - acc: 0.8975\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2361 - acc: 0.8985\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2358 - acc: 0.8987\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2329 - acc: 0.8999\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2321 - acc: 0.9005\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2279 - acc: 0.9024\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2269 - acc: 0.9028\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2251 - acc: 0.9035\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2229 - acc: 0.9048\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2217 - acc: 0.9052\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2197 - acc: 0.9062\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2187 - acc: 0.9066\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2174 - acc: 0.9070\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2143 - acc: 0.9089\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2123 - acc: 0.9094\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2104 - acc: 0.9102\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2080 - acc: 0.9117\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2060 - acc: 0.9124\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2040 - acc: 0.9135\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2036 - acc: 0.9133\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2010 - acc: 0.9143\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2009 - acc: 0.9143\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2012 - acc: 0.9144\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1967 - acc: 0.9164\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1963 - acc: 0.9168\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1941 - acc: 0.9176\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1915 - acc: 0.9188\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1894 - acc: 0.9198\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1887 - acc: 0.9200\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1882 - acc: 0.9201\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1856 - acc: 0.9213\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1837 - acc: 0.9221\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1829 - acc: 0.9224\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1817 - acc: 0.9231\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1806 - acc: 0.9236\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1785 - acc: 0.9246\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1777 - acc: 0.9249\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1757 - acc: 0.9259\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1753 - acc: 0.9257\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1750 - acc: 0.9260\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1746 - acc: 0.9263\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1714 - acc: 0.9277\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1707 - acc: 0.9280\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1702 - acc: 0.9282\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1684 - acc: 0.9291\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1675 - acc: 0.9296\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1658 - acc: 0.9300\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1649 - acc: 0.9307\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1633 - acc: 0.9316\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1627 - acc: 0.9315\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1618 - acc: 0.9320\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1610 - acc: 0.9323\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1604 - acc: 0.9329\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1661 - acc: 0.9301\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1586 - acc: 0.9332\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1571 - acc: 0.9339\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1559 - acc: 0.9349\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1546 - acc: 0.9352\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1544 - acc: 0.9353\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1528 - acc: 0.9360\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1524 - acc: 0.9360\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1519 - acc: 0.9363\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1515 - acc: 0.9364\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1503 - acc: 0.9369\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1497 - acc: 0.9372\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1482 - acc: 0.9380\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1472 - acc: 0.9382\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1466 - acc: 0.9386\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1458 - acc: 0.9388\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1459 - acc: 0.9388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72     42509\n",
      "           1       0.69      0.62      0.65     22898\n",
      "           2       0.77      0.78      0.78     38910\n",
      "\n",
      "    accuracy                           0.73    104317\n",
      "   macro avg       0.72      0.71      0.72    104317\n",
      "weighted avg       0.73      0.73      0.72    104317\n",
      "\n",
      "Acurácia\n",
      "0.71071554907335\n",
      "Precisao\n",
      "0.7250548551394168\n",
      "Recall\n",
      "0.725663122980914\n",
      "F1\n",
      "0.7247528601118983\n",
      "[[31136  4507  6866]\n",
      " [ 6523 14156  2219]\n",
      " [ 6710  1793 30407]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   3    4    7    9   16   24   31   33   38   43   48   50   52   55\n",
      "   59   60   92   93   94   99  104  105  111  113  118  128  133  135\n",
      "  144  148  150  152  153  160  170  171  176  177  180  185  189  191\n",
      "  196  197  198  199  207  212  215  216  217  221  223  231  235  237\n",
      "  240  241  245  246  253  258  262  265  271  273  300  306  307  315\n",
      "  317  320  325  326  329  330  331  338  360  365  372  374  376  377\n",
      "  378  382  383  399  401  411  412  414  417  420  425  442  444  445\n",
      "  452  455  467  469  480  481  486  491  493  501  505  520  528  531\n",
      "  537  545  552  554  557  559  566  573  574  579  583  586  590  597\n",
      "  598  604  609  617  618  619  623  625  626  630  632  642  647  652\n",
      "  657  661  664  674  678  709  711  712  728  737  739  742  743  754\n",
      "  755  757  759  761  767  770  772  787  788  791  794  798  799  802\n",
      "  807  809  810  811  817  820  827  831  837  838  851  852  853  866\n",
      "  867  868  876  880  883  889  891  907  910  916  920  932  935  938\n",
      "  950  962  964  965  968  970  979  990  992  993  994 1004 1006 1020\n",
      " 1030 1044 1046 1051 1054 1061 1062 1063 1066 1069 1080 1082 1096 1099\n",
      " 1110 1111 1112 1121 1128 1133 1138 1153 1156 1162 1164 1175 1178 1181\n",
      " 1185 1187 1188 1192 1193 1194 1196 1200 1207 1209 1213 1217 1218 1220\n",
      " 1229 1230 1244 1245 1247 1248 1249 1251 1259 1265 1268 1272 1287 1289\n",
      " 1302 1304 1323 1333 1343 1347 1348 1356 1365 1366 1367 1369 1373 1381\n",
      " 1382 1389 1390 1393 1401 1403 1404 1410 1414 1415 1417 1419 1420 1439\n",
      " 1448 1452 1456 1457 1463 1485 1504 1505 1511 1518 1526 1549 1552 1556\n",
      " 1572 1577 1584 1587 1589 1591 1592 1597 1605 1607 1608 1612 1618 1620\n",
      " 1623 1625 1630 1636 1637 1644 1655 1660 1663 1666 1669 1670 1682 1683\n",
      " 1689 1690 1697 1700 1702 1707 1709 1711 1714 1719 1720 1726 1727 1738\n",
      " 1741 1756 1765 1766 1771 1773 1782 1785 1787 1796 1800 1802 1819 1820\n",
      " 1829 1840 1846 1850 1866 1867 1870 1876 1880 1881 1882 1885 1886 1888\n",
      " 1893 1897 1899 1908 1913 1921 1924 1925 1928 1933 1945 1946 1955 1957\n",
      " 1958 1966 1968 1972 1976 1978 1981 1985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3351 - acc: 0.8320\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3110 - acc: 0.8591\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3034 - acc: 0.8638\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3002 - acc: 0.8656\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2978 - acc: 0.8669\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2971 - acc: 0.8670\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2935 - acc: 0.8684\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2913 - acc: 0.8696\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2881 - acc: 0.8711\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2852 - acc: 0.8733\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2856 - acc: 0.8726\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2848 - acc: 0.8729\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2815 - acc: 0.8742\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2784 - acc: 0.8753\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2766 - acc: 0.8769\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2753 - acc: 0.8773\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2747 - acc: 0.8780\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2738 - acc: 0.8789\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2707 - acc: 0.8803\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2734 - acc: 0.8790\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2674 - acc: 0.8830\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2721 - acc: 0.8798\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2633 - acc: 0.8838\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2589 - acc: 0.8867\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2546 - acc: 0.8892\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2505 - acc: 0.8912\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2483 - acc: 0.8922\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2449 - acc: 0.8940\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2423 - acc: 0.8953\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2393 - acc: 0.8963\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2381 - acc: 0.8974\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2361 - acc: 0.8982\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2332 - acc: 0.8993\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2316 - acc: 0.9004\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2300 - acc: 0.9012\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2268 - acc: 0.9024\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2251 - acc: 0.9032\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2220 - acc: 0.9046\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2209 - acc: 0.9053\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2194 - acc: 0.9057\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2164 - acc: 0.9073\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2162 - acc: 0.9073\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2155 - acc: 0.9075\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2117 - acc: 0.9094\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2094 - acc: 0.9103\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2071 - acc: 0.9113\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2057 - acc: 0.9119\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2041 - acc: 0.9126\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2022 - acc: 0.9135\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1998 - acc: 0.9147\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1978 - acc: 0.9155\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1970 - acc: 0.9158\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1950 - acc: 0.9167\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1939 - acc: 0.9173\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1929 - acc: 0.9180\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1909 - acc: 0.9184\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1884 - acc: 0.9199\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1870 - acc: 0.9206\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1869 - acc: 0.9204\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1846 - acc: 0.9215\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1827 - acc: 0.9223\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1808 - acc: 0.9231\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1793 - acc: 0.9241\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1798 - acc: 0.9235\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1822 - acc: 0.9227\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1771 - acc: 0.9249\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1745 - acc: 0.9261\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1734 - acc: 0.9267\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1721 - acc: 0.9270\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1713 - acc: 0.9275\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1697 - acc: 0.9282\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1681 - acc: 0.9287\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1678 - acc: 0.9290\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1664 - acc: 0.9297\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1650 - acc: 0.9304\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1644 - acc: 0.9305\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1628 - acc: 0.9311\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1624 - acc: 0.9315\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1615 - acc: 0.9318\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1598 - acc: 0.9326\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1592 - acc: 0.9328\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1579 - acc: 0.9332\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1572 - acc: 0.9338\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1565 - acc: 0.9338\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1552 - acc: 0.9349\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1546 - acc: 0.9347\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1536 - acc: 0.9355\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1528 - acc: 0.9355\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1530 - acc: 0.9358\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1522 - acc: 0.9361\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1502 - acc: 0.9370\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1497 - acc: 0.9371\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1489 - acc: 0.9374\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1486 - acc: 0.9377\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1471 - acc: 0.9382\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1466 - acc: 0.9383\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1460 - acc: 0.9387\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1450 - acc: 0.9392\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1443 - acc: 0.9396\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1442 - acc: 0.9396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.71     43658\n",
      "           1       0.68      0.63      0.65     23735\n",
      "           2       0.77      0.76      0.76     38813\n",
      "\n",
      "    accuracy                           0.72    106206\n",
      "   macro avg       0.71      0.71      0.71    106206\n",
      "weighted avg       0.72      0.72      0.72    106206\n",
      "\n",
      "Acurácia\n",
      "0.7052419821911299\n",
      "Precisao\n",
      "0.7175591509089533\n",
      "Recall\n",
      "0.717661902340734\n",
      "F1\n",
      "0.7172028772116273\n",
      "[[31857  5140  6661]\n",
      " [ 6549 14849  2337]\n",
      " [ 7390  1909 29514]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_12 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 436,203\n",
      "Trainable params: 436,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.7108818622988616, 0.6944941986547039, 0.7042798908313416, 0.71071554907335, 0.7052419821911299]\n",
      "0.7051226966098774\n",
      "Precision total\n",
      "[0.7278275997809438, 0.7110932565471653, 0.7218043947773508, 0.7250548551394168, 0.7175591509089533]\n",
      "0.720667851430766\n",
      "Recalls total\n",
      "[0.7283055636753386, 0.7104569377292228, 0.7214455949209909, 0.725663122980914, 0.717661902340734]\n",
      "0.7207066243294401\n",
      "F1 total\n",
      "[0.7266779039576208, 0.7095720255019374, 0.7201343010489973, 0.7247528601118983, 0.7172028772116273]\n",
      "0.7196679935664162\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
