{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNGRU, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0822 14:50:07.267557   504 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 14:50:07.272514   504 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 14:50:07.273512   504 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 14:50:07.275507   504 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   1    5    6 ... 1995 1998 1999] TEST: [   0    2    3    4    8   13   18   21   32   34   35   36   37   41\n",
      "   44   48   52   53   60   66   69   73   74   78   81   98  107  116\n",
      "  122  130  131  142  144  145  160  164  173  174  178  189  195  211\n",
      "  213  221  226  227  229  234  236  239  240  247  248  249  250  251\n",
      "  252  253  255  268  269  274  276  278  279  281  282  284  285  293\n",
      "  300  304  312  322  329  332  349  351  353  356  363  366  372  376\n",
      "  386  387  392  393  396  401  406  407  408  410  411  413  424  427\n",
      "  440  445  451  452  453  462  473  474  475  479  481  489  493  500\n",
      "  502  509  510  514  516  518  522  532  535  543  548  553  554  557\n",
      "  563  566  569  570  572  577  584  588  595  598  604  605  612  616\n",
      "  618  639  644  647  648  650  652  656  685  688  694  706  707  709\n",
      "  712  713  721  725  727  728  736  740  742  743  744  746  747  748\n",
      "  750  756  760  764  766  773  782  789  790  794  797  804  818  822\n",
      "  830  836  840  841  845  857  859  861  869  891  892  893  895  898\n",
      "  908  916  926  933  947  954  958  965  967  980  983  985  997 1004\n",
      " 1007 1012 1013 1015 1019 1024 1027 1035 1037 1040 1047 1065 1066 1081\n",
      " 1085 1087 1089 1093 1094 1106 1120 1123 1134 1137 1146 1153 1155 1159\n",
      " 1163 1164 1166 1171 1174 1177 1178 1180 1182 1186 1190 1196 1200 1207\n",
      " 1209 1210 1212 1220 1224 1229 1231 1232 1238 1239 1251 1253 1255 1258\n",
      " 1264 1268 1269 1273 1275 1286 1289 1300 1303 1309 1311 1314 1315 1320\n",
      " 1327 1328 1329 1340 1342 1361 1363 1375 1383 1385 1394 1403 1413 1415\n",
      " 1419 1424 1430 1440 1443 1452 1457 1458 1460 1466 1469 1477 1478 1479\n",
      " 1484 1494 1511 1512 1513 1536 1537 1538 1539 1549 1551 1569 1576 1586\n",
      " 1593 1595 1601 1611 1626 1627 1632 1633 1636 1638 1650 1651 1660 1670\n",
      " 1675 1680 1683 1684 1687 1699 1700 1704 1705 1719 1741 1751 1752 1760\n",
      " 1763 1765 1773 1777 1778 1785 1787 1789 1802 1804 1805 1808 1811 1814\n",
      " 1825 1826 1832 1842 1846 1856 1857 1860 1863 1864 1875 1884 1885 1890\n",
      " 1897 1901 1909 1914 1921 1922 1937 1938 1948 1949 1954 1961 1962 1965\n",
      " 1975 1977 1984 1991 1993 1994 1996 1997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 14:50:08.787252   504 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3322 - acc: 0.8376\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3077 - acc: 0.8614\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3019 - acc: 0.8642\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2996 - acc: 0.8650\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2967 - acc: 0.8662\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2946 - acc: 0.8674\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2935 - acc: 0.8679\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2899 - acc: 0.8700\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2888 - acc: 0.8697\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2883 - acc: 0.8701\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2868 - acc: 0.8705\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2838 - acc: 0.8718\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2797 - acc: 0.8743\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2785 - acc: 0.8747\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2801 - acc: 0.8742\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2754 - acc: 0.8766\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2755 - acc: 0.8772\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2718 - acc: 0.8786\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2724 - acc: 0.8782\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2683 - acc: 0.8805\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2643 - acc: 0.8822\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2649 - acc: 0.8823\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2584 - acc: 0.8858\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2556 - acc: 0.8875\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.2481 - acc: 0.8912\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2453 - acc: 0.8929\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.2411 - acc: 0.8952\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2380 - acc: 0.8962\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2351 - acc: 0.8981\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2338 - acc: 0.8986\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2299 - acc: 0.9007\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2261 - acc: 0.9022\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2238 - acc: 0.9034\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2224 - acc: 0.9043\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2178 - acc: 0.9063\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2155 - acc: 0.9075\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2132 - acc: 0.9087\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2099 - acc: 0.9101\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2093 - acc: 0.9104\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2060 - acc: 0.9119\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2025 - acc: 0.9135\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2022 - acc: 0.9135\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1985 - acc: 0.9153\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1947 - acc: 0.9171\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1954 - acc: 0.9171\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1913 - acc: 0.9188\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1896 - acc: 0.9193\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1869 - acc: 0.9208\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1849 - acc: 0.9216\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1852 - acc: 0.9214\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1816 - acc: 0.9231\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1798 - acc: 0.9236\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1791 - acc: 0.9242\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1765 - acc: 0.9254\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1764 - acc: 0.9254\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1737 - acc: 0.9263\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1723 - acc: 0.9274\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1702 - acc: 0.9283\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1692 - acc: 0.9283\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1678 - acc: 0.9294\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1667 - acc: 0.9296\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1649 - acc: 0.9304\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1634 - acc: 0.9310\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1638 - acc: 0.9309\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1613 - acc: 0.9321\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1593 - acc: 0.9329\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1587 - acc: 0.9332\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1580 - acc: 0.9334\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1578 - acc: 0.9337\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1547 - acc: 0.9349\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1535 - acc: 0.9355\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1537 - acc: 0.9356\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1519 - acc: 0.9364\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1507 - acc: 0.9366\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1497 - acc: 0.9369\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1490 - acc: 0.9376\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1481 - acc: 0.9380\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1473 - acc: 0.9385\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1463 - acc: 0.9387\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1457 - acc: 0.9390\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1449 - acc: 0.9394\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1434 - acc: 0.9400\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1421 - acc: 0.9405\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1411 - acc: 0.9409\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1415 - acc: 0.9408\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1417 - acc: 0.9409\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1400 - acc: 0.9413\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1381 - acc: 0.9423\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1374 - acc: 0.9429\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1368 - acc: 0.9429\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1363 - acc: 0.9431\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1372 - acc: 0.9427\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1356 - acc: 0.9433\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1340 - acc: 0.9440\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1333 - acc: 0.9444\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1326 - acc: 0.9448\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1318 - acc: 0.9450\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1313 - acc: 0.9451\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1307 - acc: 0.9456\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1309 - acc: 0.9455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71     43900\n",
      "           1       0.69      0.61      0.65     23946\n",
      "           2       0.75      0.79      0.77     40210\n",
      "\n",
      "    accuracy                           0.72    108056\n",
      "   macro avg       0.72      0.71      0.71    108056\n",
      "weighted avg       0.72      0.72      0.72    108056\n",
      "\n",
      "Acur√°cia\n",
      "0.7053727355443346\n",
      "Precisao\n",
      "0.7197273302009588\n",
      "Recall\n",
      "0.7207744132671948\n",
      "F1\n",
      "0.7194002842823131\n",
      "[[31713  4642  7545]\n",
      " [ 6615 14533  2798]\n",
      " [ 6834  1738 31638]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [  12   15   17   27   30   38   39   46   55   57   62   64   67   76\n",
      "   83   84   87   92   93   96   97   99  102  109  110  121  129  132\n",
      "  133  138  140  141  154  156  157  158  165  167  171  172  188  193\n",
      "  199  203  210  212  217  220  225  237  238  243  246  258  259  261\n",
      "  265  283  286  295  297  299  301  308  313  314  315  317  319  320\n",
      "  324  328  334  336  337  342  343  344  348  354  364  382  385  388\n",
      "  391  403  404  416  425  432  435  437  441  448  449  455  467  472\n",
      "  484  485  487  488  490  494  496  497  498  501  503  505  508  524\n",
      "  526  556  558  565  573  578  589  591  600  603  610  611  617  622\n",
      "  624  625  626  627  629  631  632  635  641  642  643  646  660  662\n",
      "  682  683  684  689  690  692  693  705  711  724  726  732  733  745\n",
      "  759  769  776  791  795  802  805  813  814  823  827  829  833  838\n",
      "  842  847  848  854  867  870  872  876  877  880  889  896  906  918\n",
      "  924  927  928  951  955  963  970  977  979  994  995  996  998 1017\n",
      " 1018 1020 1026 1033 1043 1046 1056 1060 1061 1062 1064 1069 1072 1077\n",
      " 1078 1083 1086 1098 1105 1112 1126 1129 1133 1138 1139 1141 1144 1145\n",
      " 1148 1158 1168 1184 1185 1188 1192 1199 1202 1208 1214 1216 1218 1219\n",
      " 1226 1228 1237 1249 1254 1257 1259 1261 1263 1267 1272 1276 1282 1292\n",
      " 1297 1305 1310 1312 1317 1318 1322 1331 1332 1346 1351 1352 1353 1354\n",
      " 1355 1366 1376 1377 1378 1386 1387 1389 1395 1400 1401 1402 1405 1411\n",
      " 1421 1429 1435 1437 1439 1442 1446 1450 1453 1454 1461 1470 1475 1476\n",
      " 1482 1490 1495 1497 1500 1515 1516 1521 1526 1528 1532 1541 1542 1546\n",
      " 1548 1550 1553 1556 1559 1563 1564 1566 1567 1573 1574 1575 1583 1587\n",
      " 1598 1603 1605 1616 1619 1625 1634 1641 1642 1643 1649 1659 1662 1663\n",
      " 1665 1668 1682 1692 1696 1697 1702 1708 1710 1712 1717 1723 1725 1733\n",
      " 1734 1735 1737 1738 1740 1744 1754 1758 1759 1766 1774 1780 1784 1792\n",
      " 1793 1794 1806 1810 1812 1819 1830 1831 1834 1835 1837 1840 1845 1847\n",
      " 1853 1858 1867 1872 1889 1892 1893 1895 1902 1908 1915 1926 1927 1934\n",
      " 1940 1947 1952 1963 1973 1992 1995 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.3400 - acc: 0.8294\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3152 - acc: 0.8581\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3113 - acc: 0.8604\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3073 - acc: 0.8622\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.3037 - acc: 0.8638\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.3019 - acc: 0.8648\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3003 - acc: 0.8653\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2984 - acc: 0.8678\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2957 - acc: 0.8688\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.2932 - acc: 0.8703\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2938 - acc: 0.8693\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2889 - acc: 0.8717\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2886 - acc: 0.8721\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2871 - acc: 0.8722\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.2845 - acc: 0.8737\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2820 - acc: 0.8749\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2793 - acc: 0.8762\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2780 - acc: 0.8768\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2800 - acc: 0.8765\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2747 - acc: 0.8782\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2722 - acc: 0.8797\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.2660 - acc: 0.8837\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2612 - acc: 0.8856\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2559 - acc: 0.8884\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2543 - acc: 0.8893\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2507 - acc: 0.8910\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2457 - acc: 0.8935\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2443 - acc: 0.8940\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2417 - acc: 0.8957\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2394 - acc: 0.8963\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2381 - acc: 0.8969\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2332 - acc: 0.8992\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2314 - acc: 0.9005\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2287 - acc: 0.9015\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2257 - acc: 0.9034\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2231 - acc: 0.9041\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.2225 - acc: 0.9046\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2182 - acc: 0.9066\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2162 - acc: 0.9074\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2138 - acc: 0.9085\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2103 - acc: 0.9100\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 17s 10ms/sample - loss: 0.2084 - acc: 0.9107\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2062 - acc: 0.9121\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2035 - acc: 0.9131\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2010 - acc: 0.9147\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1996 - acc: 0.9151\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1971 - acc: 0.9162\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1956 - acc: 0.9170\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1922 - acc: 0.9186\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1893 - acc: 0.9196\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1970 - acc: 0.9162\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2009 - acc: 0.9147\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1917 - acc: 0.9185\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1863 - acc: 0.9210\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1847 - acc: 0.9215\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1818 - acc: 0.9231\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1799 - acc: 0.9240\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1777 - acc: 0.9249\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1758 - acc: 0.9258\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1737 - acc: 0.9266\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1730 - acc: 0.9268\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1716 - acc: 0.9278\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1701 - acc: 0.9282\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1687 - acc: 0.9290\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1666 - acc: 0.9297\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1658 - acc: 0.9303\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1644 - acc: 0.9307\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1636 - acc: 0.9313\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1630 - acc: 0.9316\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1612 - acc: 0.9323\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1604 - acc: 0.9325\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1598 - acc: 0.9330\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1578 - acc: 0.9337\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1559 - acc: 0.9347\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1558 - acc: 0.9348\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1549 - acc: 0.9350\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1541 - acc: 0.9353\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1527 - acc: 0.9359\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1521 - acc: 0.9365\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1517 - acc: 0.9365\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1501 - acc: 0.9372\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1487 - acc: 0.9380\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1487 - acc: 0.9378\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1469 - acc: 0.9386\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1463 - acc: 0.9388\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1449 - acc: 0.9394\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1442 - acc: 0.9401\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1435 - acc: 0.9400\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1435 - acc: 0.9400\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1418 - acc: 0.9410\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1410 - acc: 0.9412\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1407 - acc: 0.9411\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1400 - acc: 0.9416\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1394 - acc: 0.9423\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1389 - acc: 0.9422\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1375 - acc: 0.9429\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1371 - acc: 0.9431\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1361 - acc: 0.9434\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1358 - acc: 0.9434\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.1349 - acc: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72     40365\n",
      "           1       0.69      0.63      0.66     22305\n",
      "           2       0.79      0.74      0.77     36224\n",
      "\n",
      "    accuracy                           0.72     98894\n",
      "   macro avg       0.72      0.71      0.71     98894\n",
      "weighted avg       0.72      0.72      0.72     98894\n",
      "\n",
      "Acur√°cia\n",
      "0.7094467587990453\n",
      "Precisao\n",
      "0.7246562889303874\n",
      "Recall\n",
      "0.7222177280724816\n",
      "F1\n",
      "0.7222142341024681\n",
      "[[30478  4659  5228]\n",
      " [ 6342 14103  1860]\n",
      " [ 7598  1784 26842]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1998] TEST: [   7   11   14   20   22   24   40   43   50   54   56   58   61   65\n",
      "   72   79   88   89   90   95  103  104  106  108  111  113  119  124\n",
      "  127  134  136  137  139  143  151  153  155  161  168  177  179  181\n",
      "  182  183  185  194  197  201  202  205  206  208  214  215  219  222\n",
      "  230  231  233  241  245  254  260  264  271  272  277  287  288  289\n",
      "  290  291  292  294  296  302  305  306  321  323  325  326  333  339\n",
      "  358  360  365  370  374  375  377  380  383  389  399  405  412  419\n",
      "  423  434  444  454  461  483  491  504  507  513  517  521  523  528\n",
      "  530  538  541  544  551  561  564  579  581  587  596  601  608  621\n",
      "  630  634  636  637  638  654  663  666  668  670  677  679  695  696\n",
      "  701  702  708  710  715  716  720  739  741  752  757  761  767  772\n",
      "  774  778  779  781  786  792  796  798  808  809  810  816  820  825\n",
      "  828  851  852  856  863  868  883  886  904  905  910  911  912  915\n",
      "  917  925  930  932  937  939  943  944  946  950  961  971  972  975\n",
      "  981  982  984  991  992  993 1005 1016 1031 1034 1036 1039 1049 1050\n",
      " 1052 1053 1054 1063 1073 1075 1076 1080 1091 1095 1111 1113 1115 1116\n",
      " 1121 1122 1130 1132 1135 1140 1142 1143 1147 1149 1165 1169 1175 1176\n",
      " 1179 1183 1187 1189 1197 1205 1211 1213 1217 1225 1227 1230 1236 1241\n",
      " 1242 1247 1250 1252 1265 1270 1274 1280 1284 1301 1313 1321 1324 1345\n",
      " 1347 1359 1360 1364 1367 1368 1372 1373 1379 1380 1382 1384 1392 1397\n",
      " 1406 1410 1418 1423 1427 1436 1447 1455 1463 1467 1471 1473 1474 1480\n",
      " 1485 1491 1493 1502 1517 1523 1525 1529 1535 1540 1545 1547 1552 1557\n",
      " 1560 1561 1562 1565 1568 1571 1572 1581 1582 1584 1588 1589 1591 1613\n",
      " 1614 1617 1618 1622 1623 1635 1640 1644 1647 1653 1664 1681 1689 1690\n",
      " 1694 1695 1714 1716 1721 1722 1726 1739 1742 1746 1755 1756 1762 1764\n",
      " 1768 1776 1781 1791 1800 1803 1807 1813 1815 1818 1820 1821 1822 1823\n",
      " 1828 1829 1833 1841 1848 1861 1868 1869 1874 1876 1880 1888 1894 1896\n",
      " 1898 1900 1906 1907 1917 1923 1925 1930 1931 1936 1939 1945 1956 1960\n",
      " 1969 1972 1978 1980 1981 1983 1988 1999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.3348 - acc: 0.7371\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3116 - acc: 0.8588\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.3045 - acc: 0.8629\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3043 - acc: 0.8623\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2999 - acc: 0.8642\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2981 - acc: 0.8651\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2946 - acc: 0.8670\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2947 - acc: 0.8667\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2911 - acc: 0.8685\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2891 - acc: 0.8701\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2889 - acc: 0.8696\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2869 - acc: 0.8711\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2835 - acc: 0.8728\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2810 - acc: 0.8738\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2799 - acc: 0.8746\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2805 - acc: 0.8746\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2773 - acc: 0.8768\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2733 - acc: 0.8783\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2720 - acc: 0.8795\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2682 - acc: 0.8815\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2662 - acc: 0.8827\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2619 - acc: 0.8849\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2594 - acc: 0.8863\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2537 - acc: 0.8889\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2482 - acc: 0.8916\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2449 - acc: 0.8932\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2418 - acc: 0.8950\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2390 - acc: 0.8964\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2361 - acc: 0.8977\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2330 - acc: 0.8990\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2326 - acc: 0.8994\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2274 - acc: 0.9019\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2250 - acc: 0.9028\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2228 - acc: 0.9039\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2191 - acc: 0.9058\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2157 - acc: 0.9073\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2145 - acc: 0.9077\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2108 - acc: 0.9094\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2105 - acc: 0.9096\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2057 - acc: 0.9118\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2038 - acc: 0.9123\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2034 - acc: 0.9128\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1988 - acc: 0.9149\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1967 - acc: 0.9157\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1947 - acc: 0.9167\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1921 - acc: 0.9179\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1935 - acc: 0.9172\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1886 - acc: 0.9192\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1872 - acc: 0.9201\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1840 - acc: 0.9217\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1827 - acc: 0.9220\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1811 - acc: 0.9230\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1799 - acc: 0.9234\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1781 - acc: 0.9243\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1757 - acc: 0.9253\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1740 - acc: 0.9260\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1741 - acc: 0.9260\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1723 - acc: 0.9268\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1709 - acc: 0.9277\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1697 - acc: 0.9280\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1674 - acc: 0.9293\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1663 - acc: 0.9298\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1650 - acc: 0.9302\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1629 - acc: 0.9311\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1622 - acc: 0.9315\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1604 - acc: 0.9322\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1595 - acc: 0.9327\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1591 - acc: 0.9333\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1571 - acc: 0.9340\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.1565 - acc: 0.9339\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1552 - acc: 0.9344\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1539 - acc: 0.9352\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1528 - acc: 0.9358\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1522 - acc: 0.9360\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1501 - acc: 0.9373\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1504 - acc: 0.9367\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1499 - acc: 0.9372\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1487 - acc: 0.9376\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1481 - acc: 0.9382\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1474 - acc: 0.9382\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1462 - acc: 0.9388\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1445 - acc: 0.9394\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1437 - acc: 0.9398\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1436 - acc: 0.9398\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1418 - acc: 0.9407\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1406 - acc: 0.9413\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1408 - acc: 0.9410\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1399 - acc: 0.9415\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1391 - acc: 0.9418\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.1392 - acc: 0.9419\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1377 - acc: 0.9424\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1371 - acc: 0.9428\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1358 - acc: 0.9432\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1352 - acc: 0.9434\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1344 - acc: 0.9440\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1334 - acc: 0.9445\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1331 - acc: 0.9445\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.1328 - acc: 0.9447\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1321 - acc: 0.9452\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.1317 - acc: 0.9453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72     43737\n",
      "           1       0.68      0.64      0.66     24615\n",
      "           2       0.77      0.74      0.75     36317\n",
      "\n",
      "    accuracy                           0.72    104669\n",
      "   macro avg       0.72      0.71      0.71    104669\n",
      "weighted avg       0.72      0.72      0.72    104669\n",
      "\n",
      "Acur√°cia\n",
      "0.7076189073138529\n",
      "Precisao\n",
      "0.71860687209912\n",
      "Recall\n",
      "0.7178916393583582\n",
      "F1\n",
      "0.717719436835558\n",
      "[[32568  5465  5704]\n",
      " [ 6601 15734  2280]\n",
      " [ 7392  2086 26839]]\n",
      "TRAIN: [   0    2    3 ... 1997 1998 1999] TEST: [   1    5    6   23   25   28   31   42   49   59   63   75   80   82\n",
      "   85   86   91  101  105  115  118  147  148  149  150  163  166  170\n",
      "  184  190  191  207  216  223  224  228  232  256  257  263  267  270\n",
      "  275  280  307  309  310  318  330  331  335  338  340  341  345  352\n",
      "  361  362  371  373  378  390  394  395  398  409  414  417  421  428\n",
      "  429  430  431  433  442  443  450  456  463  470  478  480  486  511\n",
      "  512  519  527  533  534  536  546  549  552  559  567  568  571  576\n",
      "  580  583  590  593  594  602  607  615  619  620  628  633  649  651\n",
      "  655  657  661  664  667  671  674  675  676  680  687  697  699  703\n",
      "  704  714  717  718  719  729  735  737  749  753  754  762  768  770\n",
      "  775  777  785  787  788  799  800  801  819  824  832  834  837  839\n",
      "  843  844  846  853  858  865  874  881  885  897  900  902  903  907\n",
      "  913  921  922  923  929  931  934  940  942  952  956  957  959  960\n",
      "  966  976  978  987 1001 1006 1011 1028 1038 1041 1042 1044 1048 1051\n",
      " 1055 1058 1059 1067 1074 1082 1084 1088 1090 1096 1099 1101 1102 1103\n",
      " 1104 1107 1109 1117 1119 1125 1127 1131 1150 1151 1152 1161 1173 1191\n",
      " 1193 1194 1203 1204 1215 1222 1233 1234 1235 1240 1244 1245 1246 1256\n",
      " 1260 1262 1271 1285 1287 1288 1290 1294 1295 1296 1298 1299 1304 1306\n",
      " 1316 1319 1323 1326 1330 1336 1339 1343 1350 1356 1357 1370 1371 1374\n",
      " 1381 1388 1391 1398 1399 1404 1408 1409 1414 1417 1420 1422 1428 1431\n",
      " 1434 1438 1445 1448 1462 1464 1465 1468 1486 1487 1488 1489 1492 1496\n",
      " 1501 1504 1506 1507 1510 1514 1518 1519 1520 1527 1534 1544 1554 1555\n",
      " 1578 1579 1590 1592 1597 1600 1602 1606 1608 1609 1612 1620 1621 1624\n",
      " 1628 1629 1630 1637 1639 1645 1648 1652 1656 1657 1666 1667 1671 1674\n",
      " 1676 1678 1679 1691 1693 1701 1706 1707 1711 1713 1715 1718 1724 1728\n",
      " 1730 1736 1745 1749 1767 1769 1772 1775 1779 1782 1783 1788 1795 1797\n",
      " 1801 1816 1817 1836 1843 1844 1849 1850 1852 1855 1865 1871 1877 1879\n",
      " 1881 1886 1904 1912 1913 1918 1928 1941 1942 1944 1946 1958 1959 1964\n",
      " 1967 1968 1970 1976 1985 1986 1987 1990]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 17s 11ms/sample - loss: 0.3383 - acc: 0.8305\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.3147 - acc: 0.8581\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.3099 - acc: 0.8608\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3083 - acc: 0.8612\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.3036 - acc: 0.8630\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.3015 - acc: 0.8641\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.3012 - acc: 0.8647\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2975 - acc: 0.8669\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2951 - acc: 0.8672\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2936 - acc: 0.8679\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2895 - acc: 0.8694\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2888 - acc: 0.8702\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2859 - acc: 0.8717\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2851 - acc: 0.8716\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2819 - acc: 0.8733\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2805 - acc: 0.8745\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2775 - acc: 0.8764\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2753 - acc: 0.8776\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2761 - acc: 0.8774\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2660 - acc: 0.8827\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.2593 - acc: 0.8866\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2560 - acc: 0.8882\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2547 - acc: 0.8887\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2496 - acc: 0.8912\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2479 - acc: 0.8919\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2459 - acc: 0.8930\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2427 - acc: 0.8943\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2394 - acc: 0.8961\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2383 - acc: 0.8966\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2355 - acc: 0.8975\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2356 - acc: 0.8979\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2456 - acc: 0.8934\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.2339 - acc: 0.8989\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2268 - acc: 0.9020\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 15s 10ms/sample - loss: 0.2240 - acc: 0.9036\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 16s 10ms/sample - loss: 0.2221 - acc: 0.9046\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 14s 9ms/sample - loss: 0.2195 - acc: 0.9053\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2180 - acc: 0.9058\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2151 - acc: 0.9077\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2122 - acc: 0.9089\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2108 - acc: 0.9090\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2085 - acc: 0.9104\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2052 - acc: 0.9121\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2039 - acc: 0.9125\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 14s 8ms/sample - loss: 0.2027 - acc: 0.9132\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2009 - acc: 0.9143\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1979 - acc: 0.9153\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1952 - acc: 0.9165\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1942 - acc: 0.9167\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1917 - acc: 0.9180\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.1890 - acc: 0.9193\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1886 - acc: 0.9196\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1860 - acc: 0.9204\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1840 - acc: 0.9214\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1826 - acc: 0.9223\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1808 - acc: 0.9228\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1788 - acc: 0.9237\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1787 - acc: 0.9239\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1763 - acc: 0.9251\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1757 - acc: 0.9249\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1728 - acc: 0.9265\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1719 - acc: 0.9265\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1712 - acc: 0.9268\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1689 - acc: 0.9275\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1678 - acc: 0.9284\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1669 - acc: 0.9289\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1652 - acc: 0.9297\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1635 - acc: 0.9306\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1633 - acc: 0.9308\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1613 - acc: 0.9315\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1609 - acc: 0.9314\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1590 - acc: 0.9326\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1585 - acc: 0.9325\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1587 - acc: 0.9317\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1569 - acc: 0.9325\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1558 - acc: 0.9334\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1539 - acc: 0.9341\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1522 - acc: 0.9351\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1519 - acc: 0.9352\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1509 - acc: 0.9356\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.1506 - acc: 0.9355\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1490 - acc: 0.9364\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.1490 - acc: 0.9365\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1475 - acc: 0.9367\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1471 - acc: 0.9371\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1463 - acc: 0.9377\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1454 - acc: 0.9377\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.1442 - acc: 0.9381\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1434 - acc: 0.9385\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1424 - acc: 0.9392\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1429 - acc: 0.9394\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1414 - acc: 0.9399\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1402 - acc: 0.9405\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1398 - acc: 0.9405\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1390 - acc: 0.9411\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1379 - acc: 0.9416\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1379 - acc: 0.9415\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1368 - acc: 0.9422\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.1358 - acc: 0.9425\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1363 - acc: 0.9424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72     40902\n",
      "           1       0.68      0.66      0.67     22127\n",
      "           2       0.79      0.75      0.77     37077\n",
      "\n",
      "    accuracy                           0.73    100106\n",
      "   macro avg       0.72      0.72      0.72    100106\n",
      "weighted avg       0.73      0.73      0.73    100106\n",
      "\n",
      "Acur√°cia\n",
      "0.7174546670756644\n",
      "Precisao\n",
      "0.7293116448627787\n",
      "Recall\n",
      "0.7277585759095359\n",
      "F1\n",
      "0.7280167010086482\n",
      "[[30433  4890  5579]\n",
      " [ 5887 14499  1741]\n",
      " [ 7208  1948 27921]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   9   10   16   19   26   29   33   45   47   51   68   70   71   77\n",
      "   94  100  112  114  117  120  123  125  126  128  135  146  152  159\n",
      "  162  169  175  176  180  186  187  192  196  198  200  204  209  218\n",
      "  235  242  244  262  266  273  298  303  311  316  327  346  347  350\n",
      "  355  357  359  367  368  369  379  381  384  397  400  402  415  418\n",
      "  420  422  426  436  438  439  446  447  457  458  459  460  464  465\n",
      "  466  468  469  471  476  477  482  492  495  499  506  515  520  525\n",
      "  529  531  537  539  540  542  545  547  550  555  560  562  574  575\n",
      "  582  585  586  592  597  599  606  609  613  614  623  640  645  653\n",
      "  658  659  665  669  672  673  678  681  686  691  698  700  722  723\n",
      "  730  731  734  738  751  755  758  763  765  771  780  783  784  793\n",
      "  803  806  807  811  812  815  817  821  826  831  835  849  850  855\n",
      "  860  862  864  866  871  873  875  878  879  882  884  887  888  890\n",
      "  894  899  901  909  914  919  920  935  936  938  941  945  948  949\n",
      "  953  962  964  968  969  973  974  986  988  989  990  999 1000 1002\n",
      " 1003 1008 1009 1010 1014 1021 1022 1023 1025 1029 1030 1032 1045 1057\n",
      " 1068 1070 1071 1079 1092 1097 1100 1108 1110 1114 1118 1124 1128 1136\n",
      " 1154 1156 1157 1160 1162 1167 1170 1172 1181 1195 1198 1201 1206 1221\n",
      " 1223 1243 1248 1266 1277 1278 1279 1281 1283 1291 1293 1302 1307 1308\n",
      " 1325 1333 1334 1335 1337 1338 1341 1344 1348 1349 1358 1362 1365 1369\n",
      " 1390 1393 1396 1407 1412 1416 1425 1426 1432 1433 1441 1444 1449 1451\n",
      " 1456 1459 1472 1481 1483 1498 1499 1503 1505 1508 1509 1522 1524 1530\n",
      " 1531 1533 1543 1558 1570 1577 1580 1585 1594 1596 1599 1604 1607 1610\n",
      " 1615 1631 1646 1654 1655 1658 1661 1669 1672 1673 1677 1685 1686 1688\n",
      " 1698 1703 1709 1720 1727 1729 1731 1732 1743 1747 1748 1750 1753 1757\n",
      " 1761 1770 1771 1786 1790 1796 1798 1799 1809 1824 1827 1838 1839 1851\n",
      " 1854 1859 1862 1866 1870 1873 1878 1882 1883 1887 1891 1899 1903 1905\n",
      " 1910 1911 1916 1919 1920 1924 1929 1932 1933 1935 1943 1950 1951 1953\n",
      " 1955 1957 1966 1971 1974 1979 1982 1989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 15s 9ms/sample - loss: 0.3353 - acc: 0.8023\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3113 - acc: 0.8597\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3069 - acc: 0.8619\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.3025 - acc: 0.8638\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2999 - acc: 0.8653\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2984 - acc: 0.8660\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2962 - acc: 0.8671\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2928 - acc: 0.8688\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2921 - acc: 0.8688\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2903 - acc: 0.8702\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2886 - acc: 0.8711\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2862 - acc: 0.8716\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2836 - acc: 0.8743\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2841 - acc: 0.8739\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2806 - acc: 0.8758\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2788 - acc: 0.8766\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2768 - acc: 0.8776\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2767 - acc: 0.8780\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2707 - acc: 0.8806\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2691 - acc: 0.8816\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2638 - acc: 0.8846\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2580 - acc: 0.8876\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2525 - acc: 0.8903\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2498 - acc: 0.8913\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2451 - acc: 0.8940\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2436 - acc: 0.8944\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2392 - acc: 0.8968\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2369 - acc: 0.8977\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2358 - acc: 0.8981\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2322 - acc: 0.8996\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2288 - acc: 0.9011\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 12s 8ms/sample - loss: 0.2256 - acc: 0.9029\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2231 - acc: 0.9039\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2197 - acc: 0.9058\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2191 - acc: 0.9061\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2176 - acc: 0.9066\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2125 - acc: 0.9090\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2108 - acc: 0.9099\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2076 - acc: 0.9114\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2051 - acc: 0.9126\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2022 - acc: 0.9139\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.2011 - acc: 0.9146\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1981 - acc: 0.9157\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1967 - acc: 0.9163\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1957 - acc: 0.9167\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1929 - acc: 0.9178\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1910 - acc: 0.9189\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1879 - acc: 0.9201\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1863 - acc: 0.9208\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1846 - acc: 0.9217\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1824 - acc: 0.9228\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1807 - acc: 0.9236\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1791 - acc: 0.9243\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1776 - acc: 0.9251\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1764 - acc: 0.9256\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1771 - acc: 0.9254\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1752 - acc: 0.9261\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1721 - acc: 0.9273\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1701 - acc: 0.9281\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1689 - acc: 0.9287\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1671 - acc: 0.9299\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1663 - acc: 0.9297\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1646 - acc: 0.9307\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1641 - acc: 0.9310\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1633 - acc: 0.9315\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1623 - acc: 0.9316\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1593 - acc: 0.9332\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1602 - acc: 0.9327\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1579 - acc: 0.9338\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1571 - acc: 0.9339\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1558 - acc: 0.9347\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1557 - acc: 0.9347\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1551 - acc: 0.9351\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1527 - acc: 0.9358\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1518 - acc: 0.9366\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1503 - acc: 0.9370\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1504 - acc: 0.9370\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1483 - acc: 0.9381\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1473 - acc: 0.9383\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1467 - acc: 0.9386\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1458 - acc: 0.9391\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1459 - acc: 0.9391\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1448 - acc: 0.9394\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1433 - acc: 0.9401\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1431 - acc: 0.9403\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1420 - acc: 0.9405\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1403 - acc: 0.9415\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1400 - acc: 0.9413\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1403 - acc: 0.9415\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1390 - acc: 0.9420\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1378 - acc: 0.9426\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1369 - acc: 0.9428\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1362 - acc: 0.9431\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1364 - acc: 0.9433\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1352 - acc: 0.9437\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1343 - acc: 0.9440\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1351 - acc: 0.9438\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1357 - acc: 0.9435\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1333 - acc: 0.9446\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 13s 8ms/sample - loss: 0.1312 - acc: 0.9457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.73      0.71     42694\n",
      "           1       0.68      0.61      0.65     22203\n",
      "           2       0.76      0.76      0.76     39186\n",
      "\n",
      "    accuracy                           0.72    104083\n",
      "   macro avg       0.71      0.70      0.71    104083\n",
      "weighted avg       0.72      0.72      0.72    104083\n",
      "\n",
      "Acur√°cia\n",
      "0.700882336312242\n",
      "Precisao\n",
      "0.7156505054487694\n",
      "Recall\n",
      "0.7160535341986684\n",
      "F1\n",
      "0.7153066589156425\n",
      "[[31038  4414  7242]\n",
      " [ 6289 13617  2297]\n",
      " [ 7377  1935 29874]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_20 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 798,603\n",
      "Trainable params: 798,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.70537274 0.70944676 0.70761891 0.71745467 0.70088234]\n",
      "0.7081550810090278\n",
      "Precision total\n",
      "[0.71972733 0.72465629 0.71860687 0.72931164 0.71565051]\n",
      "0.7215905283084029\n",
      "Recalls total\n",
      "[0.72077441 0.72221773 0.71789164 0.72775858 0.71605353]\n",
      "0.7209391781612478\n",
      "F1 total\n",
      "[0.71940028 0.72221423 0.71771944 0.7280167  0.71530666]\n",
      "0.720531463028926\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
