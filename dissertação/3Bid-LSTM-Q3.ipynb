{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 28:31].values\n",
    "classes = np.reshape(classes, (2000, 700, 3))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 3))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 3))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 21:26:19.110342  5592 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 21:26:19.115306  5592 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 21:26:19.116303  5592 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 21:26:19.117330  5592 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   1    2    3 ... 1995 1998 1999] TEST: [   0    4   19   21   28   33   39   48   49   54   60   62   63   65\n",
      "   75   76   84   92   95   99  101  107  108  115  122  130  132  135\n",
      "  139  146  147  154  161  173  179  181  184  192  195  202  211  216\n",
      "  221  228  233  241  247  250  257  259  261  266  270  278  281  288\n",
      "  291  301  302  304  310  319  320  329  336  339  342  350  354  364\n",
      "  373  380  385  395  399  409  410  417  421  437  440  443  445  446\n",
      "  447  450  453  460  463  468  469  479  481  487  488  491  494  500\n",
      "  505  506  509  512  519  532  533  543  544  560  561  565  570  571\n",
      "  576  581  583  584  590  594  597  600  611  618  622  624  629  632\n",
      "  637  645  658  663  670  672  687  688  689  695  700  706  707  708\n",
      "  720  725  733  736  738  740  747  757  758  762  772  775  776  777\n",
      "  784  794  800  817  831  833  850  856  857  859  869  876  879  880\n",
      "  883  884  899  900  906  917  918  920  921  928  929  940  942  948\n",
      "  951  956  958  961  963  965  977  982  983  991  994 1000 1001 1006\n",
      " 1008 1016 1027 1032 1043 1046 1050 1058 1070 1073 1077 1081 1083 1084\n",
      " 1090 1091 1092 1099 1102 1106 1110 1113 1116 1127 1128 1133 1134 1136\n",
      " 1138 1143 1145 1147 1149 1154 1163 1164 1180 1193 1194 1200 1212 1215\n",
      " 1219 1220 1231 1236 1238 1239 1257 1259 1265 1274 1293 1294 1306 1309\n",
      " 1315 1323 1328 1338 1339 1345 1347 1349 1350 1358 1368 1372 1376 1385\n",
      " 1386 1388 1392 1398 1404 1408 1411 1414 1417 1420 1426 1428 1430 1438\n",
      " 1446 1447 1451 1472 1473 1483 1485 1488 1493 1499 1503 1505 1507 1510\n",
      " 1511 1518 1523 1532 1536 1539 1544 1548 1554 1556 1561 1584 1586 1587\n",
      " 1593 1599 1600 1617 1618 1622 1628 1632 1637 1638 1639 1641 1647 1658\n",
      " 1659 1665 1666 1671 1674 1675 1679 1688 1690 1692 1695 1703 1704 1705\n",
      " 1707 1708 1710 1717 1718 1728 1734 1735 1736 1737 1753 1754 1760 1764\n",
      " 1768 1769 1774 1778 1782 1787 1790 1792 1798 1802 1803 1806 1818 1822\n",
      " 1831 1834 1836 1839 1848 1851 1856 1867 1873 1884 1890 1895 1906 1917\n",
      " 1920 1921 1930 1933 1942 1947 1950 1954 1959 1960 1963 1964 1966 1973\n",
      " 1974 1978 1980 1983 1989 1990 1996 1997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 21:26:20.116643  5592 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3566 - acc: 0.7689\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3190 - acc: 0.8534\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3049 - acc: 0.8622\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3003 - acc: 0.8653\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2956 - acc: 0.8680\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2936 - acc: 0.8690\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2921 - acc: 0.8697\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2874 - acc: 0.8718\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2867 - acc: 0.8721\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2860 - acc: 0.8727\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2818 - acc: 0.8746\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2860 - acc: 0.8717\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2821 - acc: 0.8735\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2787 - acc: 0.8747\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2755 - acc: 0.8774\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2744 - acc: 0.8782\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2714 - acc: 0.8789\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2688 - acc: 0.8809\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2672 - acc: 0.8803\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2638 - acc: 0.8824\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2623 - acc: 0.8839\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2592 - acc: 0.8850\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2559 - acc: 0.8864\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2530 - acc: 0.8868\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2518 - acc: 0.8887\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2501 - acc: 0.8891\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2487 - acc: 0.8896\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2426 - acc: 0.8930\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2364 - acc: 0.8951\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2331 - acc: 0.8975\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2301 - acc: 0.8981\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2266 - acc: 0.8998\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2238 - acc: 0.9030\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2211 - acc: 0.9032\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2200 - acc: 0.9040\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2163 - acc: 0.9057\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2111 - acc: 0.9086\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2067 - acc: 0.9110\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2052 - acc: 0.9114\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2022 - acc: 0.9130\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1984 - acc: 0.9141\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1979 - acc: 0.9138\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1947 - acc: 0.9149\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1905 - acc: 0.9173\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1870 - acc: 0.9195\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1883 - acc: 0.9172\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1839 - acc: 0.9202\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1809 - acc: 0.9226\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1784 - acc: 0.9240\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1773 - acc: 0.9245\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1751 - acc: 0.9254\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1734 - acc: 0.9258\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1693 - acc: 0.9276\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1685 - acc: 0.9279\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1687 - acc: 0.9286\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1662 - acc: 0.9295\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1635 - acc: 0.9304\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1616 - acc: 0.9315\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1584 - acc: 0.9329\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1578 - acc: 0.9329\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1556 - acc: 0.9340\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1536 - acc: 0.9349\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1526 - acc: 0.9356\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1520 - acc: 0.9359\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1502 - acc: 0.9366\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1504 - acc: 0.9365\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1482 - acc: 0.9373\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1449 - acc: 0.9391\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1444 - acc: 0.9389\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1426 - acc: 0.9397\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1423 - acc: 0.9398\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1412 - acc: 0.9406\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1403 - acc: 0.9409\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1386 - acc: 0.9419\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1374 - acc: 0.9425\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1375 - acc: 0.9424\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1356 - acc: 0.9430\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1339 - acc: 0.9439\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1340 - acc: 0.9436\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1333 - acc: 0.9440\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1317 - acc: 0.9444\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1310 - acc: 0.9449\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1291 - acc: 0.9455\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1273 - acc: 0.9465\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1263 - acc: 0.9469\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1270 - acc: 0.9465\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1273 - acc: 0.9465\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1245 - acc: 0.9478\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1258 - acc: 0.9469\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1260 - acc: 0.9466\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1221 - acc: 0.9488\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1218 - acc: 0.9488\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1205 - acc: 0.9494\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1199 - acc: 0.9495\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1184 - acc: 0.9503\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1184 - acc: 0.9505\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1179 - acc: 0.9508\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1172 - acc: 0.9510\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1168 - acc: 0.9511\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1153 - acc: 0.9519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.70     43157\n",
      "           1       0.68      0.57      0.62     23601\n",
      "           2       0.72      0.73      0.73     37249\n",
      "\n",
      "    accuracy                           0.70    104007\n",
      "   macro avg       0.69      0.68      0.68    104007\n",
      "weighted avg       0.69      0.70      0.69    104007\n",
      "\n",
      "Acurácia\n",
      "0.678495436035334\n",
      "Precisao\n",
      "0.6946104486000877\n",
      "Recall\n",
      "0.6950301422019671\n",
      "F1\n",
      "0.6935333068645704\n",
      "[[31456  4319  7382]\n",
      " [ 6839 13554  3208]\n",
      " [ 7812  2159 27278]]\n",
      "TRAIN: [   0    3    4 ... 1997 1998 1999] TEST: [   1    2    5    6   13   18   27   30   32   42   46   51   53   58\n",
      "   64   67   70   71   74   79   85   86   88   96   98  100  105  111\n",
      "  113  120  126  129  140  141  142  145  148  149  155  158  168  170\n",
      "  171  177  183  189  191  223  224  232  234  245  251  258  273  275\n",
      "  282  293  296  315  326  331  333  337  340  341  346  349  353  356\n",
      "  368  382  387  389  390  391  393  397  402  403  405  412  429  441\n",
      "  444  452  454  465  471  474  482  483  490  493  503  504  511  518\n",
      "  522  526  539  540  546  551  554  556  559  563  564  566  568  572\n",
      "  574  575  577  582  585  588  595  615  619  628  640  641  644  647\n",
      "  648  654  655  662  673  681  682  690  691  698  701  711  713  715\n",
      "  717  719  722  724  728  729  730  739  741  742  743  763  764  765\n",
      "  767  768  769  780  781  783  786  787  795  796  803  809  811  814\n",
      "  819  825  832  835  840  846  849  851  854  855  862  863  865  867\n",
      "  877  885  889  891  905  908  909  912  923  924  926  927  932  937\n",
      "  943  945  953  954  959  962  964  968  972  974  979  980  981  986\n",
      "  993 1013 1014 1022 1024 1029 1031 1045 1053 1057 1062 1065 1067 1071\n",
      " 1074 1076 1078 1089 1101 1103 1129 1131 1141 1150 1153 1157 1159 1161\n",
      " 1165 1166 1168 1170 1175 1183 1188 1192 1197 1201 1208 1224 1229 1230\n",
      " 1235 1237 1245 1247 1249 1254 1255 1263 1267 1268 1269 1270 1271 1273\n",
      " 1280 1282 1283 1286 1297 1302 1307 1308 1327 1329 1330 1331 1337 1340\n",
      " 1351 1354 1360 1363 1365 1370 1379 1395 1399 1400 1403 1412 1413 1416\n",
      " 1423 1424 1434 1436 1440 1450 1456 1457 1469 1477 1479 1496 1497 1515\n",
      " 1522 1524 1526 1527 1529 1531 1533 1534 1535 1537 1538 1540 1550 1558\n",
      " 1564 1565 1566 1570 1575 1580 1589 1594 1608 1611 1614 1615 1624 1625\n",
      " 1629 1631 1643 1644 1648 1653 1660 1661 1673 1682 1683 1693 1696 1697\n",
      " 1699 1700 1711 1715 1719 1732 1742 1743 1750 1757 1765 1771 1783 1788\n",
      " 1789 1791 1796 1799 1813 1815 1821 1823 1825 1828 1830 1838 1852 1859\n",
      " 1860 1866 1872 1888 1894 1899 1905 1910 1918 1919 1937 1941 1945 1955\n",
      " 1956 1961 1962 1976 1979 1984 1991 1992]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.3511 - acc: 0.7786\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3132 - acc: 0.8565\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3023 - acc: 0.8634\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2954 - acc: 0.8669\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2923 - acc: 0.8683\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2906 - acc: 0.8693\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2865 - acc: 0.8707\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2846 - acc: 0.8721\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2824 - acc: 0.8729\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2791 - acc: 0.8742\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2782 - acc: 0.8757\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2775 - acc: 0.8758\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2726 - acc: 0.8784\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2748 - acc: 0.8779\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2697 - acc: 0.8810\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2698 - acc: 0.8804\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2662 - acc: 0.8819\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2639 - acc: 0.8835\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 9s 5ms/sample - loss: 0.2605 - acc: 0.8852\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2567 - acc: 0.8873\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2537 - acc: 0.8886\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2482 - acc: 0.8914\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2488 - acc: 0.8907\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2442 - acc: 0.8910\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2400 - acc: 0.8934\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2384 - acc: 0.8949\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2384 - acc: 0.8957\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2318 - acc: 0.8975\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2264 - acc: 0.9018\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2229 - acc: 0.9011\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2208 - acc: 0.9043\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2184 - acc: 0.9037\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2142 - acc: 0.9070\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2099 - acc: 0.9098\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2067 - acc: 0.9111\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2044 - acc: 0.9122\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1997 - acc: 0.9145\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1965 - acc: 0.9157\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1935 - acc: 0.9173\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1914 - acc: 0.9181\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1895 - acc: 0.9174\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1859 - acc: 0.9196\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1830 - acc: 0.9212\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1789 - acc: 0.9232\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1764 - acc: 0.9241\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1747 - acc: 0.9255\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1731 - acc: 0.9255\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1707 - acc: 0.9256\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1679 - acc: 0.9272\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1656 - acc: 0.9280\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1638 - acc: 0.9282\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1617 - acc: 0.9297\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1609 - acc: 0.9289\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1581 - acc: 0.9280\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1564 - acc: 0.9309\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1556 - acc: 0.9320\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1530 - acc: 0.9345\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1517 - acc: 0.9352\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1485 - acc: 0.9373\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1485 - acc: 0.9358\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1467 - acc: 0.9359\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1448 - acc: 0.9352\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1432 - acc: 0.9384\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1423 - acc: 0.9395\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1395 - acc: 0.9397\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1395 - acc: 0.9390\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1381 - acc: 0.9392\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1368 - acc: 0.9401\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1354 - acc: 0.9429\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1355 - acc: 0.9429\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1392 - acc: 0.9410\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1316 - acc: 0.9447\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1308 - acc: 0.9447\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1289 - acc: 0.9459\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1278 - acc: 0.9464\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1277 - acc: 0.9450\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1270 - acc: 0.9461\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1261 - acc: 0.9464\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1242 - acc: 0.9479\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1235 - acc: 0.9486\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1229 - acc: 0.9484\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1218 - acc: 0.9488\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1205 - acc: 0.9494\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1210 - acc: 0.9489\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1202 - acc: 0.9485\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1184 - acc: 0.9491\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1180 - acc: 0.9506\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1180 - acc: 0.9504\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1167 - acc: 0.9506\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1151 - acc: 0.9520\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1141 - acc: 0.9523\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1128 - acc: 0.9523\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1130 - acc: 0.9523\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1131 - acc: 0.9526\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1134 - acc: 0.9530\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1124 - acc: 0.9527\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1113 - acc: 0.9531\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1110 - acc: 0.9536\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1094 - acc: 0.9540\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1087 - acc: 0.9538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69     44441\n",
      "           1       0.65      0.57      0.61     24531\n",
      "           2       0.72      0.71      0.71     39464\n",
      "\n",
      "    accuracy                           0.68    108436\n",
      "   macro avg       0.68      0.67      0.67    108436\n",
      "weighted avg       0.68      0.68      0.68    108436\n",
      "\n",
      "Acurácia\n",
      "0.66855661396982\n",
      "Precisao\n",
      "0.6840503687276476\n",
      "Recall\n",
      "0.684062488472463\n",
      "F1\n",
      "0.6829874191087179\n",
      "[[32145  4913  7383]\n",
      " [ 7031 14086  3414]\n",
      " [ 8982  2536 27946]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   7    9   10   11   12   14   20   26   34   36   37   38   41   52\n",
      "   55   57   66   72   73   77   80   81   87   90   97  103  117  118\n",
      "  119  125  143  152  157  165  167  172  174  178  180  186  187  188\n",
      "  199  203  205  207  208  209  213  217  218  219  231  249  252  264\n",
      "  267  272  277  279  280  287  297  299  303  306  307  312  316  317\n",
      "  321  325  327  328  334  347  352  355  359  361  370  374  375  396\n",
      "  404  406  408  419  424  425  430  433  434  435  438  439  448  449\n",
      "  455  462  466  467  475  477  478  484  492  495  496  497  501  513\n",
      "  517  527  529  531  534  535  536  538  549  553  555  558  587  592\n",
      "  604  605  610  613  621  625  634  635  646  650  657  659  664  666\n",
      "  674  686  696  699  703  705  709  710  712  714  727  734  748  751\n",
      "  752  753  754  759  766  771  779  785  790  801  802  804  806  823\n",
      "  826  827  829  837  844  848  868  872  878  881  888  896  903  904\n",
      "  913  914  916  922  931  941  946  950  966  969  973  989  990  996\n",
      "  997  998 1010 1018 1021 1026 1034 1035 1036 1039 1040 1041 1047 1052\n",
      " 1055 1063 1075 1079 1080 1094 1097 1100 1104 1107 1115 1121 1122 1125\n",
      " 1130 1132 1135 1137 1139 1155 1156 1158 1160 1169 1172 1174 1177 1187\n",
      " 1196 1203 1205 1207 1213 1227 1228 1248 1250 1251 1260 1262 1266 1272\n",
      " 1278 1288 1292 1298 1299 1301 1310 1314 1318 1322 1324 1332 1333 1334\n",
      " 1335 1341 1342 1343 1346 1353 1356 1357 1371 1377 1380 1381 1383 1384\n",
      " 1387 1393 1401 1402 1405 1410 1419 1425 1427 1435 1442 1445 1454 1459\n",
      " 1460 1461 1462 1463 1474 1476 1484 1489 1492 1498 1501 1514 1516 1521\n",
      " 1530 1541 1559 1568 1571 1579 1581 1582 1590 1591 1592 1597 1598 1602\n",
      " 1605 1610 1623 1627 1646 1649 1651 1652 1663 1668 1676 1677 1678 1691\n",
      " 1701 1716 1721 1722 1725 1730 1741 1745 1756 1758 1759 1763 1766 1767\n",
      " 1770 1780 1784 1785 1804 1805 1810 1826 1827 1833 1835 1837 1840 1846\n",
      " 1854 1855 1857 1858 1862 1863 1865 1869 1871 1874 1875 1876 1877 1879\n",
      " 1880 1887 1891 1893 1901 1909 1911 1926 1934 1936 1939 1949 1952 1968\n",
      " 1971 1977 1982 1985 1987 1993 1994 1995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.3601 - acc: 0.7831\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3212 - acc: 0.8526\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3075 - acc: 0.8609\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3018 - acc: 0.8576\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2992 - acc: 0.8660\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2954 - acc: 0.8683\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2906 - acc: 0.8699\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2896 - acc: 0.8702\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2891 - acc: 0.8697\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2855 - acc: 0.8729\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2847 - acc: 0.8721\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2821 - acc: 0.8734\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2817 - acc: 0.8744\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2787 - acc: 0.8759\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2762 - acc: 0.8770\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2732 - acc: 0.8786\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2735 - acc: 0.8785\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2695 - acc: 0.8804\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2662 - acc: 0.8829\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2638 - acc: 0.8840\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2621 - acc: 0.8830\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2568 - acc: 0.8870\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2553 - acc: 0.8873\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2524 - acc: 0.8898\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2458 - acc: 0.8937\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2421 - acc: 0.8954\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2395 - acc: 0.8965\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2361 - acc: 0.8981\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2385 - acc: 0.8971\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2318 - acc: 0.8990\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 9s 5ms/sample - loss: 0.2248 - acc: 0.9028\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2212 - acc: 0.9042\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2199 - acc: 0.9055\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2156 - acc: 0.9073\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2210 - acc: 0.9037\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2166 - acc: 0.9036\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2069 - acc: 0.9093\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2037 - acc: 0.9119\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1992 - acc: 0.9141\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1969 - acc: 0.9159\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1947 - acc: 0.9165\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1897 - acc: 0.9192\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1895 - acc: 0.9194\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1889 - acc: 0.9195\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1850 - acc: 0.9209\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1825 - acc: 0.9224\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1789 - acc: 0.9233\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1760 - acc: 0.9242\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1734 - acc: 0.9257\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1700 - acc: 0.9275\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1708 - acc: 0.9267\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1679 - acc: 0.9286\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1655 - acc: 0.9299\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1629 - acc: 0.9309\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1613 - acc: 0.9319\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1602 - acc: 0.9321\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1574 - acc: 0.9325\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1561 - acc: 0.9332\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1546 - acc: 0.9341\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1550 - acc: 0.9303\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1518 - acc: 0.9312\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1498 - acc: 0.9323\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1479 - acc: 0.9334\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1463 - acc: 0.9341\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1461 - acc: 0.9341\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1437 - acc: 0.9365\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1420 - acc: 0.9375\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1405 - acc: 0.9383\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1400 - acc: 0.9380\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1394 - acc: 0.9384\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1386 - acc: 0.9389\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1371 - acc: 0.9397\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1358 - acc: 0.9398\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1352 - acc: 0.9394\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1335 - acc: 0.9399\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1331 - acc: 0.9406\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1321 - acc: 0.9414\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1300 - acc: 0.9424\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1289 - acc: 0.9434\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1277 - acc: 0.9437\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1273 - acc: 0.9433\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1270 - acc: 0.9443\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1256 - acc: 0.9452\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1258 - acc: 0.9450\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1238 - acc: 0.9458\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1231 - acc: 0.9460\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1223 - acc: 0.9468\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1214 - acc: 0.9458\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1210 - acc: 0.9469\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1210 - acc: 0.9475\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1205 - acc: 0.9471\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1184 - acc: 0.9477\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1179 - acc: 0.9483\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1183 - acc: 0.9485\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1175 - acc: 0.9485\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1169 - acc: 0.9489\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1149 - acc: 0.9503\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 9s 5ms/sample - loss: 0.1146 - acc: 0.9495\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 9s 5ms/sample - loss: 0.1144 - acc: 0.9492\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1139 - acc: 0.9494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.69     41696\n",
      "           1       0.63      0.57      0.60     21909\n",
      "           2       0.73      0.73      0.73     39474\n",
      "\n",
      "    accuracy                           0.69    103079\n",
      "   macro avg       0.68      0.67      0.67    103079\n",
      "weighted avg       0.69      0.69      0.69    103079\n",
      "\n",
      "Acurácia\n",
      "0.6686377225360886\n",
      "Precisao\n",
      "0.6858464151590264\n",
      "Recall\n",
      "0.6869779489517749\n",
      "F1\n",
      "0.6858382897898552\n",
      "[[29502  4528  7666]\n",
      " [ 6220 12399  3290]\n",
      " [ 7950  2612 28912]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1998] TEST: [   3    8   16   17   24   25   29   35   43   44   61   82   83   91\n",
      "  110  112  116  121  127  128  131  133  134  153  162  163  164  166\n",
      "  175  176  182  185  190  194  196  197  198  200  201  214  215  220\n",
      "  222  227  237  238  239  244  253  256  262  271  276  285  290  292\n",
      "  300  305  308  309  313  314  318  322  323  324  330  332  335  338\n",
      "  343  348  351  358  366  369  371  372  383  388  394  398  400  401\n",
      "  411  413  414  415  416  418  420  423  426  432  457  459  461  464\n",
      "  470  472  476  480  485  486  499  508  514  516  524  525  530  537\n",
      "  541  545  550  569  580  591  593  602  609  617  638  639  651  652\n",
      "  653  656  660  661  667  677  680  683  685  693  697  702  704  716\n",
      "  718  723  731  735  749  755  756  773  782  791  792  793  798  799\n",
      "  808  810  812  813  815  821  822  836  841  842  845  847  852  858\n",
      "  860  864  873  874  886  892  901  907  910  911  915  919  925  930\n",
      "  933  934  935  936  938  939  944  947  949  960  975  976  978  984\n",
      "  987  988  995  999 1002 1003 1004 1005 1007 1011 1012 1017 1019 1020\n",
      " 1023 1025 1028 1033 1038 1044 1049 1056 1066 1068 1069 1072 1082 1085\n",
      " 1087 1109 1111 1117 1118 1120 1123 1124 1146 1151 1152 1162 1167 1173\n",
      " 1176 1178 1182 1184 1186 1189 1198 1199 1202 1204 1210 1211 1217 1218\n",
      " 1221 1222 1240 1242 1246 1252 1253 1256 1261 1264 1281 1284 1285 1287\n",
      " 1291 1296 1304 1319 1321 1325 1326 1348 1352 1359 1361 1366 1369 1373\n",
      " 1374 1375 1378 1382 1390 1397 1406 1418 1421 1431 1439 1441 1443 1444\n",
      " 1455 1458 1465 1471 1481 1487 1490 1491 1494 1502 1506 1509 1517 1519\n",
      " 1525 1528 1549 1552 1553 1560 1562 1574 1578 1585 1606 1609 1621 1626\n",
      " 1635 1640 1642 1650 1654 1655 1656 1669 1670 1672 1680 1681 1686 1702\n",
      " 1706 1709 1720 1723 1724 1726 1729 1738 1740 1744 1746 1748 1749 1761\n",
      " 1772 1773 1775 1776 1777 1781 1794 1797 1801 1811 1814 1816 1817 1819\n",
      " 1829 1832 1843 1844 1853 1861 1868 1878 1882 1885 1896 1897 1902 1903\n",
      " 1904 1908 1914 1916 1923 1931 1938 1940 1944 1946 1948 1953 1957 1965\n",
      " 1967 1970 1972 1975 1981 1986 1988 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.3607 - acc: 0.7425\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3232 - acc: 0.8502\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3091 - acc: 0.8600\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3033 - acc: 0.8636\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2993 - acc: 0.8656\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2964 - acc: 0.8671\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2953 - acc: 0.8677\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2901 - acc: 0.8698\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2885 - acc: 0.8707\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2862 - acc: 0.8722\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2838 - acc: 0.8734\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2830 - acc: 0.8740\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2794 - acc: 0.8761\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2928 - acc: 0.8696\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2822 - acc: 0.8749\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2803 - acc: 0.8762\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2751 - acc: 0.8784\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2732 - acc: 0.8799\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2708 - acc: 0.8809\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2664 - acc: 0.8835\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2649 - acc: 0.8842\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2678 - acc: 0.8818\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2633 - acc: 0.8846\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2611 - acc: 0.8850\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2560 - acc: 0.8881\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2535 - acc: 0.8898\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2501 - acc: 0.8906\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2478 - acc: 0.8917\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2418 - acc: 0.8942\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2405 - acc: 0.8957\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2363 - acc: 0.8974\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2402 - acc: 0.8957\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2300 - acc: 0.8997\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2253 - acc: 0.9016\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2218 - acc: 0.9034\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2204 - acc: 0.9020\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2153 - acc: 0.9040\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2135 - acc: 0.9049\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2113 - acc: 0.9054\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2072 - acc: 0.9067\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2046 - acc: 0.9077\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2015 - acc: 0.9083\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1984 - acc: 0.9090\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1957 - acc: 0.9094\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1959 - acc: 0.9103\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1938 - acc: 0.9101\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1879 - acc: 0.9130\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1866 - acc: 0.9140\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1842 - acc: 0.9143\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1823 - acc: 0.9160\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1781 - acc: 0.9170\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1752 - acc: 0.9213\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1736 - acc: 0.9209\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1710 - acc: 0.9209\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1700 - acc: 0.9216\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1684 - acc: 0.9219\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1670 - acc: 0.9242\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1674 - acc: 0.9236\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1658 - acc: 0.9234\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1603 - acc: 0.9257\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1584 - acc: 0.9256\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1572 - acc: 0.9272\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1566 - acc: 0.9269\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1538 - acc: 0.9291\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1515 - acc: 0.9306\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1499 - acc: 0.9304\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1529 - acc: 0.9288\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1537 - acc: 0.9284\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1479 - acc: 0.9310\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1452 - acc: 0.9335\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1432 - acc: 0.9332\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1429 - acc: 0.9328\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1423 - acc: 0.9337\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1405 - acc: 0.9346\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1386 - acc: 0.9350\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1381 - acc: 0.9355\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1358 - acc: 0.9363\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1351 - acc: 0.9378\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 9s 5ms/sample - loss: 0.1347 - acc: 0.9368\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.1340 - acc: 0.9374\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.1326 - acc: 0.9375\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.1306 - acc: 0.9377\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.1298 - acc: 0.9381\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1296 - acc: 0.9381 - 10s 6ms/sample - loss: 0.1292 - acc: 0.9383\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.1296 - acc: 0.9386\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1274 - acc: 0.9400\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1262 - acc: 0.9409\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1253 - acc: 0.9410\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1252 - acc: 0.9407\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1233 - acc: 0.9412\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1237 - acc: 0.9410\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1231 - acc: 0.9417\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1212 - acc: 0.9431\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1207 - acc: 0.9439\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1201 - acc: 0.9449\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1187 - acc: 0.9434\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1185 - acc: 0.9452\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1182 - acc: 0.9443\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1176 - acc: 0.9444\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1159 - acc: 0.9463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69     40889\n",
      "           1       0.66      0.57      0.61     23413\n",
      "           2       0.70      0.70      0.70     34866\n",
      "\n",
      "    accuracy                           0.68     99168\n",
      "   macro avg       0.67      0.66      0.67     99168\n",
      "weighted avg       0.68      0.68      0.67     99168\n",
      "\n",
      "Acurácia\n",
      "0.6627165440910542\n",
      "Precisao\n",
      "0.6756860447892147\n",
      "Recall\n",
      "0.6760951113262342\n",
      "F1\n",
      "0.6748924723169031\n",
      "[[29097  4545  7247]\n",
      " [ 6533 13406  3474]\n",
      " [ 7916  2406 24544]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [  15   22   23   31   40   45   47   50   56   59   68   69   78   89\n",
      "   93   94  102  104  106  109  114  123  124  136  137  138  144  150\n",
      "  151  156  159  160  169  193  204  206  210  212  225  226  229  230\n",
      "  235  236  240  242  243  246  248  254  255  260  263  265  268  269\n",
      "  274  283  284  286  289  294  295  298  311  344  345  357  360  362\n",
      "  363  365  367  376  377  378  379  381  384  386  392  407  422  427\n",
      "  428  431  436  442  451  456  458  473  489  498  502  507  510  515\n",
      "  520  521  523  528  542  547  548  552  557  562  567  573  578  579\n",
      "  586  589  596  598  599  601  603  606  607  608  612  614  616  620\n",
      "  623  626  627  630  631  633  636  642  643  649  665  668  669  671\n",
      "  675  676  678  679  684  692  694  721  726  732  737  744  745  746\n",
      "  750  760  761  770  774  778  788  789  797  805  807  816  818  820\n",
      "  824  828  830  834  838  839  843  853  861  866  870  871  875  882\n",
      "  887  890  893  894  895  897  898  902  952  955  957  967  970  971\n",
      "  985  992 1009 1015 1030 1037 1042 1048 1051 1054 1059 1060 1061 1064\n",
      " 1086 1088 1093 1095 1096 1098 1105 1108 1112 1114 1119 1126 1140 1142\n",
      " 1144 1148 1171 1179 1181 1185 1190 1191 1195 1206 1209 1214 1216 1223\n",
      " 1225 1226 1232 1233 1234 1241 1243 1244 1258 1275 1276 1277 1279 1289\n",
      " 1290 1295 1300 1303 1305 1311 1312 1313 1316 1317 1320 1336 1344 1355\n",
      " 1362 1364 1367 1389 1391 1394 1396 1407 1409 1415 1422 1429 1432 1433\n",
      " 1437 1448 1449 1452 1453 1464 1466 1467 1468 1470 1475 1478 1480 1482\n",
      " 1486 1495 1500 1504 1508 1512 1513 1520 1542 1543 1545 1546 1547 1551\n",
      " 1555 1557 1563 1567 1569 1572 1573 1576 1577 1583 1588 1595 1596 1601\n",
      " 1603 1604 1607 1612 1613 1616 1619 1620 1630 1633 1634 1636 1645 1657\n",
      " 1662 1664 1667 1684 1685 1687 1689 1694 1698 1712 1713 1714 1727 1731\n",
      " 1733 1739 1747 1751 1752 1755 1762 1779 1786 1793 1795 1800 1807 1808\n",
      " 1809 1812 1820 1824 1841 1842 1845 1847 1849 1850 1864 1870 1881 1883\n",
      " 1886 1889 1892 1898 1900 1907 1912 1913 1915 1922 1924 1925 1927 1928\n",
      " 1929 1932 1935 1943 1951 1958 1969 1998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 9s 6ms/sample - loss: 0.3610 - acc: 0.8140\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3222 - acc: 0.8523\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3090 - acc: 0.8614\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3048 - acc: 0.8638\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2981 - acc: 0.8673\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2967 - acc: 0.8680\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2918 - acc: 0.8705\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2903 - acc: 0.8710\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2897 - acc: 0.8710\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2862 - acc: 0.8724\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2859 - acc: 0.8719\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2827 - acc: 0.8741\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2814 - acc: 0.8753\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2788 - acc: 0.8764\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2770 - acc: 0.8779\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2733 - acc: 0.8795\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2717 - acc: 0.8802\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2687 - acc: 0.8815\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2672 - acc: 0.8827\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2648 - acc: 0.8841\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2620 - acc: 0.8854\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2603 - acc: 0.8865\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2571 - acc: 0.8878\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2532 - acc: 0.8900\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2529 - acc: 0.8899\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2558 - acc: 0.8880\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2546 - acc: 0.8889\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2723 - acc: 0.8799\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2878 - acc: 0.8720\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2716 - acc: 0.8800\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2656 - acc: 0.8829\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2606 - acc: 0.8850\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2550 - acc: 0.8877\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2491 - acc: 0.8902\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2446 - acc: 0.8926\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2423 - acc: 0.8932\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2428 - acc: 0.8931\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2357 - acc: 0.8971\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2378 - acc: 0.8966\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2327 - acc: 0.8983\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2275 - acc: 0.9006\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2259 - acc: 0.9012\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2242 - acc: 0.9031\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2204 - acc: 0.9043\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2181 - acc: 0.9059\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2136 - acc: 0.9079\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2109 - acc: 0.9089\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2105 - acc: 0.9088\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2068 - acc: 0.9106\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2036 - acc: 0.9121\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2027 - acc: 0.9123\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2243 - acc: 0.9017\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2055 - acc: 0.9114\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1994 - acc: 0.9134\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1957 - acc: 0.9155\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1931 - acc: 0.9174\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1908 - acc: 0.9181\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1880 - acc: 0.9192\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1857 - acc: 0.9211\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1843 - acc: 0.9215\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1832 - acc: 0.9219\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1809 - acc: 0.9232\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1783 - acc: 0.9242\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1771 - acc: 0.9249\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1743 - acc: 0.9258\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1737 - acc: 0.9264\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1718 - acc: 0.9268\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1691 - acc: 0.9283\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1668 - acc: 0.9292\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1657 - acc: 0.9296\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1647 - acc: 0.9297\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1624 - acc: 0.9311\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1616 - acc: 0.9312\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1597 - acc: 0.9320\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1587 - acc: 0.9325\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1589 - acc: 0.9324\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1558 - acc: 0.9337\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1539 - acc: 0.9347\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1548 - acc: 0.9345\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1537 - acc: 0.9343\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1512 - acc: 0.9361\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1492 - acc: 0.9372\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1475 - acc: 0.9377\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1473 - acc: 0.9378\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1461 - acc: 0.9382\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1441 - acc: 0.9385\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1443 - acc: 0.9389\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1429 - acc: 0.9392\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1409 - acc: 0.9406\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1416 - acc: 0.9398\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1413 - acc: 0.9403\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1401 - acc: 0.9397\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1378 - acc: 0.9402\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1361 - acc: 0.9414\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1352 - acc: 0.9422\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1345 - acc: 0.9420\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1352 - acc: 0.9415\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1324 - acc: 0.9432\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1317 - acc: 0.9437\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.1315 - acc: 0.9441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.70     41415\n",
      "           1       0.67      0.55      0.60     21742\n",
      "           2       0.72      0.73      0.73     37961\n",
      "\n",
      "    accuracy                           0.69    101118\n",
      "   macro avg       0.69      0.67      0.68    101118\n",
      "weighted avg       0.69      0.69      0.69    101118\n",
      "\n",
      "Acurácia\n",
      "0.6681856702535173\n",
      "Precisao\n",
      "0.688819734215664\n",
      "Recall\n",
      "0.6890761288791313\n",
      "F1\n",
      "0.6872050463706801\n",
      "[[29972  3826  7617]\n",
      " [ 6566 11953  3223]\n",
      " [ 8185  2023 27753]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_12 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 3)            603       \n",
      "=================================================================\n",
      "Total params: 581,403\n",
      "Trainable params: 581,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.67849544 0.66855661 0.66863772 0.66271654 0.66818567]\n",
      "0.6693183973771628\n",
      "Precision total\n",
      "[0.69461045 0.68405037 0.68584642 0.67568604 0.68881973]\n",
      "0.6858026022983281\n",
      "Recalls total\n",
      "[0.69503014 0.68406249 0.68697795 0.67609511 0.68907613]\n",
      "0.6862483639663141\n",
      "F1 total\n",
      "[0.69353331 0.68298742 0.68583829 0.67489247 0.68720505]\n",
      "0.6848913068901454\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accu)\n",
    "accu = np.array(accu)\n",
    "print(accu.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "precisions = np.array(precisions)\n",
    "print(precisions.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "recalls = np.array(recalls)\n",
    "print(recalls.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f1 = np.array(f1)\n",
    "print(f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
