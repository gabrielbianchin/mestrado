{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNGRU, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "   # model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuq8 = []\n",
    "precisionsq8 = []\n",
    "recallsq8 = []\n",
    "f1q8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuq8.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisionsq8.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recallsq8.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1q8.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acurácia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0822 13:15:31.532517 11676 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 13:15:31.538491 11676 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 13:15:31.539533 11676 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 13:15:31.540486 11676 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1996 1997 1999] TEST: [   3    5   13   29   31   36   42   47   54   61   70   71   75   77\n",
      "   78   81   82   86   87   88   93   94   98  100  105  118  125  126\n",
      "  131  144  146  148  151  159  160  171  173  177  180  184  189  190\n",
      "  191  205  208  211  212  215  218  224  226  230  242  248  252  253\n",
      "  254  257  260  264  265  274  290  292  296  297  300  306  314  344\n",
      "  363  366  367  388  390  392  397  399  404  409  411  414  415  416\n",
      "  420  422  423  428  435  440  441  446  448  450  457  459  469  470\n",
      "  479  482  487  497  507  509  510  515  520  522  526  532  533  540\n",
      "  553  555  556  559  573  579  581  590  596  598  601  616  618  622\n",
      "  636  648  652  653  661  664  675  677  682  684  687  692  693  725\n",
      "  729  733  735  736  738  739  748  757  759  762  772  775  777  778\n",
      "  783  786  799  800  806  816  817  819  821  825  836  841  842  847\n",
      "  848  851  854  855  856  863  869  882  886  895  897  900  901  902\n",
      "  905  908  911  916  920  921  939  951  957  961  967  973  976  980\n",
      "  982  986  989 1002 1006 1011 1013 1014 1016 1022 1023 1026 1035 1038\n",
      " 1042 1043 1056 1057 1058 1062 1065 1070 1083 1086 1096 1099 1101 1104\n",
      " 1112 1118 1119 1120 1121 1126 1147 1149 1151 1152 1155 1161 1175 1178\n",
      " 1183 1187 1188 1190 1192 1194 1202 1203 1206 1213 1220 1233 1236 1237\n",
      " 1250 1259 1269 1270 1280 1282 1286 1291 1292 1294 1295 1296 1304 1306\n",
      " 1309 1315 1329 1332 1335 1337 1343 1348 1351 1354 1358 1359 1372 1375\n",
      " 1376 1381 1385 1409 1419 1425 1429 1430 1432 1445 1446 1447 1453 1458\n",
      " 1462 1465 1467 1471 1481 1489 1492 1510 1512 1517 1526 1530 1532 1533\n",
      " 1538 1540 1543 1549 1552 1553 1566 1568 1569 1575 1579 1585 1590 1601\n",
      " 1604 1607 1617 1620 1627 1633 1635 1636 1637 1639 1642 1648 1657 1666\n",
      " 1671 1688 1695 1696 1699 1702 1705 1707 1710 1719 1721 1722 1730 1734\n",
      " 1745 1746 1751 1756 1782 1785 1792 1794 1798 1805 1808 1810 1819 1821\n",
      " 1822 1827 1829 1833 1834 1836 1841 1843 1845 1851 1860 1862 1870 1876\n",
      " 1877 1880 1900 1903 1909 1912 1924 1927 1928 1934 1935 1951 1956 1957\n",
      " 1960 1964 1969 1970 1978 1983 1994 1998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 13:15:33.197146 11676 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 19s 12ms/sample - loss: 0.5862 - acc: 0.1642\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.5358 - acc: 0.1731\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.5236 - acc: 0.1790\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.5163 - acc: 0.1824\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.5132 - acc: 0.1835\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.5096 - acc: 0.1850\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.5056 - acc: 0.1867\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.5008 - acc: 0.1892\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4983 - acc: 0.1901\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4927 - acc: 0.1924\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4935 - acc: 0.1920\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4868 - acc: 0.1948\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4849 - acc: 0.1959\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4815 - acc: 0.1971\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4797 - acc: 0.1977\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4763 - acc: 0.19920s - loss: 0.4761 - acc: 0.199\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4705 - acc: 0.2018\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4626 - acc: 0.2046\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4552 - acc: 0.2077\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4511 - acc: 0.2088\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4460 - acc: 0.2107\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4439 - acc: 0.2115\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4389 - acc: 0.21365s - l\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4362 - acc: 0.2141\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4325 - acc: 0.2155\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4289 - acc: 0.2167\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4251 - acc: 0.2182\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4221 - acc: 0.2192\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4186 - acc: 0.2208\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4158 - acc: 0.2214\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4120 - acc: 0.2229\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4087 - acc: 0.2240\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4047 - acc: 0.2252\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4021 - acc: 0.2257\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3992 - acc: 0.2270\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3963 - acc: 0.2279\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3925 - acc: 0.2296\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3896 - acc: 0.2302\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3877 - acc: 0.2307\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3839 - acc: 0.2321\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3826 - acc: 0.2327\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3798 - acc: 0.2333\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3768 - acc: 0.2345\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3740 - acc: 0.2354\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3710 - acc: 0.2363\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3683 - acc: 0.2371\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3654 - acc: 0.2380\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3631 - acc: 0.2387\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3628 - acc: 0.2389\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3589 - acc: 0.2404\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3570 - acc: 0.2408\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3545 - acc: 0.2415\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3536 - acc: 0.2419\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3510 - acc: 0.2426\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3480 - acc: 0.2437\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3469 - acc: 0.2440\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3443 - acc: 0.2447\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3429 - acc: 0.2453\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3416 - acc: 0.2458\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3401 - acc: 0.2460\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3378 - acc: 0.2470\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3360 - acc: 0.2477\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3346 - acc: 0.2482\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3319 - acc: 0.2490\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3313 - acc: 0.2491\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3304 - acc: 0.2492\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3293 - acc: 0.2497\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3270 - acc: 0.2506\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3251 - acc: 0.2513\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3243 - acc: 0.2516\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3217 - acc: 0.2525\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3207 - acc: 0.2526\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3199 - acc: 0.2526\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3182 - acc: 0.2535\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3170 - acc: 0.2538\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3168 - acc: 0.2541\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3147 - acc: 0.2544\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3142 - acc: 0.2548\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3121 - acc: 0.2554\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3111 - acc: 0.2558\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3102 - acc: 0.2564\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3098 - acc: 0.2562\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3079 - acc: 0.2570\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3074 - acc: 0.2571\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3054 - acc: 0.2578\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3048 - acc: 0.2579\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3040 - acc: 0.2582\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3028 - acc: 0.2591\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3018 - acc: 0.2589\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3005 - acc: 0.2596\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2996 - acc: 0.2596\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2983 - acc: 0.2604\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2976 - acc: 0.2606\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2957 - acc: 0.2609\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2952 - acc: 0.2612\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2940 - acc: 0.2614\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2936 - acc: 0.2617\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2937 - acc: 0.2617\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2924 - acc: 0.2621\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2908 - acc: 0.2624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.01      1165\n",
      "           1       0.64      0.70      0.67     21658\n",
      "           2       0.34      0.26      0.29      3724\n",
      "           3       0.74      0.82      0.78     33144\n",
      "           4       0.43      0.18      0.25       685\n",
      "           5       0.48      0.53      0.51     20018\n",
      "           6       0.37      0.11      0.17      8716\n",
      "           7       0.41      0.44      0.42     11162\n",
      "\n",
      "    accuracy                           0.60    100272\n",
      "   macro avg       0.55      0.38      0.39    100272\n",
      "weighted avg       0.58      0.60      0.58    100272\n",
      "\n",
      "Acurácia\n",
      "0.3808415176149993\n",
      "Precisao\n",
      "0.5843429913694231\n",
      "Recall\n",
      "0.5984322642412637\n",
      "F1\n",
      "0.5779087520420751\n",
      "[[    3   262    34   137     0   558    40   131]\n",
      " [    0 15131   217  2152    10  3032   205   911]\n",
      " [    0   448   976   890    14   681    98   617]\n",
      " [    0  1771   462 27260    85  1810   155  1601]\n",
      " [    0    68    15   341   123    63    11    64]\n",
      " [    0  3586   458  2513    18 10658   722  2063]\n",
      " [    0  1298   259  1165     9  3360   972  1653]\n",
      " [    0  1134   478  2182    25  2014   446  4883]]\n",
      "TRAIN: [   0    1    2 ... 1996 1997 1998] TEST: [   6    7    9   11   16   17   18   22   39   49   53   65   67   68\n",
      "   69   72   74   84   91   96   99  101  107  117  121  134  135  143\n",
      "  147  149  154  163  164  174  175  206  207  209  217  222  228  229\n",
      "  241  243  250  251  258  287  295  298  301  302  303  310  311  318\n",
      "  320  324  325  328  333  334  337  342  343  346  349  358  361  372\n",
      "  373  379  383  389  398  406  418  424  426  432  436  443  444  456\n",
      "  466  471  472  473  474  476  489  503  506  511  512  517  521  527\n",
      "  530  535  539  541  542  543  558  560  562  577  584  585  586  587\n",
      "  589  592  593  602  603  605  608  610  617  621  625  632  634  640\n",
      "  647  649  655  662  674  679  683  694  696  700  703  708  713  714\n",
      "  720  731  732  740  741  742  745  746  747  758  763  764  766  773\n",
      "  781  789  792  795  804  807  810  812  814  830  833  835  840  846\n",
      "  865  874  879  888  890  896  899  922  923  926  928  932  936  943\n",
      "  945  948  956  960  964  965  987  992  994  999 1003 1004 1009 1017\n",
      " 1020 1031 1040 1055 1059 1061 1066 1067 1073 1081 1085 1087 1089 1091\n",
      " 1092 1094 1107 1109 1116 1127 1134 1138 1159 1162 1168 1171 1181 1186\n",
      " 1189 1191 1197 1199 1201 1208 1211 1212 1215 1221 1230 1231 1234 1252\n",
      " 1254 1260 1262 1263 1267 1268 1274 1281 1283 1287 1290 1299 1301 1308\n",
      " 1311 1323 1324 1326 1328 1330 1333 1336 1338 1339 1341 1345 1349 1360\n",
      " 1374 1380 1382 1386 1390 1397 1404 1411 1418 1421 1424 1435 1436 1441\n",
      " 1451 1454 1456 1460 1466 1473 1474 1478 1487 1500 1501 1506 1507 1513\n",
      " 1514 1527 1531 1535 1539 1542 1544 1559 1562 1563 1570 1578 1582 1586\n",
      " 1587 1588 1592 1609 1619 1621 1623 1624 1625 1628 1632 1638 1640 1650\n",
      " 1652 1655 1658 1659 1670 1674 1683 1689 1691 1693 1698 1718 1724 1731\n",
      " 1739 1741 1744 1747 1749 1750 1754 1755 1762 1767 1768 1769 1772 1780\n",
      " 1784 1787 1796 1799 1800 1801 1802 1806 1807 1812 1815 1817 1818 1823\n",
      " 1846 1855 1857 1865 1871 1878 1879 1887 1888 1895 1896 1905 1907 1908\n",
      " 1917 1919 1920 1926 1929 1938 1939 1942 1945 1949 1950 1952 1958 1961\n",
      " 1963 1972 1975 1977 1979 1985 1990 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 12s 7ms/sample - loss: 0.5781 - acc: 0.1635\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.5269 - acc: 0.1728\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.5145 - acc: 0.1788\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.5089 - acc: 0.1812\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.5039 - acc: 0.1833\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4995 - acc: 0.1850\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4957 - acc: 0.1866\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4915 - acc: 0.1888\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4880 - acc: 0.1903\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4869 - acc: 0.1906\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4840 - acc: 0.1921\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4802 - acc: 0.1936\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4754 - acc: 0.1958\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4760 - acc: 0.1949\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4714 - acc: 0.1976\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4668 - acc: 0.1992\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4630 - acc: 0.2013\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4575 - acc: 0.2030\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4501 - acc: 0.2058\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4464 - acc: 0.2074\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4397 - acc: 0.2098\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4351 - acc: 0.2112\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4324 - acc: 0.2125\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4313 - acc: 0.2125\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4267 - acc: 0.2143\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4237 - acc: 0.2154\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4210 - acc: 0.2158\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4178 - acc: 0.2175\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4149 - acc: 0.2185\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4105 - acc: 0.2196\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4080 - acc: 0.2204\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4061 - acc: 0.2213\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4029 - acc: 0.2225\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3988 - acc: 0.2236\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3951 - acc: 0.2250\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3919 - acc: 0.2264\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3896 - acc: 0.2269\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3870 - acc: 0.2278\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3842 - acc: 0.2285\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3798 - acc: 0.2298\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3773 - acc: 0.2307\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3742 - acc: 0.2316\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3715 - acc: 0.2328\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3694 - acc: 0.2333\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3667 - acc: 0.2338\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3630 - acc: 0.2357\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3619 - acc: 0.2358\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3591 - acc: 0.2364\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3568 - acc: 0.2375\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3542 - acc: 0.2383\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3516 - acc: 0.2392\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3489 - acc: 0.2398\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3472 - acc: 0.2404\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3464 - acc: 0.2407\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3435 - acc: 0.2415\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3417 - acc: 0.2423\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3391 - acc: 0.2433\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3383 - acc: 0.2433\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3360 - acc: 0.2440\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3342 - acc: 0.2445\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3313 - acc: 0.2454\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3299 - acc: 0.2461\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3291 - acc: 0.2461\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3259 - acc: 0.2476\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3247 - acc: 0.2477\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3233 - acc: 0.2482\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3208 - acc: 0.2488\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3196 - acc: 0.2493\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3187 - acc: 0.2496\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3168 - acc: 0.2503\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3161 - acc: 0.2504\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3142 - acc: 0.2513\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3143 - acc: 0.2510\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3129 - acc: 0.2519\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3110 - acc: 0.2521\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3099 - acc: 0.2525\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3080 - acc: 0.2532\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3064 - acc: 0.2539\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3049 - acc: 0.2544\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3039 - acc: 0.2549\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3024 - acc: 0.2549\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3017 - acc: 0.2552\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2989 - acc: 0.2563\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2996 - acc: 0.2558\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2979 - acc: 0.2566\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2980 - acc: 0.2564\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2961 - acc: 0.2572\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2936 - acc: 0.2578\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2929 - acc: 0.2584\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2928 - acc: 0.2584\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2919 - acc: 0.2587\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.2903 - acc: 0.2590\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2906 - acc: 0.2589\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2897 - acc: 0.2594\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2882 - acc: 0.2597\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2872 - acc: 0.2602\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2858 - acc: 0.2604\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.2855 - acc: 0.2607\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2840 - acc: 0.2613\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2838 - acc: 0.2613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.00      0.01      1282\n",
      "           1       0.66      0.67      0.67     21584\n",
      "           2       0.34      0.28      0.31      4272\n",
      "           3       0.74      0.82      0.78     34149\n",
      "           4       0.61      0.32      0.42       779\n",
      "           5       0.47      0.55      0.51     21054\n",
      "           6       0.38      0.11      0.18      9400\n",
      "           7       0.40      0.45      0.42     11977\n",
      "\n",
      "    accuracy                           0.59    104497\n",
      "   macro avg       0.49      0.40      0.41    104497\n",
      "weighted avg       0.58      0.59      0.58    104497\n",
      "\n",
      "Acurácia\n",
      "0.4011994586249975\n",
      "Precisao\n",
      "0.577430473407783\n",
      "Recall\n",
      "0.5941223193010325\n",
      "F1\n",
      "0.5752851601719448\n",
      "[[    6   270    55   158     1   590    45   157]\n",
      " [    2 14416   241  2105    21  3463   224  1112]\n",
      " [    2   405  1200   985     2   881   100   697]\n",
      " [    0  1351   498 28152    95  2253   151  1649]\n",
      " [    0    37     9   345   248    72     2    66]\n",
      " [    6  3250   603  2387    13 11645   780  2370]\n",
      " [    1  1104   313  1249    14  3757  1070  1892]\n",
      " [    0   934   566  2459    15  2207   449  5347]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   1    2    3 ... 1997 1998 1999] TEST: [   0    4    8   19   21   24   25   34   37   38   51   52   55   76\n",
      "   79   80   83  104  106  111  112  113  119  124  132  142  153  155\n",
      "  162  165  168  194  196  198  214  219  221  232  233  234  236  238\n",
      "  246  255  261  268  270  272  277  278  281  282  284  288  291  293\n",
      "  294  307  308  323  329  340  341  345  348  350  352  353  354  357\n",
      "  359  365  374  375  376  381  382  396  403  405  407  412  417  419\n",
      "  421  429  430  431  433  439  445  447  451  452  453  455  467  475\n",
      "  485  486  488  494  495  500  501  528  531  536  537  538  545  550\n",
      "  551  561  563  565  570  574  583  597  599  607  613  619  624  626\n",
      "  628  629  633  638  639  650  654  656  657  658  663  668  669  673\n",
      "  680  695  706  711  719  721  734  737  752  753  754  756  765  770\n",
      "  774  776  797  801  805  808  809  834  860  866  868  872  875  877\n",
      "  878  887  891  894  898  904  907  914  915  918  919  929  930  941\n",
      "  947  954  971  972  975  977  979  998 1012 1019 1025 1029 1034 1037\n",
      " 1049 1051 1063 1068 1076 1078 1079 1084 1097 1098 1102 1105 1111 1113\n",
      " 1114 1117 1124 1128 1137 1141 1143 1146 1148 1154 1158 1165 1167 1169\n",
      " 1172 1173 1176 1182 1195 1196 1205 1214 1222 1223 1224 1225 1239 1241\n",
      " 1244 1245 1247 1251 1255 1257 1258 1266 1272 1275 1293 1298 1307 1310\n",
      " 1314 1316 1317 1322 1327 1340 1342 1363 1365 1366 1369 1370 1371 1379\n",
      " 1387 1388 1403 1405 1408 1414 1416 1417 1423 1427 1428 1440 1443 1449\n",
      " 1452 1459 1477 1482 1483 1485 1490 1495 1497 1504 1515 1521 1523 1524\n",
      " 1537 1541 1545 1546 1550 1554 1555 1556 1558 1560 1561 1564 1565 1573\n",
      " 1574 1577 1583 1589 1595 1597 1598 1605 1610 1612 1614 1616 1631 1643\n",
      " 1644 1647 1651 1656 1660 1662 1663 1665 1676 1681 1682 1686 1687 1694\n",
      " 1697 1701 1709 1715 1716 1717 1723 1725 1727 1729 1733 1757 1771 1776\n",
      " 1777 1779 1783 1788 1793 1795 1811 1825 1826 1828 1830 1835 1837 1840\n",
      " 1850 1852 1853 1854 1864 1866 1872 1882 1883 1884 1890 1892 1893 1894\n",
      " 1897 1899 1901 1914 1915 1918 1922 1925 1931 1940 1946 1947 1953 1962\n",
      " 1965 1966 1973 1974 1982 1984 1988 1991]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.5778 - acc: 0.1601\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5254 - acc: 0.1697\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5128 - acc: 0.1761\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.5054 - acc: 0.1790\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5010 - acc: 0.1814\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4971 - acc: 0.1827\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4961 - acc: 0.1829\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4917 - acc: 0.1849\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4891 - acc: 0.1862\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4843 - acc: 0.1885\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4809 - acc: 0.1898\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4772 - acc: 0.1915\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4744 - acc: 0.1925\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4702 - acc: 0.1945\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4680 - acc: 0.1952\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4640 - acc: 0.1970\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4596 - acc: 0.1987\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4514 - acc: 0.2020\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4460 - acc: 0.2040\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4395 - acc: 0.2063\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4382 - acc: 0.2069\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4330 - acc: 0.2088\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4277 - acc: 0.2108\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4251 - acc: 0.2115\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4215 - acc: 0.2129\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4178 - acc: 0.2141\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4151 - acc: 0.2153\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.4110 - acc: 0.2165\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.4079 - acc: 0.2177\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4056 - acc: 0.2182\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4005 - acc: 0.2202\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3983 - acc: 0.2210\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3956 - acc: 0.2216\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3934 - acc: 0.2224\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3895 - acc: 0.2238\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3844 - acc: 0.2256\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3814 - acc: 0.2266\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3789 - acc: 0.2271\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3749 - acc: 0.2285\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3740 - acc: 0.2287\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3696 - acc: 0.2302\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3680 - acc: 0.2309\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3649 - acc: 0.2319\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3620 - acc: 0.2328\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3590 - acc: 0.2335\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3565 - acc: 0.2344\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3557 - acc: 0.2346\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3520 - acc: 0.2360\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3494 - acc: 0.2368\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3471 - acc: 0.2374\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3453 - acc: 0.2381\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3425 - acc: 0.2388\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3405 - acc: 0.2396\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3383 - acc: 0.2403\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3365 - acc: 0.2407\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3358 - acc: 0.2410\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3333 - acc: 0.2418\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3323 - acc: 0.2420\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3288 - acc: 0.2436\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3278 - acc: 0.2435\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3246 - acc: 0.2448\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3240 - acc: 0.2450\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3251 - acc: 0.2448\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3213 - acc: 0.2456\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3201 - acc: 0.2460\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3176 - acc: 0.2466\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3160 - acc: 0.2473\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3150 - acc: 0.2480\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3127 - acc: 0.2485\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3126 - acc: 0.2487\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.3099 - acc: 0.2498\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 7ms/sample - loss: 0.3099 - acc: 0.2496\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3083 - acc: 0.2501\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3060 - acc: 0.2509\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3050 - acc: 0.2512\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3039 - acc: 0.2514\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3023 - acc: 0.2522\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3024 - acc: 0.2520\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3006 - acc: 0.2526\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2996 - acc: 0.2533\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2982 - acc: 0.2533\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2969 - acc: 0.2537\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2960 - acc: 0.2543\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2941 - acc: 0.2549\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2932 - acc: 0.2550\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2926 - acc: 0.2552\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2915 - acc: 0.2557\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2904 - acc: 0.2561\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2890 - acc: 0.2564\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2886 - acc: 0.2567\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2873 - acc: 0.2569\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2862 - acc: 0.2577\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2861 - acc: 0.2575\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2851 - acc: 0.2580\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2841 - acc: 0.2582\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2831 - acc: 0.2588\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2819 - acc: 0.2589\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2810 - acc: 0.2598\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2797 - acc: 0.2595\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2799 - acc: 0.2598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.00      1373\n",
      "           1       0.64      0.68      0.66     22571\n",
      "           2       0.31      0.27      0.29      3962\n",
      "           3       0.75      0.81      0.78     35958\n",
      "           4       0.58      0.25      0.35       842\n",
      "           5       0.46      0.54      0.49     21637\n",
      "           6       0.36      0.11      0.17      9593\n",
      "           7       0.40      0.42      0.41     12135\n",
      "\n",
      "    accuracy                           0.59    108071\n",
      "   macro avg       0.50      0.39      0.39    108071\n",
      "weighted avg       0.57      0.59      0.57    108071\n",
      "\n",
      "Acurácia\n",
      "0.3854811009932829\n",
      "Precisao\n",
      "0.5719754768091974\n",
      "Recall\n",
      "0.5876692174588928\n",
      "F1\n",
      "0.5689934762513187\n",
      "[[    2   313    44   167     1   626    48   172]\n",
      " [    1 15416   293  2108    21  3519   238   975]\n",
      " [    0   460  1073   907     6   772    79   665]\n",
      " [    0  1793   652 28995    83  2467   199  1769]\n",
      " [    0    39    11   421   213    78     8    72]\n",
      " [    1  3743   565  2557    15 11639   810  2307]\n",
      " [    0  1399   315  1285     9  3814  1040  1731]\n",
      " [    0  1097   528  2308    21  2561   488  5132]]\n",
      "TRAIN: [   0    3    4 ... 1996 1998 1999] TEST: [   1    2   26   32   35   41   45   46   50   58   59   64   89   92\n",
      "  102  114  116  128  130  133  137  140  156  166  176  178  181  182\n",
      "  183  186  192  197  199  201  202  203  210  216  220  225  231  235\n",
      "  240  244  245  247  249  256  267  269  275  280  299  304  309  316\n",
      "  319  321  326  327  332  335  338  362  370  377  380  384  387  393\n",
      "  395  402  408  410  413  425  427  460  463  464  465  477  478  480\n",
      "  484  490  491  492  493  496  499  504  505  514  516  519  524  525\n",
      "  544  546  547  548  549  554  566  571  572  588  604  609  612  614\n",
      "  615  627  631  641  645  659  667  671  678  681  691  697  698  699\n",
      "  701  704  709  712  718  722  744  749  751  755  760  761  768  769\n",
      "  771  779  787  790  798  802  803  811  820  822  828  831  838  843\n",
      "  849  850  853  857  858  859  862  864  867  870  871  873  880  881\n",
      "  884  885  893  909  910  913  917  927  931  933  940  946  958  969\n",
      "  970  978  981  983  985  988  990  993  995 1000 1001 1005 1008 1010\n",
      " 1015 1021 1024 1028 1030 1032 1041 1046 1047 1048 1069 1072 1074 1075\n",
      " 1077 1082 1088 1093 1100 1103 1110 1115 1122 1123 1125 1130 1131 1132\n",
      " 1133 1140 1142 1145 1150 1156 1157 1160 1166 1174 1177 1193 1200 1207\n",
      " 1210 1217 1218 1219 1226 1228 1232 1240 1243 1248 1261 1265 1271 1273\n",
      " 1276 1277 1284 1285 1288 1297 1300 1305 1319 1321 1331 1344 1346 1347\n",
      " 1350 1353 1355 1357 1364 1368 1373 1383 1393 1395 1396 1400 1401 1402\n",
      " 1412 1420 1426 1433 1434 1439 1444 1448 1450 1461 1470 1484 1486 1488\n",
      " 1491 1493 1494 1498 1499 1502 1503 1509 1511 1518 1520 1522 1525 1534\n",
      " 1557 1572 1580 1584 1591 1594 1599 1600 1602 1603 1608 1613 1618 1626\n",
      " 1629 1634 1641 1646 1649 1654 1661 1667 1668 1673 1677 1678 1679 1680\n",
      " 1685 1690 1692 1700 1704 1708 1711 1712 1713 1726 1728 1732 1735 1738\n",
      " 1740 1748 1753 1758 1763 1765 1773 1774 1775 1786 1790 1797 1803 1813\n",
      " 1816 1820 1839 1844 1847 1848 1849 1858 1861 1863 1868 1869 1873 1874\n",
      " 1889 1898 1902 1904 1906 1911 1913 1921 1930 1932 1933 1936 1941 1948\n",
      " 1954 1959 1968 1980 1986 1987 1992 1997]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.5883 - acc: 0.1612\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5330 - acc: 0.1725\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5208 - acc: 0.1791\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5131 - acc: 0.1825\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5097 - acc: 0.1835\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5055 - acc: 0.1854\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5031 - acc: 0.1865\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4972 - acc: 0.1889\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4933 - acc: 0.1910\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4915 - acc: 0.1916\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4885 - acc: 0.1931\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4842 - acc: 0.1948\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4850 - acc: 0.1943\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4802 - acc: 0.1965\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4743 - acc: 0.1989\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4729 - acc: 0.1995\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4700 - acc: 0.2007\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4661 - acc: 0.2020\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4593 - acc: 0.2048\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4570 - acc: 0.2056\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4459 - acc: 0.2099\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4422 - acc: 0.2114\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4376 - acc: 0.2133\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4344 - acc: 0.2142\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4327 - acc: 0.2148\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4271 - acc: 0.2167\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4236 - acc: 0.2182\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4196 - acc: 0.2192\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4180 - acc: 0.2199\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4148 - acc: 0.2210\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4099 - acc: 0.2228\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4070 - acc: 0.2237\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4026 - acc: 0.2249\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3997 - acc: 0.2260\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3972 - acc: 0.2267\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3944 - acc: 0.2276\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3914 - acc: 0.2289\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3879 - acc: 0.2297\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3853 - acc: 0.2303\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3830 - acc: 0.2314\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3797 - acc: 0.2326\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3774 - acc: 0.2333\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3754 - acc: 0.2337\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3731 - acc: 0.2344\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3701 - acc: 0.2353\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3688 - acc: 0.2360\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3654 - acc: 0.2368\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3613 - acc: 0.2381\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3599 - acc: 0.2387\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3579 - acc: 0.2395\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3557 - acc: 0.2399\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3532 - acc: 0.2408\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3506 - acc: 0.2414\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3506 - acc: 0.2419\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3478 - acc: 0.2427\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3455 - acc: 0.2434\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3440 - acc: 0.2440\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3417 - acc: 0.2445\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3399 - acc: 0.2452\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3392 - acc: 0.2457\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3370 - acc: 0.2460\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3351 - acc: 0.2468\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3354 - acc: 0.2464\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3334 - acc: 0.2473\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3294 - acc: 0.2485\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3277 - acc: 0.2492\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3262 - acc: 0.2496\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3252 - acc: 0.2500\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3239 - acc: 0.2505\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3220 - acc: 0.2507\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3209 - acc: 0.2511\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3198 - acc: 0.2519\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3184 - acc: 0.2521\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3172 - acc: 0.2527\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3158 - acc: 0.2527\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3155 - acc: 0.2532\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3136 - acc: 0.2538\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3123 - acc: 0.2542\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3115 - acc: 0.2540\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3106 - acc: 0.2550\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3088 - acc: 0.2552\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3075 - acc: 0.2559\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3074 - acc: 0.2556\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3063 - acc: 0.2561\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3056 - acc: 0.2564\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3030 - acc: 0.2572\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3022 - acc: 0.2574\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3017 - acc: 0.2577\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2995 - acc: 0.2584\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3009 - acc: 0.2580\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2983 - acc: 0.2587\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2981 - acc: 0.2590\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2966 - acc: 0.2594\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2954 - acc: 0.2597\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2951 - acc: 0.2599\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2941 - acc: 0.2601\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2934 - acc: 0.2607\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2930 - acc: 0.2604\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2910 - acc: 0.2612\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2895 - acc: 0.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1207\n",
      "           1       0.64      0.71      0.67     22248\n",
      "           2       0.47      0.31      0.37      4192\n",
      "           3       0.76      0.82      0.79     32085\n",
      "           4       0.60      0.28      0.38       624\n",
      "           5       0.48      0.56      0.51     20814\n",
      "           6       0.38      0.13      0.20      9138\n",
      "           7       0.41      0.43      0.42     11383\n",
      "\n",
      "    accuracy                           0.60    101691\n",
      "   macro avg       0.47      0.40      0.42    101691\n",
      "weighted avg       0.58      0.60      0.58    101691\n",
      "\n",
      "Acurácia\n",
      "0.40446799989857135\n",
      "Precisao\n",
      "0.5799384885957153\n",
      "Recall\n",
      "0.6010659743733467\n",
      "F1\n",
      "0.5828036386427987\n",
      "[[    0   297    20   113     2   597    62   116]\n",
      " [    0 15701   142  1873     6  3312   276   938]\n",
      " [    0   464  1306   842     2   811   105   662]\n",
      " [    0  1789   370 26185    63  2029   183  1466]\n",
      " [    0    69     4   279   173    55     4    40]\n",
      " [    0  3761   360  2107    12 11590   854  2130]\n",
      " [    0  1352   201  1103     9  3604  1224  1645]\n",
      " [    0  1084   376  2082    20  2364   513  4944]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [  10   12   14   15   20   23   27   28   30   33   40   43   44   48\n",
      "   56   57   60   62   63   66   73   85   90   95   97  103  108  109\n",
      "  110  115  120  122  123  127  129  136  138  139  141  145  150  152\n",
      "  157  158  161  167  169  170  172  179  185  187  188  193  195  200\n",
      "  204  213  223  227  237  239  259  262  263  266  271  273  276  279\n",
      "  283  285  286  289  305  312  313  315  317  322  330  331  336  339\n",
      "  347  351  355  356  360  364  368  369  371  378  385  386  391  394\n",
      "  400  401  434  437  438  442  449  454  458  461  462  468  481  483\n",
      "  498  502  508  513  518  523  529  534  552  557  564  567  568  569\n",
      "  575  576  578  580  582  591  594  595  600  606  611  620  623  630\n",
      "  635  637  642  643  644  646  651  660  665  666  670  672  676  685\n",
      "  686  688  689  690  702  705  707  710  715  716  717  723  724  726\n",
      "  727  728  730  743  750  767  780  782  784  785  788  791  793  794\n",
      "  796  813  815  818  823  824  826  827  829  832  837  839  844  845\n",
      "  852  861  876  883  889  892  903  906  912  924  925  934  935  937\n",
      "  938  942  944  949  950  952  953  955  959  962  963  966  968  974\n",
      "  984  991  996  997 1007 1018 1027 1033 1036 1039 1044 1045 1050 1052\n",
      " 1053 1054 1060 1064 1071 1080 1090 1095 1106 1108 1129 1135 1136 1139\n",
      " 1144 1153 1163 1164 1170 1179 1180 1184 1185 1198 1204 1209 1216 1227\n",
      " 1229 1235 1238 1242 1246 1249 1253 1256 1264 1278 1279 1289 1302 1303\n",
      " 1312 1313 1318 1320 1325 1334 1352 1356 1361 1362 1367 1377 1378 1384\n",
      " 1389 1391 1392 1394 1398 1399 1406 1407 1410 1413 1415 1422 1431 1437\n",
      " 1438 1442 1455 1457 1463 1464 1468 1469 1472 1475 1476 1479 1480 1496\n",
      " 1505 1508 1516 1519 1528 1529 1536 1547 1548 1551 1567 1571 1576 1581\n",
      " 1593 1596 1606 1611 1615 1622 1630 1645 1653 1664 1669 1672 1675 1684\n",
      " 1703 1706 1714 1720 1736 1737 1742 1743 1752 1759 1760 1761 1764 1766\n",
      " 1770 1778 1781 1789 1791 1804 1809 1814 1824 1831 1832 1838 1842 1856\n",
      " 1859 1867 1875 1881 1885 1886 1891 1910 1916 1923 1937 1943 1944 1955\n",
      " 1967 1971 1976 1981 1989 1993 1995 1996]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 11s 7ms/sample - loss: 0.5845 - acc: 0.1640\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5341 - acc: 0.1728\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5211 - acc: 0.1791\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5136 - acc: 0.1823\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5094 - acc: 0.1841\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5063 - acc: 0.1853\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5024 - acc: 0.1871\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5005 - acc: 0.1878\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4932 - acc: 0.1913\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4895 - acc: 0.1929\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4914 - acc: 0.1924\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4855 - acc: 0.1946\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4831 - acc: 0.1958\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4828 - acc: 0.1956\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4759 - acc: 0.1988\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4738 - acc: 0.1997\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4706 - acc: 0.2008\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4658 - acc: 0.2028\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4616 - acc: 0.2044\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4547 - acc: 0.2071\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4486 - acc: 0.2091\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4434 - acc: 0.2111\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4412 - acc: 0.2118\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4360 - acc: 0.2139\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4344 - acc: 0.2143\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4293 - acc: 0.2159\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4254 - acc: 0.2177\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4212 - acc: 0.2189\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4180 - acc: 0.2199\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4169 - acc: 0.2203\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4116 - acc: 0.2222\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4078 - acc: 0.2235\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4045 - acc: 0.2246\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.4046 - acc: 0.2239\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3997 - acc: 0.2258\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3958 - acc: 0.2273\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3928 - acc: 0.2284\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3891 - acc: 0.2294\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3863 - acc: 0.2304\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3830 - acc: 0.2313\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3814 - acc: 0.2319\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3784 - acc: 0.2327\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3752 - acc: 0.2340\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3720 - acc: 0.2349\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3697 - acc: 0.2355\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3675 - acc: 0.2363\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3660 - acc: 0.2369\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3628 - acc: 0.2381\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3596 - acc: 0.2388\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3583 - acc: 0.2392\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3551 - acc: 0.2404\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3530 - acc: 0.2410\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3513 - acc: 0.2414\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3499 - acc: 0.2422\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3477 - acc: 0.2425\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3467 - acc: 0.2431\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3428 - acc: 0.2439\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3416 - acc: 0.2446\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3407 - acc: 0.2449\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3389 - acc: 0.2453\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3361 - acc: 0.2463\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3332 - acc: 0.2475\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3332 - acc: 0.2472\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3306 - acc: 0.2484\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3292 - acc: 0.2489\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3270 - acc: 0.2495\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3265 - acc: 0.2495\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3241 - acc: 0.2502\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3226 - acc: 0.2506\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3214 - acc: 0.2510\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3206 - acc: 0.2514\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3190 - acc: 0.2519\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3180 - acc: 0.2522\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3167 - acc: 0.2525\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3142 - acc: 0.2538\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3143 - acc: 0.2536\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3124 - acc: 0.2541\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3113 - acc: 0.2545\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3095 - acc: 0.2551\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3078 - acc: 0.2558\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3071 - acc: 0.2562\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3057 - acc: 0.2564\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3053 - acc: 0.2566\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3047 - acc: 0.2565\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3035 - acc: 0.2574\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3018 - acc: 0.2578\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3017 - acc: 0.2580\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.3002 - acc: 0.2583\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2973 - acc: 0.2593\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2974 - acc: 0.2593\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2959 - acc: 0.2599\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2958 - acc: 0.2597\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2958 - acc: 0.2596\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2944 - acc: 0.2603\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2924 - acc: 0.2609\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2932 - acc: 0.2605\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2911 - acc: 0.2613\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2900 - acc: 0.2617\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2889 - acc: 0.2621\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.2894 - acc: 0.2619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.01      1237\n",
      "           1       0.63      0.70      0.67     20871\n",
      "           2       0.34      0.27      0.30      3721\n",
      "           3       0.74      0.82      0.78     33807\n",
      "           4       0.50      0.27      0.35       587\n",
      "           5       0.48      0.55      0.51     20562\n",
      "           6       0.37      0.12      0.18      8741\n",
      "           7       0.43      0.42      0.42     11751\n",
      "\n",
      "    accuracy                           0.60    101277\n",
      "   macro avg       0.50      0.39      0.40    101277\n",
      "weighted avg       0.58      0.60      0.58    101277\n",
      "\n",
      "Acurácia\n",
      "0.39296823991235785\n",
      "Precisao\n",
      "0.578623710713331\n",
      "Recall\n",
      "0.5986255517047306\n",
      "F1\n",
      "0.5780177462533034\n",
      "[[    6   298    42   173     0   557    43   118]\n",
      " [    2 14653   232  2195    11  2726   207   845]\n",
      " [    0   452   990   889     1   719    71   599]\n",
      " [    0  1827   447 27643   102  2162   193  1433]\n",
      " [    0    50    25   264   160    40     4    44]\n",
      " [    1  3539   482  2530    10 11263   696  2041]\n",
      " [    1  1193   269  1154     7  3608  1012  1497]\n",
      " [    2  1163   444  2518    30  2210   484  4900]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_16 (Bidirectio (None, 700, 200)          73200     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 700, 200)          181200    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 618,408\n",
      "Trainable params: 618,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácias total\n",
      "[0.3808415176149993, 0.4011994586249975, 0.3854811009932829, 0.40446799989857135, 0.39296823991235785]\n",
      "0.3929916634088418\n",
      "Precision total\n",
      "[0.5843429913694231, 0.577430473407783, 0.5719754768091974, 0.5799384885957153, 0.578623710713331]\n",
      "0.5784622281790901\n",
      "Recalls total\n",
      "[0.5984322642412637, 0.5941223193010325, 0.5876692174588928, 0.6010659743733467, 0.5986255517047306]\n",
      "0.5959830654158532\n",
      "F1 total\n",
      "[0.5779087520420751, 0.5752851601719448, 0.5689934762513187, 0.5828036386427987, 0.5780177462533034]\n",
      "0.5766017546722881\n"
     ]
    }
   ],
   "source": [
    "print('Acurácias total')\n",
    "print(accuq8)\n",
    "a = np.array(accuq8)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisionsq8)\n",
    "p = np.array(precisionsq8)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recallsq8)\n",
    "r = np.array(recallsq8)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1q8)\n",
    "f = np.array(f1q8)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
