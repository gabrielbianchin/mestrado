{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNLSTM(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 11:08:02.610097 11104 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 11:08:02.616066 11104 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 11:08:02.617049 11104 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0818 11:08:02.618046 11104 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [  11   21   35   49   53   56   57   79   82   86   99  107  111  114\n",
      "  115  118  119  133  138  139  143  151  158  160  161  163  165  170\n",
      "  171  174  186  188  189  191  202  208  211  215  218  220  222  224\n",
      "  225  227  229  230  232  253  254  258  268  270  278  281  284  287\n",
      "  292  294  306  311  319  323  325  331  333  337  340  343  346  374\n",
      "  379  384  400  403  406  407  412  413  416  434  435  437  442  446\n",
      "  448  452  462  463  467  470  475  479  482  485  488  496  502  506\n",
      "  509  514  518  519  520  526  531  542  547  562  563  567  570  573\n",
      "  577  593  595  599  606  613  615  622  623  624  628  630  633  635\n",
      "  643  645  654  657  658  668  674  676  682  684  686  691  696  701\n",
      "  705  709  711  712  730  735  739  743  744  748  749  751  756  758\n",
      "  760  772  778  780  781  786  789  790  794  802  804  809  813  815\n",
      "  818  821  822  824  825  826  846  848  853  855  857  859  867  872\n",
      "  883  885  886  891  892  897  904  914  917  926  928  930  941  945\n",
      "  969  971  975  981  987  989  991  992 1002 1003 1005 1010 1014 1017\n",
      " 1018 1023 1031 1033 1034 1045 1053 1063 1064 1066 1068 1069 1084 1092\n",
      " 1101 1102 1108 1111 1119 1121 1123 1124 1129 1132 1141 1144 1148 1149\n",
      " 1158 1161 1167 1189 1192 1200 1204 1216 1221 1224 1248 1250 1255 1257\n",
      " 1262 1265 1268 1272 1274 1284 1286 1294 1295 1298 1300 1302 1306 1308\n",
      " 1318 1325 1327 1329 1333 1334 1336 1338 1340 1342 1344 1347 1355 1362\n",
      " 1364 1376 1377 1378 1383 1384 1390 1399 1402 1404 1410 1411 1414 1421\n",
      " 1426 1433 1445 1459 1460 1463 1469 1472 1475 1497 1499 1500 1510 1520\n",
      " 1522 1523 1532 1538 1540 1545 1551 1562 1564 1565 1572 1580 1583 1587\n",
      " 1589 1613 1625 1626 1635 1637 1643 1644 1646 1654 1657 1661 1680 1681\n",
      " 1692 1704 1705 1714 1717 1720 1726 1735 1738 1742 1747 1754 1758 1765\n",
      " 1769 1773 1776 1779 1780 1781 1791 1794 1797 1802 1805 1806 1819 1822\n",
      " 1824 1835 1847 1868 1871 1873 1874 1877 1878 1881 1887 1888 1892 1894\n",
      " 1897 1902 1904 1909 1917 1925 1931 1939 1940 1941 1942 1944 1946 1947\n",
      " 1962 1975 1976 1977 1987 1990 1991 1995]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 11:08:03.419931 11104 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.6211 - acc: 0.1400\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5564 - acc: 0.1610\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5296 - acc: 0.1715\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5190 - acc: 0.1762\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5129 - acc: 0.1796\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5093 - acc: 0.1815\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5027 - acc: 0.1845\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4968 - acc: 0.1870\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4930 - acc: 0.1884\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4918 - acc: 0.1893\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4883 - acc: 0.1902\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4857 - acc: 0.1918\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4827 - acc: 0.1928\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4835 - acc: 0.1924\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4784 - acc: 0.1944\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4765 - acc: 0.1953\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4739 - acc: 0.1963\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4708 - acc: 0.1974\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4686 - acc: 0.1983\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4657 - acc: 0.1996\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4642 - acc: 0.2001\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4605 - acc: 0.2013\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4572 - acc: 0.2026\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4545 - acc: 0.2037\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4547 - acc: 0.2037\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4520 - acc: 0.2044\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4463 - acc: 0.2067\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4443 - acc: 0.2073\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4413 - acc: 0.2084\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4384 - acc: 0.2094\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4328 - acc: 0.2115\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4296 - acc: 0.2124\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4297 - acc: 0.2126\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4239 - acc: 0.2147\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4216 - acc: 0.2155\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4170 - acc: 0.2169\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4142 - acc: 0.2178\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4100 - acc: 0.2195\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4073 - acc: 0.2201\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4036 - acc: 0.2215\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3997 - acc: 0.2230\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3976 - acc: 0.2238\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3970 - acc: 0.2237\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3919 - acc: 0.2253\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3879 - acc: 0.2264\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3856 - acc: 0.2275\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3820 - acc: 0.2285\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3815 - acc: 0.2284\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3820 - acc: 0.2284\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3749 - acc: 0.2309\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3722 - acc: 0.2314\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3704 - acc: 0.2323\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3677 - acc: 0.2331\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3645 - acc: 0.2340\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3627 - acc: 0.2349\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3594 - acc: 0.2359\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3592 - acc: 0.2360\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3551 - acc: 0.2370\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3559 - acc: 0.2367\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3515 - acc: 0.2381\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3492 - acc: 0.2391\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3485 - acc: 0.2389\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3450 - acc: 0.2403\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3420 - acc: 0.2410\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3423 - acc: 0.2412\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3415 - acc: 0.2414\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3383 - acc: 0.2424\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3372 - acc: 0.2428\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3343 - acc: 0.2438\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3336 - acc: 0.2436\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3340 - acc: 0.2437\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3310 - acc: 0.2447\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3276 - acc: 0.2459\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3269 - acc: 0.2460\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3250 - acc: 0.2466\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3248 - acc: 0.2466\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3240 - acc: 0.2473\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3295 - acc: 0.2451\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3226 - acc: 0.2475\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3199 - acc: 0.2485\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3179 - acc: 0.2490\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3152 - acc: 0.2498\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3152 - acc: 0.2498\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3133 - acc: 0.2507\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3109 - acc: 0.2514\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3101 - acc: 0.2517\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3098 - acc: 0.2516\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3107 - acc: 0.2514\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3145 - acc: 0.2501\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3081 - acc: 0.2525\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3059 - acc: 0.2532\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3041 - acc: 0.2534\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3053 - acc: 0.2532\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3034 - acc: 0.2538\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3005 - acc: 0.2550\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.2993 - acc: 0.2552\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.2997 - acc: 0.2549\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.2992 - acc: 0.2552\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.2971 - acc: 0.2562\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.2958 - acc: 0.2563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1276\n",
      "           1       0.59      0.63      0.61     21758\n",
      "           2       0.29      0.19      0.23      4008\n",
      "           3       0.70      0.73      0.71     34720\n",
      "           4       0.46      0.19      0.27       801\n",
      "           5       0.40      0.55      0.46     20862\n",
      "           6       0.33      0.06      0.11      9294\n",
      "           7       0.36      0.36      0.36     11763\n",
      "\n",
      "    accuracy                           0.54    104482\n",
      "   macro avg       0.39      0.34      0.34    104482\n",
      "weighted avg       0.52      0.54      0.52    104482\n",
      "\n",
      "Acur√°cia\n",
      "0.3371989476133228\n",
      "Precisao\n",
      "0.5182006635183862\n",
      "Recall\n",
      "0.536523037460998\n",
      "F1\n",
      "0.5158061111484329\n",
      "[[    0   297    27   190     1   608    31   122]\n",
      " [    0 13698   257  2755    13  3912   139   984]\n",
      " [    0   487   743   920     6  1157    73   622]\n",
      " [    0  2836   483 25238   122  3858   136  2047]\n",
      " [    0    74    15   399   150    84     7    72]\n",
      " [    0  3416   453  3031     9 11446   466  2041]\n",
      " [    0  1356   211  1302    13  4296   589  1527]\n",
      " [    0  1236   357  2374    13  3250   340  4193]]\n",
      "TRAIN: [   0    4    5 ... 1996 1997 1999] TEST: [   1    2    3    7    8   10   12   20   25   37   38   43   46   55\n",
      "   59   64   83   87   94   96  100  101  102  104  126  128  129  136\n",
      "  144  145  147  154  164  172  173  184  194  195  197  200  204  212\n",
      "  213  216  217  221  233  238  245  246  260  261  264  271  276  280\n",
      "  296  305  307  314  317  318  329  338  349  352  360  363  364  369\n",
      "  370  375  386  389  393  396  398  401  402  405  409  414  415  424\n",
      "  427  429  432  436  438  447  451  453  460  469  473  486  491  497\n",
      "  499  501  508  512  517  523  527  532  535  544  548  551  552  561\n",
      "  568  569  571  572  580  581  588  589  594  596  597  601  602  603\n",
      "  608  610  611  612  617  618  621  625  637  644  646  650  655  661\n",
      "  665  667  675  694  697  698  700  703  720  723  724  728  736  741\n",
      "  745  757  759  766  767  773  784  791  799  805  806  823  830  836\n",
      "  838  845  847  849  854  864  868  869  871  876  877  879  888  890\n",
      "  893  899  907  913  915  918  920  927  931  937  946  951  954  955\n",
      "  956  959  962  963  966  982  984  985 1001 1019 1020 1021 1025 1030\n",
      " 1032 1041 1043 1047 1052 1055 1058 1059 1073 1079 1088 1091 1094 1097\n",
      " 1099 1100 1103 1104 1107 1110 1113 1120 1142 1150 1151 1157 1160 1168\n",
      " 1171 1177 1178 1186 1197 1198 1202 1203 1205 1206 1213 1220 1222 1229\n",
      " 1235 1236 1242 1247 1249 1258 1266 1270 1277 1280 1285 1287 1291 1299\n",
      " 1301 1312 1323 1328 1332 1348 1359 1366 1371 1375 1393 1403 1405 1408\n",
      " 1418 1430 1434 1440 1442 1443 1446 1451 1453 1455 1457 1462 1468 1476\n",
      " 1477 1478 1490 1495 1501 1516 1519 1521 1530 1531 1542 1544 1547 1548\n",
      " 1550 1557 1559 1567 1570 1576 1578 1585 1586 1592 1601 1605 1607 1608\n",
      " 1614 1616 1620 1630 1631 1640 1648 1649 1650 1655 1658 1670 1671 1677\n",
      " 1683 1688 1696 1711 1712 1718 1719 1727 1743 1746 1748 1756 1757 1759\n",
      " 1761 1766 1772 1788 1790 1811 1812 1820 1827 1832 1833 1838 1849 1850\n",
      " 1856 1862 1864 1867 1880 1883 1886 1889 1890 1891 1899 1903 1906 1910\n",
      " 1919 1923 1930 1935 1937 1948 1950 1953 1954 1955 1965 1966 1967 1970\n",
      " 1972 1974 1980 1983 1985 1988 1992 1998]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.6166 - acc: 0.1410\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5555 - acc: 0.1602\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5291 - acc: 0.1705\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5178 - acc: 0.1759\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5093 - acc: 0.1801\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5074 - acc: 0.1810\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5006 - acc: 0.1840\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4957 - acc: 0.1861\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4942 - acc: 0.1866\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4901 - acc: 0.1884\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4892 - acc: 0.1887\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4866 - acc: 0.1895\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4849 - acc: 0.1900\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4835 - acc: 0.1907\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4795 - acc: 0.1927\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4768 - acc: 0.1934\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4727 - acc: 0.1952\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4710 - acc: 0.1961\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4691 - acc: 0.1966\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4658 - acc: 0.1980\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4632 - acc: 0.1989\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4618 - acc: 0.1994\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4597 - acc: 0.2004\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4556 - acc: 0.2021\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4520 - acc: 0.2034\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4501 - acc: 0.2043\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4469 - acc: 0.2051\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4443 - acc: 0.2061\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4441 - acc: 0.2060\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4385 - acc: 0.2083\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4339 - acc: 0.2099\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4355 - acc: 0.2091\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4298 - acc: 0.2111\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4266 - acc: 0.2124\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4308 - acc: 0.2110\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4239 - acc: 0.2133\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4208 - acc: 0.2145\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4189 - acc: 0.2149\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4106 - acc: 0.2178\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4110 - acc: 0.2176\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4089 - acc: 0.2185\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4054 - acc: 0.2197\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3984 - acc: 0.2219\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3949 - acc: 0.2231\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3904 - acc: 0.2246\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3888 - acc: 0.2251\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3867 - acc: 0.2255\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3851 - acc: 0.2262\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3808 - acc: 0.2275\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3796 - acc: 0.2281\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3735 - acc: 0.2301\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3723 - acc: 0.2304\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3709 - acc: 0.2306\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3683 - acc: 0.2318\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3646 - acc: 0.2329\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3628 - acc: 0.2333\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3614 - acc: 0.2337\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3578 - acc: 0.2349\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3556 - acc: 0.2356\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3539 - acc: 0.2361\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3503 - acc: 0.2373\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3491 - acc: 0.2379\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3471 - acc: 0.2384\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3452 - acc: 0.2388\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3451 - acc: 0.2386\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3415 - acc: 0.2402\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3389 - acc: 0.2408\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3384 - acc: 0.2412\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3357 - acc: 0.2417\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3332 - acc: 0.2424\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3312 - acc: 0.2433\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3304 - acc: 0.2435\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3282 - acc: 0.2442\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3260 - acc: 0.2451\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3277 - acc: 0.2444\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3241 - acc: 0.2455\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3216 - acc: 0.2467\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3205 - acc: 0.2470\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3185 - acc: 0.2478\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3178 - acc: 0.2478\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3161 - acc: 0.2483\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3149 - acc: 0.2489\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3132 - acc: 0.2492\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3117 - acc: 0.2499\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3120 - acc: 0.2496\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3079 - acc: 0.2510\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3136 - acc: 0.2491\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3073 - acc: 0.2511\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3045 - acc: 0.2521\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3036 - acc: 0.2523\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3024 - acc: 0.2530\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3010 - acc: 0.2531\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3006 - acc: 0.2531\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.2995 - acc: 0.2538\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.2993 - acc: 0.2540\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3006 - acc: 0.2535\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.2996 - acc: 0.2538\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.2959 - acc: 0.2550\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.2937 - acc: 0.2558\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.2921 - acc: 0.2562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.00      1348\n",
      "           1       0.60      0.66      0.63     23024\n",
      "           2       0.27      0.21      0.24      3893\n",
      "           3       0.70      0.76      0.73     34125\n",
      "           4       0.41      0.21      0.27       673\n",
      "           5       0.43      0.52      0.47     21408\n",
      "           6       0.34      0.08      0.13      9225\n",
      "           7       0.39      0.38      0.38     12049\n",
      "\n",
      "    accuracy                           0.55    105745\n",
      "   macro avg       0.45      0.35      0.36    105745\n",
      "weighted avg       0.54      0.55      0.53    105745\n",
      "\n",
      "Acur√°cia\n",
      "0.3529153921064021\n",
      "Precisao\n",
      "0.5362492531929546\n",
      "Recall\n",
      "0.5545699560262897\n",
      "F1\n",
      "0.5336454678925882\n",
      "[[    1   336    32   216     3   600    31   129]\n",
      " [    0 15141   277  2740    18  3653   206   989]\n",
      " [    0   487   830   947     5   930    73   621]\n",
      " [    0  2647   576 26033   120  2909   171  1669]\n",
      " [    0    81    16   331   138    53     2    52]\n",
      " [    0  3953   533  3021    28 11166   609  2098]\n",
      " [    0  1365   274  1370    10  3756   770  1680]\n",
      " [    1  1341   540  2555    18  2613   417  4564]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [  13   14   15   19   23   24   26   27   31   32   33   34   39   44\n",
      "   51   54   60   62   63   65   66   69   71   73   75   76   77   84\n",
      "   88   90   95  105  108  124  130  131  132  148  152  156  169  177\n",
      "  180  193  206  214  219  226  242  248  252  256  257  263  269  272\n",
      "  273  275  283  285  288  291  293  299  300  302  303  315  316  320\n",
      "  328  336  341  342  345  350  355  358  359  361  372  373  376  378\n",
      "  381  385  387  388  390  395  397  411  419  421  425  431  440  444\n",
      "  445  449  458  461  464  468  471  474  478  483  493  494  495  498\n",
      "  505  507  511  515  521  530  536  553  564  574  578  586  587  598\n",
      "  626  627  629  634  638  640  652  659  660  664  673  679  680  687\n",
      "  690  692  695  699  704  706  707  710  716  722  727  729  732  737\n",
      "  740  742  746  747  755  762  765  768  771  797  807  812  816  817\n",
      "  829  832  834  844  862  887  894  898  900  902  903  906  908  911\n",
      "  912  922  925  952  953  958  960  964  967  983  986  993  994  995\n",
      "  997  999 1008 1009 1016 1022 1024 1026 1037 1038 1044 1046 1054 1056\n",
      " 1065 1071 1072 1078 1081 1082 1085 1098 1109 1133 1140 1154 1156 1159\n",
      " 1165 1174 1176 1181 1182 1183 1184 1185 1187 1188 1190 1194 1195 1201\n",
      " 1208 1217 1223 1227 1228 1237 1239 1240 1246 1259 1260 1263 1279 1282\n",
      " 1283 1289 1292 1297 1303 1307 1310 1313 1314 1319 1321 1326 1346 1350\n",
      " 1351 1353 1360 1361 1367 1369 1372 1379 1387 1391 1394 1395 1396 1401\n",
      " 1413 1416 1417 1429 1431 1432 1435 1438 1439 1441 1448 1449 1454 1456\n",
      " 1466 1479 1481 1485 1488 1491 1494 1498 1507 1509 1515 1517 1528 1533\n",
      " 1536 1537 1539 1543 1549 1555 1556 1558 1560 1582 1584 1591 1594 1595\n",
      " 1598 1602 1603 1612 1632 1633 1638 1641 1656 1659 1663 1666 1675 1676\n",
      " 1687 1689 1691 1698 1701 1702 1707 1708 1709 1710 1713 1723 1731 1732\n",
      " 1736 1740 1745 1749 1751 1762 1775 1778 1787 1789 1795 1798 1800 1804\n",
      " 1807 1810 1813 1829 1837 1845 1851 1854 1857 1858 1859 1860 1861 1879\n",
      " 1882 1884 1885 1898 1900 1905 1908 1911 1915 1916 1921 1924 1927 1929\n",
      " 1932 1945 1958 1959 1973 1979 1981 1994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.6214 - acc: 0.1441\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5667 - acc: 0.1606\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5377 - acc: 0.1710\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5252 - acc: 0.1760\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5185 - acc: 0.1794\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5117 - acc: 0.1828\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5098 - acc: 0.1832\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5038 - acc: 0.1861\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4979 - acc: 0.1889\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4979 - acc: 0.1888\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4918 - acc: 0.1915\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4902 - acc: 0.1916\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4896 - acc: 0.1919\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4870 - acc: 0.1932\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4825 - acc: 0.1950\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4826 - acc: 0.1950\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4802 - acc: 0.1957\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4777 - acc: 0.1968\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4773 - acc: 0.1967\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4731 - acc: 0.1985\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4735 - acc: 0.1979\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4703 - acc: 0.1996\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4657 - acc: 0.2014\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4665 - acc: 0.2011\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4614 - acc: 0.2029\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4603 - acc: 0.2034\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4563 - acc: 0.2051\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4560 - acc: 0.2050\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4510 - acc: 0.2066\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4460 - acc: 0.2088\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4453 - acc: 0.2088\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4430 - acc: 0.2096\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4404 - acc: 0.2105\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4392 - acc: 0.2111\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4341 - acc: 0.2130\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4308 - acc: 0.2137\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4275 - acc: 0.2153\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4234 - acc: 0.2168\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4219 - acc: 0.2170\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4175 - acc: 0.2188\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4150 - acc: 0.2195\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4113 - acc: 0.2205\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4087 - acc: 0.2215\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4045 - acc: 0.2230\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4027 - acc: 0.2235\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3997 - acc: 0.2244\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3967 - acc: 0.2253\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3931 - acc: 0.2266\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3901 - acc: 0.2277\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3867 - acc: 0.2289\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3858 - acc: 0.2292\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3814 - acc: 0.2303\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3771 - acc: 0.2318\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3787 - acc: 0.2313\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3749 - acc: 0.2324\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3831 - acc: 0.2297\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3857 - acc: 0.2288\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3693 - acc: 0.2345\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3643 - acc: 0.2360\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3623 - acc: 0.2363\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3604 - acc: 0.2370\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3579 - acc: 0.2381\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3573 - acc: 0.2381\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.3893 - acc: 0.2275\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3710 - acc: 0.2335\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3581 - acc: 0.2376\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3521 - acc: 0.2400\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3490 - acc: 0.2407\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3470 - acc: 0.2415\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3439 - acc: 0.2425\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3435 - acc: 0.2422\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3422 - acc: 0.2430\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.3395 - acc: 0.2441\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.3376 - acc: 0.2445\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3361 - acc: 0.2449\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3343 - acc: 0.2453\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.3332 - acc: 0.2458\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3334 - acc: 0.2456\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.3306 - acc: 0.2465\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3293 - acc: 0.2469\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3272 - acc: 0.2478\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3266 - acc: 0.2479\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3262 - acc: 0.2483\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3244 - acc: 0.2484\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3211 - acc: 0.2497\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3206 - acc: 0.2500\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3203 - acc: 0.2498\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3184 - acc: 0.2507\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3172 - acc: 0.2513\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3162 - acc: 0.2512\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3146 - acc: 0.2520\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3151 - acc: 0.2519\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3127 - acc: 0.2526\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3108 - acc: 0.2531\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3098 - acc: 0.2537\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3085 - acc: 0.2540\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3075 - acc: 0.2539\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3080 - acc: 0.2542\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3073 - acc: 0.2540\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3039 - acc: 0.2555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1177\n",
      "           1       0.61      0.65      0.63     21638\n",
      "           2       0.34      0.24      0.28      4094\n",
      "           3       0.69      0.76      0.73     33410\n",
      "           4       0.36      0.21      0.27       690\n",
      "           5       0.42      0.54      0.48     20459\n",
      "           6       0.36      0.06      0.10      9029\n",
      "           7       0.38      0.38      0.38     11719\n",
      "\n",
      "    accuracy                           0.56    102216\n",
      "   macro avg       0.40      0.36      0.36    102216\n",
      "weighted avg       0.53      0.56      0.53    102216\n",
      "\n",
      "Acur√°cia\n",
      "0.3559423813073241\n",
      "Precisao\n",
      "0.5338225271644209\n",
      "Recall\n",
      "0.5560284104249824\n",
      "F1\n",
      "0.5329092123930853\n",
      "[[    0   208    24   208     5   562    34   136]\n",
      " [    0 14138   250  2661    36  3587   112   854]\n",
      " [    0   468   981   976    10  1018    58   583]\n",
      " [    0  2372   481 25488   125  3071    88  1785]\n",
      " [    0    70    11   345   146    78     2    38]\n",
      " [    0  3357   438  3034    34 11139   411  2046]\n",
      " [    0  1210   256  1416    16  3989   543  1599]\n",
      " [    0  1188   450  2553    32  2829   267  4400]]\n",
      "TRAIN: [   1    2    3 ... 1997 1998 1999] TEST: [   0    5    6    9   29   30   40   42   45   47   48   52   58   61\n",
      "   67   72   74   91   97  103  106  109  110  113  120  122  123  125\n",
      "  134  135  137  142  155  157  162  176  178  181  183  192  196  203\n",
      "  209  210  231  236  237  239  240  244  247  249  250  255  259  262\n",
      "  267  274  282  289  295  301  304  308  310  313  322  326  327  335\n",
      "  348  351  354  356  357  368  371  377  380  382  392  394  399  404\n",
      "  408  410  417  420  422  423  428  430  433  439  454  456  465  466\n",
      "  480  490  492  500  510  522  525  533  534  539  541  543  550  557\n",
      "  558  560  566  576  579  584  585  592  604  607  609  614  616  639\n",
      "  641  651  656  666  669  670  671  681  685  688  689  693  717  718\n",
      "  719  721  725  726  733  738  752  753  764  775  782  787  788  793\n",
      "  795  796  798  800  801  814  819  820  835  839  840  841  842  843\n",
      "  850  860  861  863  865  870  873  874  878  881  882  889  895  896\n",
      "  901  909  916  919  924  932  933  938  944  947  949  957  965  968\n",
      "  978  980  996 1006 1015 1035 1036 1039 1040 1042 1048 1050 1051 1057\n",
      " 1061 1067 1070 1074 1077 1083 1087 1089 1093 1095 1096 1106 1112 1117\n",
      " 1118 1126 1127 1131 1135 1137 1138 1139 1143 1145 1153 1163 1166 1170\n",
      " 1173 1180 1193 1209 1210 1212 1214 1219 1230 1232 1234 1238 1243 1245\n",
      " 1251 1256 1261 1264 1276 1281 1288 1296 1304 1309 1315 1317 1322 1331\n",
      " 1341 1345 1357 1358 1363 1368 1370 1374 1381 1382 1388 1392 1397 1398\n",
      " 1409 1415 1422 1424 1427 1437 1447 1464 1471 1473 1482 1486 1489 1492\n",
      " 1493 1496 1503 1505 1506 1511 1514 1518 1526 1529 1534 1546 1552 1553\n",
      " 1554 1561 1568 1571 1573 1574 1577 1588 1597 1599 1609 1618 1619 1621\n",
      " 1623 1627 1634 1639 1642 1647 1653 1660 1665 1667 1669 1672 1673 1684\n",
      " 1685 1690 1693 1697 1700 1703 1715 1729 1733 1737 1741 1744 1753 1764\n",
      " 1767 1768 1770 1777 1782 1783 1784 1792 1793 1799 1808 1809 1814 1826\n",
      " 1828 1834 1836 1839 1843 1844 1846 1863 1865 1870 1872 1875 1876 1893\n",
      " 1896 1901 1912 1918 1926 1928 1933 1934 1938 1943 1949 1951 1952 1956\n",
      " 1957 1960 1961 1963 1964 1969 1971 1993]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.6239 - acc: 0.1460\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5638 - acc: 0.1632\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5374 - acc: 0.1740\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5259 - acc: 0.1787\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5182 - acc: 0.1825\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5132 - acc: 0.1848\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5083 - acc: 0.1871\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5037 - acc: 0.1893\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5004 - acc: 0.1904\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4959 - acc: 0.1925\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4932 - acc: 0.1935\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4921 - acc: 0.1940\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4924 - acc: 0.1932\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4877 - acc: 0.1956\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4849 - acc: 0.1969\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4822 - acc: 0.1982\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4786 - acc: 0.1995\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4775 - acc: 0.1999\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4749 - acc: 0.2006\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4732 - acc: 0.2015\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4703 - acc: 0.2027\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4669 - acc: 0.2042\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4653 - acc: 0.2046\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4644 - acc: 0.2049\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4590 - acc: 0.2070\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4549 - acc: 0.2082\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4550 - acc: 0.2083\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4510 - acc: 0.2103\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4479 - acc: 0.2109\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4427 - acc: 0.2128\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4403 - acc: 0.2136\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4393 - acc: 0.2139\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4369 - acc: 0.2148\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4306 - acc: 0.2168\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4288 - acc: 0.2175\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4242 - acc: 0.2192\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4202 - acc: 0.2204\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4184 - acc: 0.2212\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4154 - acc: 0.2224\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4103 - acc: 0.2240\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4080 - acc: 0.2248\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4029 - acc: 0.2265\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4012 - acc: 0.2267\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3988 - acc: 0.2277\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3965 - acc: 0.2280\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.3925 - acc: 0.2296\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3883 - acc: 0.2308\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3869 - acc: 0.2313\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3839 - acc: 0.2321\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3806 - acc: 0.2336\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3791 - acc: 0.2339\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3761 - acc: 0.2348\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3731 - acc: 0.2359\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3713 - acc: 0.2363\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3694 - acc: 0.2370\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3658 - acc: 0.2379\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3639 - acc: 0.2387\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3625 - acc: 0.2392\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3594 - acc: 0.2401\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3605 - acc: 0.2396\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3695 - acc: 0.2367\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3549 - acc: 0.2419\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3499 - acc: 0.2429\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3484 - acc: 0.2439\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3463 - acc: 0.2443\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3442 - acc: 0.2450\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3430 - acc: 0.2451\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3430 - acc: 0.2453\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3477 - acc: 0.2436\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3425 - acc: 0.2455\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.3382 - acc: 0.2470\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3339 - acc: 0.2482\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3341 - acc: 0.2481\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3313 - acc: 0.2488\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3302 - acc: 0.2495\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3295 - acc: 0.2497\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3281 - acc: 0.2500\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3267 - acc: 0.2505\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3238 - acc: 0.2515\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3231 - acc: 0.2515\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3225 - acc: 0.2518\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3217 - acc: 0.2525\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3194 - acc: 0.2530\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3185 - acc: 0.2533\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3174 - acc: 0.2537\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3147 - acc: 0.2545\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3152 - acc: 0.2544\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3141 - acc: 0.2545\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3120 - acc: 0.2556\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3110 - acc: 0.2557\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3093 - acc: 0.2561\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3101 - acc: 0.2562\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3082 - acc: 0.2567\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3067 - acc: 0.2569\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3072 - acc: 0.2568\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3041 - acc: 0.2580\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3032 - acc: 0.2584\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3035 - acc: 0.2584\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.3008 - acc: 0.2591\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3007 - acc: 0.2594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.00      1233\n",
      "           1       0.62      0.61      0.62     21039\n",
      "           2       0.26      0.21      0.23      3612\n",
      "           3       0.69      0.76      0.72     32682\n",
      "           4       0.42      0.23      0.30       700\n",
      "           5       0.42      0.54      0.47     20091\n",
      "           6       0.32      0.06      0.10      8751\n",
      "           7       0.38      0.36      0.37     11165\n",
      "\n",
      "    accuracy                           0.55     99273\n",
      "   macro avg       0.45      0.35      0.35     99273\n",
      "weighted avg       0.53      0.55      0.52     99273\n",
      "\n",
      "Acur√°cia\n",
      "0.3487471656498319\n",
      "Precisao\n",
      "0.5298425077236827\n",
      "Recall\n",
      "0.5467952011120849\n",
      "F1\n",
      "0.5248162391379048\n",
      "[[    1   248    54   189     4   571    46   120]\n",
      " [    0 12935   318  2932    24  3754   151   925]\n",
      " [    0   343   764   972    13   963    54   503]\n",
      " [    0  2205   523 24965   119  3173    98  1599]\n",
      " [    0    83     9   331   164    67     3    43]\n",
      " [    0  3085   527  3106    28 10869   457  2019]\n",
      " [    0  1106   287  1319    21  3969   531  1518]\n",
      " [    1  1010   430  2523    22  2806   320  4053]]\n",
      "TRAIN: [   0    1    2 ... 1994 1995 1998] TEST: [   4   16   17   18   22   28   36   41   50   68   70   78   80   81\n",
      "   85   89   92   93   98  112  116  117  121  127  140  141  146  149\n",
      "  150  153  159  166  167  168  175  179  182  185  187  190  198  199\n",
      "  201  205  207  223  228  234  235  241  243  251  265  266  277  279\n",
      "  286  290  297  298  309  312  321  324  330  332  334  339  344  347\n",
      "  353  362  365  366  367  383  391  418  426  441  443  450  455  457\n",
      "  459  472  476  477  481  484  487  489  503  504  513  516  524  528\n",
      "  529  537  538  540  545  546  549  554  555  556  559  565  575  582\n",
      "  583  590  591  600  605  619  620  631  632  636  642  647  648  649\n",
      "  653  662  663  672  677  678  683  702  708  713  714  715  731  734\n",
      "  750  754  761  763  769  770  774  776  777  779  783  785  792  803\n",
      "  808  810  811  827  828  831  833  837  851  852  856  858  866  875\n",
      "  880  884  905  910  921  923  929  934  935  936  939  940  942  943\n",
      "  948  950  961  970  972  973  974  976  977  979  988  990  998 1000\n",
      " 1004 1007 1011 1012 1013 1027 1028 1029 1049 1060 1062 1075 1076 1080\n",
      " 1086 1090 1105 1114 1115 1116 1122 1125 1128 1130 1134 1136 1146 1147\n",
      " 1152 1155 1162 1164 1169 1172 1175 1179 1191 1196 1199 1207 1211 1215\n",
      " 1218 1225 1226 1231 1233 1241 1244 1252 1253 1254 1267 1269 1271 1273\n",
      " 1275 1278 1290 1293 1305 1311 1316 1320 1324 1330 1335 1337 1339 1343\n",
      " 1349 1352 1354 1356 1365 1373 1380 1385 1386 1389 1400 1406 1407 1412\n",
      " 1419 1420 1423 1425 1428 1436 1444 1450 1452 1458 1461 1465 1467 1470\n",
      " 1474 1480 1483 1484 1487 1502 1504 1508 1512 1513 1524 1525 1527 1535\n",
      " 1541 1563 1566 1569 1575 1579 1581 1590 1593 1596 1600 1604 1606 1610\n",
      " 1611 1615 1617 1622 1624 1628 1629 1636 1645 1651 1652 1662 1664 1668\n",
      " 1674 1678 1679 1682 1686 1694 1695 1699 1706 1716 1721 1722 1724 1725\n",
      " 1728 1730 1734 1739 1750 1752 1755 1760 1763 1771 1774 1785 1786 1796\n",
      " 1801 1803 1815 1816 1817 1818 1821 1823 1825 1830 1831 1840 1841 1842\n",
      " 1848 1852 1853 1855 1866 1869 1895 1907 1913 1914 1920 1922 1936 1968\n",
      " 1978 1982 1984 1986 1989 1996 1997 1999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.6215 - acc: 0.1395\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.5625 - acc: 0.1600\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.5343 - acc: 0.1702\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 5s 3ms/sample - loss: 0.5207 - acc: 0.1760\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5150 - acc: 0.1789\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.5070 - acc: 0.1828\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5012 - acc: 0.1857\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.5000 - acc: 0.1864\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4941 - acc: 0.1891\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4926 - acc: 0.1892\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4902 - acc: 0.1903\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4861 - acc: 0.1921\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4863 - acc: 0.1918\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4830 - acc: 0.1933\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4811 - acc: 0.1941\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4773 - acc: 0.1952\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4738 - acc: 0.1968\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4719 - acc: 0.1976\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4717 - acc: 0.1973\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4683 - acc: 0.1987\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4658 - acc: 0.2003\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4631 - acc: 0.2011\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4625 - acc: 0.2010\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4580 - acc: 0.2028\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4586 - acc: 0.2024\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4531 - acc: 0.2047\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4502 - acc: 0.2057\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4480 - acc: 0.2063\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4447 - acc: 0.2080\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4403 - acc: 0.2091\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4367 - acc: 0.2102\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4348 - acc: 0.2113\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4311 - acc: 0.2127\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4285 - acc: 0.2132\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4254 - acc: 0.2145\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4242 - acc: 0.2149\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4165 - acc: 0.2179\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4152 - acc: 0.2179\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.4124 - acc: 0.2191\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4083 - acc: 0.2204\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4030 - acc: 0.2220\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.4016 - acc: 0.2225\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3999 - acc: 0.2230\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3953 - acc: 0.2249\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3907 - acc: 0.2261\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3896 - acc: 0.2264\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3878 - acc: 0.2275\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3837 - acc: 0.2286\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3809 - acc: 0.2294\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3797 - acc: 0.2297\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3749 - acc: 0.2313\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3742 - acc: 0.2315\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3834 - acc: 0.2284\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3706 - acc: 0.2328\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3654 - acc: 0.2342\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3630 - acc: 0.2348\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3605 - acc: 0.2361\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3586 - acc: 0.2363\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3552 - acc: 0.2373\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3522 - acc: 0.2384\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3525 - acc: 0.2382\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3500 - acc: 0.2392\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3478 - acc: 0.2401\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3451 - acc: 0.2408\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3417 - acc: 0.2417\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3409 - acc: 0.2422\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3405 - acc: 0.2421\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3385 - acc: 0.2426\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3378 - acc: 0.2429\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3341 - acc: 0.2441\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3342 - acc: 0.2444\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3328 - acc: 0.2447\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3311 - acc: 0.2449\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3280 - acc: 0.2463\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3296 - acc: 0.2453\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3258 - acc: 0.2469\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3263 - acc: 0.2468\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3253 - acc: 0.2469\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3205 - acc: 0.2487\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3203 - acc: 0.2486\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3212 - acc: 0.2483\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3174 - acc: 0.2496\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3150 - acc: 0.2505\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3134 - acc: 0.2506\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3129 - acc: 0.2510\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3124 - acc: 0.2513\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3145 - acc: 0.2506\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3092 - acc: 0.2522\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3078 - acc: 0.2528\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3066 - acc: 0.2530\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3059 - acc: 0.2534\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3036 - acc: 0.2540\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3041 - acc: 0.2538\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 6s 3ms/sample - loss: 0.3035 - acc: 0.2540\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3029 - acc: 0.2544\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3008 - acc: 0.2549\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.3017 - acc: 0.2545\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.2987 - acc: 0.2555\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.2972 - acc: 0.2561\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 6s 4ms/sample - loss: 0.2959 - acc: 0.2566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1230\n",
      "           1       0.58      0.66      0.62     21473\n",
      "           2       0.34      0.23      0.28      4264\n",
      "           3       0.71      0.74      0.73     34206\n",
      "           4       0.58      0.25      0.35       653\n",
      "           5       0.43      0.54      0.48     21265\n",
      "           6       0.36      0.07      0.11      9289\n",
      "           7       0.37      0.40      0.38     11712\n",
      "\n",
      "    accuracy                           0.55    104092\n",
      "   macro avg       0.55      0.36      0.37    104092\n",
      "weighted avg       0.55      0.55      0.53    104092\n",
      "\n",
      "Acur√°cia\n",
      "0.36109926613388743\n",
      "Precisao\n",
      "0.5454382647049127\n",
      "Recall\n",
      "0.5528282673020021\n",
      "F1\n",
      "0.531516504154013\n",
      "[[    1   261    33   170     0   590    29   146]\n",
      " [    0 14277   201  2611    15  3324   125   920]\n",
      " [    0   523   991   883    13  1028    84   742]\n",
      " [    0  2913   537 25402    63  3172   105  2014]\n",
      " [    0    86    13   287   161    57     8    41]\n",
      " [    0  3810   441  2800     9 11444   463  2298]\n",
      " [    0  1332   221  1288     5  4105   605  1733]\n",
      " [    0  1287   436  2262    10  2770   283  4664]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_8 (Bidirection (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 340,808\n",
      "Trainable params: 340,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.3371989476133228, 0.3529153921064021, 0.3559423813073241, 0.3487471656498319, 0.36109926613388743]\n",
      "0.35118063056215365\n",
      "Precision total\n",
      "[0.5182006635183862, 0.5362492531929546, 0.5338225271644209, 0.5298425077236827, 0.5454382647049127]\n",
      "0.5327106432608714\n",
      "Recalls total\n",
      "[0.536523037460998, 0.5545699560262897, 0.5560284104249824, 0.5467952011120849, 0.5528282673020021]\n",
      "0.5493489744652714\n",
      "F1 total\n",
      "[0.5158061111484329, 0.5336454678925882, 0.5329092123930853, 0.5248162391379048, 0.531516504154013]\n",
      "0.5277387069452047\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accu)\n",
    "a = np.array(accu)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "p = np.array(precisions)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "r = np.array(recalls)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f = np.array(f1)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
