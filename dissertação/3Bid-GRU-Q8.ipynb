{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 700, 20)\n",
      "(2000, 700, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "previsores = base.iloc[:1400000,0:20].values\n",
    "previsores = np.reshape(previsores, (2000, 700, 20))\n",
    "print(previsores.shape)\n",
    "\n",
    "classes = base.iloc[:1400000, 20:28].values\n",
    "classes = np.reshape(classes, (2000, 700, 8))\n",
    "print(classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNGRU, Bidirectional, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarRede():\n",
    "    model = Sequential()\n",
    "  \n",
    "    #model.add(Masking(mask_value = 0, input_shape = (700, 20)))\n",
    "  \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True), input_shape = (700, 20)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Bidirectional(CuDNNGRU(100, return_sequences = True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(8, activation = 'softmax'))\n",
    "  \n",
    "    model.compile(optimizer = 'adam', metrics = ['acc'], loss='categorical_crossentropy')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train, epochs = 100, verbose = 1, batch_size = 32)\n",
    "  \n",
    "    predicted = model.predict(x_test)\n",
    "  \n",
    "    y_teste = []\n",
    "    predict = []\n",
    "  \n",
    "    predicted = np.reshape(predicted, (predicted.shape[0] * predicted.shape[1], 8))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0] * y_test.shape[1], 8))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0] * x_test.shape[1], 20))\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        cont = 0\n",
    "        for j in range(len(x_test[i])):\n",
    "            cont += x_test[i][j]\n",
    "        if cont != 0:\n",
    "            y_teste.append(y_test[i])\n",
    "            predict.append(predicted[i])\n",
    "    \n",
    "    y_teste = np.asarray(y_teste)\n",
    "    predict = np.asarray(predict)\n",
    "\n",
    "    predicted = predict\n",
    "    y_test = y_teste\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accu.append(balanced_accuracy_score(y_test, predicted)) \n",
    "    precisions.append(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    recalls.append(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    f1.append(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    print('Acur√°cia')\n",
    "    print(balanced_accuracy_score(y_test, predicted))\n",
    "    print('Precisao')\n",
    "    print(precision_score(y_test, predicted, average = 'weighted'))\n",
    "    print('Recall')\n",
    "    print(recall_score(y_test, predicted, average = 'weighted'))\n",
    "    print('F1')\n",
    "    print(f1_score(y_test, predicted, average = 'weighted'))\n",
    "    print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0822 10:32:57.561510  8048 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 10:32:57.565528  8048 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 10:32:57.566497  8048 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 10:32:57.567495  8048 deprecation.py:506] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 1996 1998 1999] TEST: [   4   16   26   27   29   33   38   41   43   49   52   55   59   66\n",
      "   68   72   77   79   83   92   93   97  112  115  117  124  126  138\n",
      "  140  143  145  157  159  164  166  173  176  178  187  193  199  203\n",
      "  204  209  210  214  215  221  223  225  226  245  246  251  255  258\n",
      "  262  265  267  269  273  274  280  288  295  297  313  314  315  317\n",
      "  319  320  325  333  334  336  342  344  345  347  352  360  362  369\n",
      "  373  380  388  394  395  401  413  423  433  441  445  446  452  461\n",
      "  462  467  470  473  474  480  487  492  497  499  521  522  523  529\n",
      "  533  543  548  554  561  572  582  586  589  591  595  596  600  603\n",
      "  604  605  610  615  622  629  630  636  638  639  644  646  648  655\n",
      "  656  661  668  669  676  680  681  696  710  714  716  718  722  729\n",
      "  740  743  747  748  752  757  764  765  798  810  823  827  828  845\n",
      "  851  852  870  871  872  877  878  896  899  907  909  911  914  927\n",
      "  929  946  952  953  957  960  961  965  974  975  983  985  986  991\n",
      "  992  997  999 1000 1003 1006 1009 1017 1020 1023 1028 1033 1041 1058\n",
      " 1059 1060 1064 1071 1072 1076 1080 1091 1101 1108 1111 1128 1138 1147\n",
      " 1149 1151 1155 1163 1171 1172 1179 1183 1187 1191 1204 1206 1213 1217\n",
      " 1224 1225 1236 1251 1256 1257 1276 1280 1283 1302 1307 1324 1325 1327\n",
      " 1331 1335 1337 1345 1346 1348 1349 1350 1351 1358 1361 1367 1385 1395\n",
      " 1398 1405 1406 1408 1430 1433 1434 1435 1441 1447 1451 1456 1457 1459\n",
      " 1460 1472 1473 1488 1498 1501 1502 1509 1515 1523 1527 1530 1541 1546\n",
      " 1548 1556 1558 1560 1562 1565 1566 1569 1572 1577 1578 1580 1586 1597\n",
      " 1598 1599 1608 1614 1615 1621 1624 1631 1632 1635 1636 1639 1643 1652\n",
      " 1658 1663 1670 1674 1677 1678 1681 1686 1687 1694 1706 1714 1715 1716\n",
      " 1717 1719 1727 1730 1733 1734 1739 1745 1751 1762 1764 1775 1778 1783\n",
      " 1785 1789 1791 1794 1795 1796 1798 1801 1805 1807 1809 1810 1820 1821\n",
      " 1824 1833 1836 1840 1849 1850 1854 1857 1861 1870 1872 1877 1880 1889\n",
      " 1899 1906 1909 1910 1916 1919 1923 1924 1926 1930 1936 1955 1960 1962\n",
      " 1965 1975 1977 1978 1987 1988 1992 1997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 10:32:58.453149  8048 deprecation.py:323] From c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 10s 6ms/sample - loss: 0.5900 - acc: 0.1589\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5331 - acc: 0.1722\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5222 - acc: 0.1775\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5129 - acc: 0.1813\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5101 - acc: 0.1823\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5055 - acc: 0.1844\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5010 - acc: 0.1864\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4974 - acc: 0.1881\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4931 - acc: 0.1897\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4925 - acc: 0.1902\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4879 - acc: 0.1923\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4860 - acc: 0.1929\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4878 - acc: 0.1922\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4819 - acc: 0.1948\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4785 - acc: 0.1964\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4742 - acc: 0.1982\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4712 - acc: 0.1995\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4669 - acc: 0.2013\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4614 - acc: 0.2033\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4534 - acc: 0.2059\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4487 - acc: 0.2084\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4441 - acc: 0.2096\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4420 - acc: 0.2110\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4388 - acc: 0.2117\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4371 - acc: 0.2123\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4336 - acc: 0.2132\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4303 - acc: 0.2152\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4269 - acc: 0.2162\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4261 - acc: 0.2163\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4214 - acc: 0.2180\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4191 - acc: 0.2190\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4167 - acc: 0.2198\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4131 - acc: 0.2208\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4104 - acc: 0.2221\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4092 - acc: 0.2223\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4051 - acc: 0.2238\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4018 - acc: 0.2248\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4000 - acc: 0.2254\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3965 - acc: 0.2262\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3954 - acc: 0.2267\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3924 - acc: 0.2275\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3896 - acc: 0.2287\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3861 - acc: 0.2301\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3835 - acc: 0.2306\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3809 - acc: 0.2313\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3787 - acc: 0.2321\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3767 - acc: 0.2329\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3750 - acc: 0.2330\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3722 - acc: 0.2340\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3693 - acc: 0.2349\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3670 - acc: 0.2359\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3651 - acc: 0.2364\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3643 - acc: 0.2367\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3606 - acc: 0.2383\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3588 - acc: 0.2387\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3570 - acc: 0.2391\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3550 - acc: 0.2397\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3541 - acc: 0.2398\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3509 - acc: 0.2410\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3501 - acc: 0.2412\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3477 - acc: 0.2420\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3461 - acc: 0.2423\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3438 - acc: 0.2434\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3432 - acc: 0.2433\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3410 - acc: 0.2439\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3399 - acc: 0.2446\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3373 - acc: 0.2454\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3355 - acc: 0.2460\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3351 - acc: 0.2458\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3330 - acc: 0.2466\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3324 - acc: 0.2467\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3314 - acc: 0.2471\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3290 - acc: 0.2481\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3271 - acc: 0.2485\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3254 - acc: 0.2491\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3251 - acc: 0.2495\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3231 - acc: 0.2497\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3220 - acc: 0.2500\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3202 - acc: 0.2509\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3188 - acc: 0.2512\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3180 - acc: 0.2519\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3182 - acc: 0.2515\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3166 - acc: 0.2524\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3159 - acc: 0.2521\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3136 - acc: 0.2530\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3128 - acc: 0.2534\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3117 - acc: 0.2537\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3107 - acc: 0.2540\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3099 - acc: 0.2544\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3082 - acc: 0.2550\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3076 - acc: 0.2551\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3088 - acc: 0.2545\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3062 - acc: 0.2554\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3047 - acc: 0.2557\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3035 - acc: 0.2564\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3030 - acc: 0.2568\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3014 - acc: 0.2570\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3015 - acc: 0.2572\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2999 - acc: 0.2578\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2999 - acc: 0.2575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1193\n",
      "           1       0.62      0.73      0.67     20974\n",
      "           2       0.37      0.27      0.31      3737\n",
      "           3       0.76      0.82      0.79     34544\n",
      "           4       0.64      0.32      0.43       735\n",
      "           5       0.46      0.55      0.50     20601\n",
      "           6       0.39      0.09      0.15      9024\n",
      "           7       0.40      0.39      0.40     11693\n",
      "\n",
      "    accuracy                           0.60    102501\n",
      "   macro avg       0.58      0.40      0.41    102501\n",
      "weighted avg       0.59      0.60      0.58    102501\n",
      "\n",
      "Acur√°cia\n",
      "0.3954158072319261\n",
      "Precisao\n",
      "0.586060707274666\n",
      "Recall\n",
      "0.5985990380581653\n",
      "F1\n",
      "0.5759417896003483\n",
      "[[    1   274    26   147     0   615    34    96]\n",
      " [    0 15225   193  1645    17  2917   148   829]\n",
      " [    0   435  1001   782     3   823    79   614]\n",
      " [    0  1980   402 28207    88  2226   146  1495]\n",
      " [    0    34     6   340   236    68     7    44]\n",
      " [    0  3853   399  2429     8 11273   583  2056]\n",
      " [    0  1399   250  1185     4  3746   825  1615]\n",
      " [    0  1223   435  2503    11  2622   310  4589]]\n",
      "TRAIN: [   2    3    4 ... 1997 1998 1999] TEST: [   0    1    9   13   14   18   21   30   36   37   39   47   64   65\n",
      "   67   71   74   81   82   84   86   99  103  105  106  107  108  128\n",
      "  130  131  134  137  151  152  154  155  156  158  161  171  175  182\n",
      "  184  192  200  202  211  219  220  229  231  235  236  237  238  243\n",
      "  248  249  252  254  256  261  266  271  276  284  301  303  305  307\n",
      "  310  312  339  340  343  354  361  375  376  383  390  393  398  400\n",
      "  402  405  407  408  416  419  428  432  438  439  443  448  454  455\n",
      "  458  459  469  471  481  482  483  486  491  494  502  506  517  534\n",
      "  535  540  541  550  559  560  564  565  569  570  571  573  575  593\n",
      "  606  607  611  612  613  621  625  628  635  640  641  642  645  658\n",
      "  659  662  667  674  677  684  686  687  688  695  704  717  723  725\n",
      "  731  735  737  750  758  767  771  772  773  776  777  782  786  789\n",
      "  792  794  796  799  800  802  809  816  820  826  839  844  846  855\n",
      "  857  861  867  869  873  875  879  881  888  902  903  908  916  926\n",
      "  931  932  936  949  968  970  971  973  977  981  996 1001 1007 1011\n",
      " 1019 1049 1053 1081 1085 1089 1100 1105 1107 1109 1114 1118 1120 1122\n",
      " 1124 1126 1134 1141 1148 1153 1161 1164 1167 1170 1180 1189 1194 1197\n",
      " 1201 1203 1209 1210 1211 1212 1220 1221 1223 1227 1229 1231 1237 1239\n",
      " 1240 1254 1255 1261 1262 1263 1264 1267 1275 1286 1288 1297 1314 1315\n",
      " 1323 1326 1330 1332 1336 1338 1355 1366 1378 1388 1396 1402 1410 1412\n",
      " 1419 1422 1428 1431 1442 1453 1455 1469 1477 1480 1482 1485 1486 1487\n",
      " 1495 1507 1512 1516 1528 1529 1531 1536 1538 1539 1543 1550 1557 1559\n",
      " 1564 1570 1575 1583 1589 1605 1610 1612 1617 1618 1625 1626 1628 1638\n",
      " 1656 1657 1660 1661 1668 1669 1671 1672 1675 1676 1682 1684 1688 1689\n",
      " 1695 1700 1702 1704 1710 1723 1725 1735 1738 1744 1748 1749 1754 1757\n",
      " 1765 1766 1767 1772 1781 1782 1788 1793 1797 1799 1800 1808 1814 1817\n",
      " 1818 1819 1825 1827 1829 1832 1837 1844 1848 1851 1855 1859 1874 1879\n",
      " 1885 1888 1904 1905 1907 1908 1912 1917 1921 1922 1925 1927 1935 1937\n",
      " 1938 1940 1942 1944 1954 1968 1972 1993]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5857 - acc: 0.1617\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5308 - acc: 0.1717\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5197 - acc: 0.1770\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5112 - acc: 0.1807\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5070 - acc: 0.1821\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5034 - acc: 0.1836\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4998 - acc: 0.1854\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4952 - acc: 0.1877\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4913 - acc: 0.1894\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4881 - acc: 0.1908\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4852 - acc: 0.1921\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4833 - acc: 0.1929\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4844 - acc: 0.1922\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4795 - acc: 0.1945\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4780 - acc: 0.1950\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4727 - acc: 0.1975\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4690 - acc: 0.1991\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4655 - acc: 0.2000\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4596 - acc: 0.2026\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4517 - acc: 0.2058\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4467 - acc: 0.2075\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4445 - acc: 0.2082\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4397 - acc: 0.2101\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4359 - acc: 0.2116\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4338 - acc: 0.2121\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4310 - acc: 0.2132\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4276 - acc: 0.2143\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4241 - acc: 0.2157\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4215 - acc: 0.2166\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4192 - acc: 0.2173\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4165 - acc: 0.2183\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4145 - acc: 0.2189\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4106 - acc: 0.2204\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4084 - acc: 0.2209\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4046 - acc: 0.2221\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4023 - acc: 0.2230\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3994 - acc: 0.2242\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3968 - acc: 0.2250\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3943 - acc: 0.2256\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3920 - acc: 0.2267\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3887 - acc: 0.2277\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3885 - acc: 0.2275\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3849 - acc: 0.2287\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3819 - acc: 0.2300\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3789 - acc: 0.2306\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3777 - acc: 0.2310\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3783 - acc: 0.2311\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3714 - acc: 0.2334\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3697 - acc: 0.2339\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3667 - acc: 0.2348\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3647 - acc: 0.2352\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3622 - acc: 0.2362\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3621 - acc: 0.2360\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3606 - acc: 0.2364\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3566 - acc: 0.2381\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3543 - acc: 0.2386\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3529 - acc: 0.2391\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3504 - acc: 0.2399\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3490 - acc: 0.2404\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3459 - acc: 0.2412\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3449 - acc: 0.2415\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3440 - acc: 0.2418\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3420 - acc: 0.2423\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3397 - acc: 0.2434\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3381 - acc: 0.2437\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3370 - acc: 0.2440\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3362 - acc: 0.2446\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3339 - acc: 0.2448\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3321 - acc: 0.2457\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3294 - acc: 0.2466\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3287 - acc: 0.2467\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3276 - acc: 0.2472\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3248 - acc: 0.2481\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3242 - acc: 0.2482\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3230 - acc: 0.2488\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3217 - acc: 0.2493\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3209 - acc: 0.2494\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3197 - acc: 0.2498\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3177 - acc: 0.2504\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3166 - acc: 0.2506\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3161 - acc: 0.2511\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3140 - acc: 0.2516\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3127 - acc: 0.2522\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3115 - acc: 0.2524\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3106 - acc: 0.2525\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3087 - acc: 0.2532\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3085 - acc: 0.2533\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3070 - acc: 0.2538\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3060 - acc: 0.2542\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3049 - acc: 0.2544\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3040 - acc: 0.2548\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3035 - acc: 0.2548\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3027 - acc: 0.2552\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3018 - acc: 0.2555\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2999 - acc: 0.2563\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2985 - acc: 0.2567\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2987 - acc: 0.2566\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2979 - acc: 0.2570\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2970 - acc: 0.2570\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2959 - acc: 0.2574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.00      0.01      1245\n",
      "           1       0.64      0.66      0.65     22132\n",
      "           2       0.36      0.24      0.29      4223\n",
      "           3       0.73      0.82      0.77     34108\n",
      "           4       0.50      0.21      0.29       728\n",
      "           5       0.46      0.56      0.51     20723\n",
      "           6       0.34      0.10      0.15      9153\n",
      "           7       0.41      0.41      0.41     11538\n",
      "\n",
      "    accuracy                           0.59    103850\n",
      "   macro avg       0.53      0.37      0.38    103850\n",
      "weighted avg       0.57      0.59      0.57    103850\n",
      "\n",
      "Acur√°cia\n",
      "0.3748542117855684\n",
      "Precisao\n",
      "0.5705125983172444\n",
      "Recall\n",
      "0.5876263842079923\n",
      "F1\n",
      "0.56535897127814\n",
      "[[    4   275    26   188     1   578    52   121]\n",
      " [    0 14549   251  2618    24  3472   244   974]\n",
      " [    0   442  1012  1098     7   922    89   653]\n",
      " [    0  1568   455 28083    90  2202   155  1555]\n",
      " [    0    67     9   405   151    46     9    41]\n",
      " [    1  3376   436  2575     7 11587   699  2042]\n",
      " [    0  1231   212  1326    12  3909   884  1579]\n",
      " [    0  1083   402  2428    13  2411   446  4755]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    4 ... 1993 1996 1997] TEST: [   2    3    5    7   12   15   17   23   24   25   31   46   48   69\n",
      "   73   75   76   88   94   96   98  100  101  102  110  113  114  118\n",
      "  120  132  135  142  147  148  162  163  172  174  179  185  190  197\n",
      "  216  222  227  247  250  253  270  275  279  323  324  329  332  337\n",
      "  341  353  358  359  365  366  381  387  396  397  399  403  404  412\n",
      "  415  422  435  447  449  460  464  466  476  477  478  485  488  489\n",
      "  500  501  503  504  516  518  525  526  528  532  538  553  556  557\n",
      "  558  574  576  590  592  597  609  614  616  623  627  631  632  637\n",
      "  643  649  663  666  672  675  682  683  685  690  692  701  706  708\n",
      "  711  720  742  745  753  754  755  763  768  783  784  787  788  797\n",
      "  801  804  807  817  822  825  829  830  832  833  836  837  840  854\n",
      "  858  863  864  865  868  880  882  883  884  890  895  900  904  906\n",
      "  910  913  917  921  933  934  935  941  954  962  966  967  972  976\n",
      "  978  980  984  989 1004 1013 1016 1018 1022 1025 1027 1034 1040 1042\n",
      " 1043 1045 1046 1048 1051 1052 1057 1061 1062 1066 1067 1068 1070 1077\n",
      " 1082 1084 1112 1113 1116 1119 1127 1129 1135 1139 1142 1145 1152 1158\n",
      " 1159 1160 1166 1168 1169 1181 1185 1193 1198 1207 1230 1234 1238 1241\n",
      " 1246 1247 1250 1258 1269 1271 1279 1284 1285 1303 1304 1305 1308 1310\n",
      " 1312 1316 1321 1322 1334 1339 1352 1363 1368 1371 1373 1374 1379 1380\n",
      " 1381 1386 1390 1391 1403 1409 1411 1414 1416 1417 1420 1423 1438 1444\n",
      " 1445 1450 1452 1454 1458 1462 1464 1471 1491 1492 1493 1494 1504 1506\n",
      " 1508 1517 1520 1526 1542 1544 1551 1553 1555 1563 1567 1568 1574 1576\n",
      " 1582 1585 1587 1592 1593 1603 1611 1616 1619 1622 1623 1629 1630 1633\n",
      " 1642 1644 1645 1648 1650 1651 1673 1683 1685 1692 1693 1697 1699 1701\n",
      " 1708 1712 1722 1729 1736 1740 1756 1758 1759 1761 1763 1768 1773 1777\n",
      " 1780 1784 1786 1787 1802 1811 1816 1823 1834 1841 1847 1856 1863 1864\n",
      " 1865 1871 1873 1876 1881 1882 1884 1887 1890 1892 1893 1895 1896 1900\n",
      " 1901 1915 1920 1929 1941 1946 1947 1951 1957 1958 1963 1967 1969 1980\n",
      " 1984 1986 1989 1991 1994 1995 1998 1999]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5949 - acc: 0.1620\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5412 - acc: 0.1734\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5278 - acc: 0.1800\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5204 - acc: 0.1832\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5159 - acc: 0.1850\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5095 - acc: 0.1879\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5075 - acc: 0.1885\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5022 - acc: 0.1913\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4971 - acc: 0.1934\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4946 - acc: 0.1947\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4920 - acc: 0.1962\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4905 - acc: 0.1965\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4857 - acc: 0.1985\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4829 - acc: 0.1998\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4804 - acc: 0.2006\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4761 - acc: 0.2024\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4721 - acc: 0.2040\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4651 - acc: 0.2068\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4585 - acc: 0.2094\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4558 - acc: 0.2103\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4521 - acc: 0.2117\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4484 - acc: 0.2130\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4462 - acc: 0.2138\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4434 - acc: 0.2151\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4402 - acc: 0.2161\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4392 - acc: 0.2165\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4354 - acc: 0.2175\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4318 - acc: 0.2187\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4294 - acc: 0.2197\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4275 - acc: 0.2201\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4258 - acc: 0.2207\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4211 - acc: 0.2227\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4188 - acc: 0.2236\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4166 - acc: 0.2240\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4145 - acc: 0.2249\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4106 - acc: 0.2262\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4074 - acc: 0.2272\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4063 - acc: 0.2278\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4030 - acc: 0.2285\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4008 - acc: 0.2292\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3975 - acc: 0.2304\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3958 - acc: 0.2310\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3936 - acc: 0.2318\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3899 - acc: 0.2331\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3872 - acc: 0.2337\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3859 - acc: 0.2344\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3830 - acc: 0.2353\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3803 - acc: 0.2362\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3801 - acc: 0.2363\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3762 - acc: 0.2377\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3734 - acc: 0.2382\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3720 - acc: 0.2387\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3716 - acc: 0.2389\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3681 - acc: 0.2400\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3649 - acc: 0.2412\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3649 - acc: 0.2410\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3622 - acc: 0.2420\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3596 - acc: 0.2428\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3571 - acc: 0.2435\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3556 - acc: 0.2439\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3541 - acc: 0.2443\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3525 - acc: 0.2449\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3533 - acc: 0.2444\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3485 - acc: 0.2460\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3467 - acc: 0.2469\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3457 - acc: 0.2471\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3444 - acc: 0.2475\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3426 - acc: 0.2479\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3403 - acc: 0.2487\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3402 - acc: 0.2489\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3383 - acc: 0.2494\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3366 - acc: 0.2500\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3356 - acc: 0.2503\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3351 - acc: 0.2507\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3333 - acc: 0.2509\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3325 - acc: 0.2516\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3315 - acc: 0.2518\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3281 - acc: 0.2528\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3274 - acc: 0.2530\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3259 - acc: 0.2534\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3254 - acc: 0.2537\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3230 - acc: 0.2547\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3228 - acc: 0.2544\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3214 - acc: 0.2552\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3200 - acc: 0.2556\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3186 - acc: 0.2558\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3180 - acc: 0.2559\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3165 - acc: 0.2570\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3156 - acc: 0.2569\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3140 - acc: 0.2575\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3147 - acc: 0.2573\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3128 - acc: 0.2578\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3121 - acc: 0.2581\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3116 - acc: 0.2583\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3094 - acc: 0.2590\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3097 - acc: 0.2588\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3074 - acc: 0.2597\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3075 - acc: 0.2594\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3061 - acc: 0.2602\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3043 - acc: 0.2607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\gabriel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1181\n",
      "           1       0.62      0.70      0.66     19700\n",
      "           2       0.38      0.23      0.29      3692\n",
      "           3       0.74      0.83      0.78     32898\n",
      "           4       0.49      0.20      0.29       718\n",
      "           5       0.47      0.53      0.50     19573\n",
      "           6       0.38      0.12      0.18      8532\n",
      "           7       0.41      0.40      0.40     10898\n",
      "\n",
      "    accuracy                           0.59     97192\n",
      "   macro avg       0.43      0.38      0.39     97192\n",
      "weighted avg       0.57      0.59      0.57     97192\n",
      "\n",
      "Acur√°cia\n",
      "0.3764314093914398\n",
      "Precisao\n",
      "0.5664396111581879\n",
      "Recall\n",
      "0.5938760391801794\n",
      "F1\n",
      "0.5712182879951612\n",
      "[[    0   300    23   154     1   532    42   129]\n",
      " [    0 13778   158  2114    13  2691   190   756]\n",
      " [    0   464   861   920     6   827    89   525]\n",
      " [    0  1607   345 27159    83  2107   176  1421]\n",
      " [    0    51     3   402   147    68     2    45]\n",
      " [    0  3680   350  2523    21 10439   724  1836]\n",
      " [    0  1304   180  1172    10  3362  1006  1498]\n",
      " [    0  1044   345  2458    22  2280   419  4330]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   8   11   19   44   45   50   51   56   60   63   78   80   89   91\n",
      "   95  119  122  123  125  127  129  133  136  139  146  150  160  165\n",
      "  168  170  180  183  188  194  196  198  205  207  208  217  218  232\n",
      "  234  240  241  242  244  257  263  268  272  278  281  287  289  290\n",
      "  292  294  296  298  300  302  308  311  316  318  326  327  331  349\n",
      "  357  363  364  368  372  379  382  386  389  391  392  406  409  410\n",
      "  414  417  418  420  421  425  426  427  429  431  436  444  457  463\n",
      "  465  475  490  493  496  505  507  509  512  513  524  527  530  536\n",
      "  537  542  545  546  547  551  552  563  578  579  580  584  585  587\n",
      "  588  594  598  602  617  619  626  647  657  665  673  679  689  697\n",
      "  703  709  721  726  727  728  734  736  738  751  759  760  761  774\n",
      "  775  778  779  781  785  791  795  806  808  815  818  819  824  841\n",
      "  842  843  848  850  860  866  876  885  889  891  894  905  915  918\n",
      "  922  924  925  938  942  945  947  950  955  959  963  969  979  988\n",
      "  990  998 1002 1005 1015 1021 1024 1026 1029 1030 1036 1038 1039 1044\n",
      " 1047 1055 1069 1073 1074 1075 1079 1083 1086 1087 1093 1096 1099 1102\n",
      " 1103 1104 1106 1117 1121 1123 1131 1133 1140 1146 1150 1156 1157 1165\n",
      " 1173 1176 1177 1178 1186 1190 1192 1195 1199 1202 1205 1219 1222 1226\n",
      " 1232 1242 1243 1245 1248 1249 1252 1253 1260 1266 1270 1272 1277 1289\n",
      " 1290 1294 1301 1309 1317 1318 1319 1328 1341 1353 1357 1359 1360 1365\n",
      " 1369 1370 1376 1389 1392 1394 1397 1404 1418 1421 1424 1426 1429 1440\n",
      " 1448 1449 1463 1465 1466 1467 1474 1475 1479 1483 1484 1489 1496 1497\n",
      " 1499 1500 1503 1511 1513 1518 1524 1532 1545 1549 1561 1571 1573 1579\n",
      " 1581 1584 1588 1590 1591 1594 1595 1602 1604 1607 1609 1613 1627 1634\n",
      " 1640 1641 1653 1654 1659 1664 1665 1679 1680 1691 1703 1705 1709 1720\n",
      " 1721 1732 1746 1753 1755 1760 1769 1770 1776 1792 1803 1804 1812 1828\n",
      " 1831 1835 1839 1842 1846 1860 1862 1866 1867 1868 1869 1875 1878 1886\n",
      " 1894 1903 1913 1928 1931 1933 1934 1939 1943 1948 1949 1950 1952 1953\n",
      " 1956 1961 1970 1971 1976 1979 1982 1996]\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5824 - acc: 0.1581\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5252 - acc: 0.1709\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5148 - acc: 0.1764\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5063 - acc: 0.1800\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5023 - acc: 0.1812\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4966 - acc: 0.1837\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4937 - acc: 0.1851\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4912 - acc: 0.1861\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4854 - acc: 0.1891\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4810 - acc: 0.1913\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4809 - acc: 0.1908\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4757 - acc: 0.1932\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4724 - acc: 0.1948\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4727 - acc: 0.1945\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4691 - acc: 0.1963\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4659 - acc: 0.1975\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4616 - acc: 0.1992\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4562 - acc: 0.2013\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4496 - acc: 0.2037\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4442 - acc: 0.2060\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4401 - acc: 0.2071\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4371 - acc: 0.2086\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4335 - acc: 0.2096\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4295 - acc: 0.2111\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4266 - acc: 0.2123\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4251 - acc: 0.2126\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4224 - acc: 0.2135\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4203 - acc: 0.2148\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4166 - acc: 0.2159\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4144 - acc: 0.2167\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4112 - acc: 0.2176\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4070 - acc: 0.2192\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4052 - acc: 0.2194\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4018 - acc: 0.2208\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4001 - acc: 0.2210\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3973 - acc: 0.2223\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3939 - acc: 0.2234\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3908 - acc: 0.2245\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3888 - acc: 0.2250\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3878 - acc: 0.2255\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3837 - acc: 0.2269\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3819 - acc: 0.2273\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3785 - acc: 0.2280\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3762 - acc: 0.2292\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3738 - acc: 0.2300\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3720 - acc: 0.2304\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3681 - acc: 0.2313\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3666 - acc: 0.2324\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3657 - acc: 0.2323\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3617 - acc: 0.2339\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3599 - acc: 0.2342\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3582 - acc: 0.2347\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3559 - acc: 0.2355\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3553 - acc: 0.2358\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3526 - acc: 0.2366\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3503 - acc: 0.2374\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3485 - acc: 0.2377\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3451 - acc: 0.2392\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3437 - acc: 0.2395\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3426 - acc: 0.2399\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3410 - acc: 0.2401\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3384 - acc: 0.2411\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3376 - acc: 0.2414\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3358 - acc: 0.2425\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3349 - acc: 0.2421\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3325 - acc: 0.2430\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3301 - acc: 0.2438\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3301 - acc: 0.2438\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3278 - acc: 0.2444\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3273 - acc: 0.2449\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3263 - acc: 0.2452\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3245 - acc: 0.2458\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3222 - acc: 0.2465\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3214 - acc: 0.2467\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3207 - acc: 0.2468\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3189 - acc: 0.2475\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3185 - acc: 0.2477\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3150 - acc: 0.2489\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3147 - acc: 0.2488\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3145 - acc: 0.2489\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3122 - acc: 0.2499\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3116 - acc: 0.2498\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3100 - acc: 0.2506\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3093 - acc: 0.2509\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3076 - acc: 0.2511\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3065 - acc: 0.2517\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3065 - acc: 0.2515\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3057 - acc: 0.2522\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3041 - acc: 0.2524\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3031 - acc: 0.2529\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3012 - acc: 0.2532\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3010 - acc: 0.2534\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2996 - acc: 0.2539\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2990 - acc: 0.2543\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2974 - acc: 0.2546\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2961 - acc: 0.2553\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2964 - acc: 0.2550\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2958 - acc: 0.2553\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2946 - acc: 0.2559\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.2924 - acc: 0.2561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.00      1422\n",
      "           1       0.62      0.72      0.66     23271\n",
      "           2       0.37      0.29      0.32      4188\n",
      "           3       0.75      0.80      0.78     33561\n",
      "           4       0.52      0.33      0.41       658\n",
      "           5       0.48      0.54      0.51     22234\n",
      "           6       0.33      0.12      0.18      9465\n",
      "           7       0.41      0.42      0.42     12090\n",
      "\n",
      "    accuracy                           0.59    106889\n",
      "   macro avg       0.50      0.40      0.41    106889\n",
      "weighted avg       0.57      0.59      0.57    106889\n",
      "\n",
      "Acur√°cia\n",
      "0.402602609506242\n",
      "Precisao\n",
      "0.5725049992103205\n",
      "Recall\n",
      "0.5918756841209105\n",
      "F1\n",
      "0.5727755101809545\n",
      "[[    1   331    58   145     1   672    72   142]\n",
      " [    0 16711   238  1901    17  3123   319   962]\n",
      " [    0   589  1212   885     4   802   104   592]\n",
      " [    0  2124   482 27013   136  2003   190  1613]\n",
      " [    0    45     5   281   219    51    11    46]\n",
      " [    1  4328   532  2299    17 11904   979  2174]\n",
      " [    0  1585   285  1089    11  3657  1145  1693]\n",
      " [    0  1384   504  2171    15  2357   599  5060]]\n",
      "TRAIN: [   0    1    2 ... 1997 1998 1999] TEST: [   6   10   20   22   28   32   34   35   40   42   53   54   57   58\n",
      "   61   62   70   85   87   90  104  109  111  116  121  141  144  149\n",
      "  153  167  169  177  181  186  189  191  195  201  206  212  213  224\n",
      "  228  230  233  239  259  260  264  277  282  283  285  286  291  293\n",
      "  299  304  306  309  321  322  328  330  335  338  346  348  350  351\n",
      "  355  356  367  370  371  374  377  378  384  385  411  424  430  434\n",
      "  437  440  442  450  451  453  456  468  472  479  484  495  498  508\n",
      "  510  511  514  515  519  520  531  539  544  549  555  562  566  567\n",
      "  568  577  581  583  599  601  608  618  620  624  633  634  650  651\n",
      "  652  653  654  660  664  670  671  678  691  693  694  698  699  700\n",
      "  702  705  707  712  713  715  719  724  730  732  733  739  741  744\n",
      "  746  749  756  762  766  769  770  780  790  793  803  805  811  812\n",
      "  813  814  821  831  834  835  838  847  849  853  856  859  862  874\n",
      "  886  887  892  893  897  898  901  912  919  920  923  928  930  937\n",
      "  939  940  943  944  948  951  956  958  964  982  987  993  994  995\n",
      " 1008 1010 1012 1014 1031 1032 1035 1037 1050 1054 1056 1063 1065 1078\n",
      " 1088 1090 1092 1094 1095 1097 1098 1110 1115 1125 1130 1132 1136 1137\n",
      " 1143 1144 1154 1162 1174 1175 1182 1184 1188 1196 1200 1208 1214 1215\n",
      " 1216 1218 1228 1233 1235 1244 1259 1265 1268 1273 1274 1278 1281 1282\n",
      " 1287 1291 1292 1293 1295 1296 1298 1299 1300 1306 1311 1313 1320 1329\n",
      " 1333 1340 1342 1343 1344 1347 1354 1356 1362 1364 1372 1375 1377 1382\n",
      " 1383 1384 1387 1393 1399 1400 1401 1407 1413 1415 1425 1427 1432 1436\n",
      " 1437 1439 1443 1446 1461 1468 1470 1476 1478 1481 1490 1505 1510 1514\n",
      " 1519 1521 1522 1525 1533 1534 1535 1537 1540 1547 1552 1554 1596 1600\n",
      " 1601 1606 1620 1637 1646 1647 1649 1655 1662 1666 1667 1690 1696 1698\n",
      " 1707 1711 1713 1718 1724 1726 1728 1731 1737 1741 1742 1743 1747 1750\n",
      " 1752 1771 1774 1779 1790 1806 1813 1815 1822 1826 1830 1838 1843 1845\n",
      " 1852 1853 1858 1883 1891 1897 1898 1902 1911 1914 1918 1932 1945 1959\n",
      " 1964 1966 1973 1974 1981 1983 1985 1990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5831 - acc: 0.1644\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5291 - acc: 0.1711\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5180 - acc: 0.1764\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5092 - acc: 0.1802\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5050 - acc: 0.1823\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.5004 - acc: 0.1844\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4972 - acc: 0.1856\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4936 - acc: 0.1872\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4913 - acc: 0.1882\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4878 - acc: 0.1896\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4827 - acc: 0.1920\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4832 - acc: 0.1916\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4795 - acc: 0.1931\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4759 - acc: 0.1947\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4756 - acc: 0.1949\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4709 - acc: 0.1963\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4666 - acc: 0.1985\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4575 - acc: 0.2017\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4550 - acc: 0.2032\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4469 - acc: 0.2060\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4439 - acc: 0.2072\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4396 - acc: 0.2089\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4377 - acc: 0.2094\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4358 - acc: 0.2103\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4320 - acc: 0.2115\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4279 - acc: 0.2131\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4263 - acc: 0.2136\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4241 - acc: 0.2144\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4205 - acc: 0.2158\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4169 - acc: 0.2171\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4152 - acc: 0.2174\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4125 - acc: 0.2182\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4092 - acc: 0.2196\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4079 - acc: 0.2199\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4037 - acc: 0.2215\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.4008 - acc: 0.2223\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3994 - acc: 0.2230\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3979 - acc: 0.2233\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3957 - acc: 0.2239\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3910 - acc: 0.2256\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3897 - acc: 0.2262\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3867 - acc: 0.2269\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3840 - acc: 0.2278\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3815 - acc: 0.2286\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3794 - acc: 0.2293\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3768 - acc: 0.2304\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3739 - acc: 0.2312\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3710 - acc: 0.2321\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3695 - acc: 0.2322\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3675 - acc: 0.2332\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3660 - acc: 0.2336\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3629 - acc: 0.2344\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3607 - acc: 0.2355\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3590 - acc: 0.2359\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3572 - acc: 0.2366\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3548 - acc: 0.2373\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3524 - acc: 0.2379\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3514 - acc: 0.2383\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3488 - acc: 0.2388\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3476 - acc: 0.2394\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3454 - acc: 0.2401\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3431 - acc: 0.2410\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3435 - acc: 0.2407\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3409 - acc: 0.2418\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3391 - acc: 0.2421\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3376 - acc: 0.2425\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3366 - acc: 0.2431\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3347 - acc: 0.2436\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3341 - acc: 0.2438\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3323 - acc: 0.2443\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3295 - acc: 0.2451\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3283 - acc: 0.2458\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3268 - acc: 0.2463\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3265 - acc: 0.2464\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3239 - acc: 0.2472\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3235 - acc: 0.2474\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3222 - acc: 0.2477\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3214 - acc: 0.2480\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3202 - acc: 0.2484\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3182 - acc: 0.2489\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3166 - acc: 0.2493\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3158 - acc: 0.2499\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3146 - acc: 0.2502\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3139 - acc: 0.2505\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3127 - acc: 0.2510\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3107 - acc: 0.2513\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3103 - acc: 0.2517\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3097 - acc: 0.2518\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3091 - acc: 0.2520\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3077 - acc: 0.2525\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3071 - acc: 0.2527\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3054 - acc: 0.2534\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3042 - acc: 0.2538\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3026 - acc: 0.2542\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3013 - acc: 0.2545\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 8s 5ms/sample - loss: 0.3013 - acc: 0.2548\n",
      "Epoch 97/100\n",
      " 800/1600 [==============>...............] - ETA: 3s - loss: 0.2979 - acc: 0.2522"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in kf.split(previsores):\n",
    "    model = None\n",
    "    model = criarRede()\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_and_evaluate_model(model, previsores[train_index], classes[train_index],\n",
    "                           previsores[test_index], classes[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_12 (Bidirectio (None, 700, 200)          97600     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 700, 200)          241600    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 700, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 700, 8)            1608      \n",
      "=================================================================\n",
      "Total params: 582,408\n",
      "Trainable params: 582,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cias total\n",
      "[0.35915306700300176, 0.3657686597248555, 0.3730990359060886, 0.37638366504933796, 0.36126651444129454]\n",
      "0.36713418842491563\n",
      "Precision total\n",
      "[0.5358692747285666, 0.539195778418488, 0.5569088715949322, 0.5490842678202248, 0.5354863437177183]\n",
      "0.543308907255986\n",
      "Recalls total\n",
      "[0.5482441618136844, 0.5525903724410886, 0.5657434373839448, 0.5568281331483504, 0.5502227044312471]\n",
      "0.554725761843663\n",
      "F1 total\n",
      "[0.531212445417416, 0.5355347926101575, 0.5486665733108644, 0.5431847405399131, 0.5336637122813451]\n",
      "0.5384524528319392\n"
     ]
    }
   ],
   "source": [
    "print('Acur√°cias total')\n",
    "print(accu)\n",
    "a = np.array(accu)\n",
    "print(a.mean())\n",
    "print('Precision total')\n",
    "print(precisions)\n",
    "p = np.array(precisions)\n",
    "print(p.mean())\n",
    "print('Recalls total')\n",
    "print(recalls)\n",
    "r = np.array(recalls)\n",
    "print(r.mean())\n",
    "print('F1 total')\n",
    "print(f1)\n",
    "f = np.array(f1)\n",
    "print(f.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
